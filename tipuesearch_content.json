{"pages":[{"url":"Semantic-model-built-with-tensorflow-deployed-with-Flask-and-Heroku-Part-1.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Once you create a cool deep learning application, next step is to deploy it so that anyone all over the world can use your cool application. In this blog and the next blog , I will explore simple ways to deploy deep learning models into a public cloud platform. This blog will focus on the model development phase. Quick google search shows there are various public clouds that let us deploy our model: Amazon Web Service Google Cloud Heroku The great advantages of AWS and Google cloud is that they offer GPU instances. However, it requires DevOps work needed to set up a server i.e., it takes longer for setting up the server. While Heroku does not offer GPU instance, it is VERY easy to set up and host at most 5 applications FOR FREE. It seems that Heroku is a good place to learn deployment in general for the first time. So this blog will use Heroku as a tool. In the first blog post, I will create a simple deep learning model for sentiment analysis using tensor-flow. The model takes tweet or short text as a sentence and return the likelihood of happiness. Although it is relatively easy to deploy a prediction model to Heroku, this comes with some costs: for example, maximum slug size is 500 MB or a request has not been processed by a worker within 30 secounds . Because of these restrictions, my focus in this blog was to simplify the model and to reduce the size of the model weights. In the next blog , I will discuss how to deploy this model in Heroku. If you are not interested in the model development, I recommend you to jump to the second phase. To motivate the readers, click here to see my deployed web app Reference: Deep learning semantic model built with tensorflow deployed using Flask and Heroku Part 1 (Model Development) Deep learning semantic model built with tensorflow deployed using Flask and Heroku Part 2 (Deployment) Sentiment analysis model development Social media is a great place to share your thoughts about anything to anyone. Social media became a important place to learn public opinions. Especially twitter is considered one of the most important place for this purpose because of its scale, diversity of topics and public access to the contents. SemEval offers public tweet dataset that researchers can share task on Sentiment Analysis on Twitter. The task ran in 2013, 2014, 2015 and 2016, attracting over 40+ participating teams in all four editions. I will use their public labeled data to develop a model that tells the happiness level. I will first create a simgle layer LSTM model. Later, I will try transfer learning using GloVe weights. To start, I import necessary modules. In [1]: import matplotlib.pyplot as plt import sys , time , os , warnings import numpy as np import pandas as pd from collections import Counter import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"0\" #\"2,3\" GPU 2 and 3 is only visible import tensorflow as tf warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] tensorflow version 1.5.0 Download data from SemEval website The data is downloaded from here and located at the following directory. In [2]: dir_data = \"../Sentiment/2017_English_final/GOLD/Subtask_A/\" In [3]: ls $ dir_data livejournal-2014test-A.tsv twitter-2014test-A.txt README.txt twitter-2015test-A.txt sms-2013test-A.tsv twitter-2015train-A.txt twitter-2013dev-A.txt twitter-2016dev-A.txt twitter-2013test-A.txt twitter-2016devtest-A.txt twitter-2013train-A.txt twitter-2016test-A.txt twitter-2014sarcasm-A.txt twitter-2016train-A.txt Load dataset I will only use the training data because only training data contains labels. In [4]: def extractData ( path_data ): file = open ( path_data , 'r' ) text = file . read () file . close () texts = text . split ( \" \\n \" ) data = [] for line in texts : cols = line . split ( \" \\t \" ) data . append ( cols ) data = pd . DataFrame ( data ) return ( data ) d = {} for i in [ 3 , 5 , 6 ]: path_data = dir_data + \"twitter-201{}train-A.txt\" . format ( i ) d [ path_data ] = extractData ( path_data ) print ( \"combine the data\" ) data = pd . concat ( d ) data = data . reset_index () data = data [[ 1 , 2 ]] data . columns = [ \"class\" , \"text\" ] ## remove NaN data = data . loc [ ~ data [ \"class\" ] . isnull (),:] combine the data Descriptive analysis The distribution of the positive, neutral and negative tweets. Roughly the same number of positive and negative tweets. In [5]: c = Counter ( data [ \"class\" ] . values ) tot = np . sum ( c . values ()) labels = [ str ( k ) + \" \" + str ( np . round ( i * 100 / tot , 3 )) + \"%\" for k , i in c . items ()] x = range ( len ( c )) plt . bar ( x , c . values ()) plt . xticks ( x , labels ) plt . show () Example tweets Let's take a look at 20 randomly selected raw tweets. In [6]: Ntweet = 20 index = np . random . choice ( range ( len ( data )), Ntweet ) for i in index : row = data . iloc [ i ,:] print ( \"{:6}: {:}\" . format ( row . iloc [ 0 ], row . iloc [ 1 ])) positive: @owyposadas see you on Monday!!! SM Bacolod!!! woooohhhh neutral: @KennethBartlet9 Rush said the other day that HRC was worst S.O.S. ever but Kerry is catching up..(may have been quoting someone) positive: We are excited to announce our next Day on the Green on Sunday January 27th - Australia Day Long Weekend! We are... http://t.co/P8BNE6NV positive: Astrology may be a blasphemous concept to some\\u002c but I am the absolute epitome of my sign. Every last detail. positive: @USC_Zack @broncopeyton @gsf_23 lmfao when May comes around\\u002c watch Zachary cheer for the Bulls or Celtics lololol positive: @jurxssicpratt i just ordered one from Amazon it'll be here on Friday let's fucking rOLL negative: \"Ask for Lil Wayne, get Michael Jackson. Bug me about Wiz Khalifa, get Guns & Roses. The point of the night is to step outside the matrix..\" neutral: Chelsea need to beat Ajax to keep up there battle with Ajax for 2nd place #NextGenSeries neutral: @Steilo_T2G lol yea I did nigga ! & I said wassup _ was tryin to see was u going up to fairground when the sun leave & shit neutral: @mac_b_from_tn yeah but we have Memorial Day\\u002c 4th of July\\u002c Cinco de Mayo\\u002c Thanksgiving\\u002c Presidents Day\\u002c MLK day\\u002c and do they do Labor Day? negative: Think imma go see Flight tomorrow.. Denzel\\u2019s the shit positive: #nowplaying Bob Marley - Sun Is Shining on Surf Shack Radio, The Best Mix of Music on the Beach neutral: Anyone going to Siberia for the HOD show on Saturday? positive: RT @JakeAustinXXX: @tinroom tonight! Come see me and the hottest gogo boys in Dallas!! @gaypornfanatic @bonepilot positive: OMG I can't wait to see what Beyonce is gonna do at the VMA'S Sunday August 28th! http://twitpic.com/62q0w9 neutral: Photo: Flatbush Zombies\\u002c Santos Party House October 17th! #ACIDALUMNI (Taken with Instagram) http://t.co/PYtQP9ej neutral: Chelsea are going to write a blank check to Juventus for Pogba tomorrow. negative: \"\\\"\"\"\"No you may not kick it.\\\"\"\"\" -Tribe Called Quest answering a text from Billy Cundiff\" neutral: Geoff Shackelford: Honda Classic Ratings Up 78%: Michael Hiestand with the http://t.co/bhTkmp2i http://t.co/lpceXHmM positive: Ravens play eagles on the 11th at 730! #GETPUMPED Give integer labels to indicate the negative, neutral and positive tweets To simplify the problem and focus only on whether the tweet is happy, I will combine the negative and neutral tweets. Negative : 0 Neutral : 0 Positive : 1 This means that the 42% of tweets are 0 and 58% are 1. In [7]: classes = [] for cl in data [ \"class\" ]: if cl == \"negative\" : classes . append ( 0 ) elif cl == \"positive\" : classes . append ( 1 ) elif cl == \"neutral\" : classes . append ( 0 ) else : print ( \"SHOULD NOT BE HERE\" ) data [ \"class\" ] = classes Text cleaning I need to admit the this text cleaning section needs serious improvement... for example, I should remove URL or hash tags. We should also combine grammatically same words, e.g. \"I've\", \"I have\", \"I hav\" should be all treated the same. Serious text cleaning will improve the model performance substantially. I should also For now, I just do: get rid of quotes from the text assign a single space between word and !. So that ! is treated as a single word. Do the same for ?. In [8]: import re from copy import copy def clean_text ( texts_original ): texts = [] for text in texts_original : otext = copy ( text ) text = otext . replace ( \"!\" , \" !\" ) . replace ( \"?\" , \" ?\" ) text = text . replace ( '\"' , \"\" ) text = text . replace ( '\"' , \"\" ) text = text . replace ( '\"' , \"\" ) if text == \"\" : print ( otext ) texts . append ( text ) return ( texts ) data [ \"text\" ] = clean_text ( data [ \"text\" ] . values ) Tokenizer Here I use Keras's preprocessing tokenizer to create a dictionary that maps word string to index ID. I will only extract the 5000 most common words and everything else is ignored. In [9]: from tensorflow.contrib.keras import preprocessing nb_words = 5000 tokenizer = preprocessing . text . Tokenizer ( nb_words ) tokenizer . fit_on_texts ( data [ \"text\" ] . values ) vocab_size = nb_words + 1 index_word = { v : k for k , v in tokenizer . word_index . items ()} print ( \"The sentence has {} unique words\" . format ( len ( np . unique ( tokenizer . word_index . keys ())))) print ( \"> vocab_size={}\" . format ( vocab_size )) The sentence has 33795 unique words > vocab_size=5001 Record tokenizer The tokenizer takes a string sentence, and create a list containing index IDs. This tokenizer also needs to be used during prediction to process the input sentence. So I need do save it. In general, tokenizer object can be saved and loaded using pickle as discussed in stack overflow : import pickle # saving with open('tokenizer.pickle', 'wb') as handle: pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) # loading with open('tokenizer.pickle', 'rb') as handle: tokenizer = pickle.load(handle) However, this create 1.5MB pickle object. This may or may not be too large for Heroku application, which only allows 500MB memory space and 30 seconds for every backend calculation. In practice, this tokenizer function is quite simple: it just maps the word and the word index. So instead of saving the tokenizer object itself, I will just record the words in the order of ID and save it into a csv file. This file size is as small as 33KB. In [10]: mytokenizer = [] for i in range ( 1 , nb_words + 1 ): mytokenizer . append ( index_word [ i ]) pd . DataFrame ({ \"tokenizer\" : mytokenizer }) . to_csv ( \"tokenizer.csv\" , index = False ) The following code contains extract word_index dictionary and index_word dictionary from the csv. In [11]: def get_word_index_from_csv (): tokenizer = pd . read_csv ( \"tokenizer.csv\" )[ \"tokenizer\" ] . values word_index = {} index_word = {} for index , word in enumerate ( tokenizer , 1 ): word_index [ word ] = index index_word [ index ] = word return ( word_index , index_word ) word_index , index_word = get_word_index_from_csv () Now using word_index, encode each sentence. In [12]: def texts_to_sequences ( line ): out = [] for l in line . split (): llower = l . lower () if llower in word_index . keys (): out . append ( word_index [ llower ]) return ( out ) N = data . shape [ 0 ] prop_train = 0.8 Ntrain = int ( N * prop_train ) Ntest = N - Ntrain sequences , index_train , index_test = [], [], [] count = 0 for irow , line in enumerate ( data [ \"text\" ]): encoded = texts_to_sequences ( line ) sequences . append ( encoded ) if irow < Ntrain : index_train . append ( count ) else : index_test . append ( count ) count += 1 print ( 'Total Sequences: %d ' % ( len ( sequences ))) Total Sequences: 16173 Let's look at the example encoded tweets. In [13]: def print_text ( encoded ): ''' encoded : a list containing index e.g. [1, 300, 2] index_word : dictionary {0 : \"am\", 1 : \"I\", 2 : \".\",..} ''' for k in encoded : print ( \"{}({})\" . format ( index_word [ k ], k )), print ( \"\" ) set_seed ( 1 ) random_index = np . random . choice ( data . shape [ 0 ], 20 ) for irow , line in enumerate ( data [ \"text\" ] . iloc [ random_index ]): encoded = texts_to_sequences ( line ) print ( \"irow={}\" . format ( random_index [ irow ])) print_text ( encoded ) print ( \"\" ) irow=13349 was(32) having(462) trouble(3840) yesterday(386) downloading(4818) edition(1409) using(1101) google(154) loads(3527) the(1) 1st(46) page(1020) then(100) irow=235 paid(1291) a(6) few(482) for(13) a(6) jim(924) white(392) record(612) if(42) i(3) recall(2421) but(30) not(43) in(5) the(1) so(37) i(3) may(21) well(157) be(20) missing(973) the(1) irow=12172 watching(142) tomorrow(18) just(28) because(186) conor(419) mcgregor(443) and(8) faber(4685) are(45) the(1) irow=5192 going(31) to(2) be(20) here(120) tomorrow(18) have(25) lots(959) to(2) swap(2022) london(450) book(388) swap(2022) via(261) irow=16127 anyone(244) want(85) to(2) come(90) to(2) kenny(2470) chesney(3899) and(8) jason(685) at(17) metlife(405) on(4) saturday(52) i(3) have(25) an(80) extra(1029) ticket(604) and(8) by(66) extra(1029) i(3) mean(603) a(6) plus(569) one(61) irow=905 safe(896) and(8) sound(729) at(17) airport(2197) feels(1512) good(69) to(2) be(20) back(94) at(17) the(1) land(2103) of(12) rising(1502) sun(103) irow=10955 android(695) poor(1737) cousin(3438) to(2) the(1) apple(119) of(12) the(1) a(6) of(12) the(1) irow=7813 on(4) the(1) 7(158) hour(662) journey(1439) from(41) uni(3224) to(2) park(200) my(22) real(242) come(90) on(4) newcastle(1983) my(22) 1st(46) european(2447) night(33) at(17) sjp(2993) irow=2895 once(706) it(16) can(59) never(246) be(20) i(3) was(32) in(5) constitution(1145) hall(611) last(75) got(96) so(37) much(202) history(767) was(32) almost(642) lost(488) irow=5056 enter(2131) our(122) competition(1753) to(2) win(153) a(6) london(450) map(1881) puzzle(3378) just(28) follow(521) and(8) friday(47) irow=144 midnight(1695) on(4) the(1) east(979) coast(3032) which(315) means(694) its(151) birthday(147) happy(140) 20th(672) birthday(147) nick(747) j(830) irow=4225 allstar(2953) game(53) weekend(236) in(5) texas(1229) for(13) the(1) 21st(750) yea(1359) irow=7751 march(237) 16(413) luke(3480) bryan(3586) is(14) gonna(138) at(17) the(1) houston(925) i(3) have(25) to(2) its(151) a(6) must(590) irow=10989 en(2035) f(1376) we(40) fail(2291) 1st(46) we(40) chase(2248) 4(121) top(264) 4(121) finish(676) at(17) the(1) end(286) like(49) arsenal(227) irow=14844 appeared(3737) on(4) thursday(83) 3(82) at(17) the(1) 14th(692) place(247) in(5) the(1) top20(4672) of(12) irow=3462 kevin(756) rudd(1200) out(36) and(8) lives(2015) it(16) up(48) at(17) herald(2628) kevin(756) rudd(1200) out(36) irow=15641 lexus(331) 300(4285) black(159) with(19) tan(3466) wednesday(135) it(16) now(64) irow=9394 man(201) will(29) be(20) performing(680) at(17) next(110) friday(47) for(13) the(1) contact(1538) for(13) tickets(211) irow=5396 yeah(490) best(105) way(197) yeah(490) i(3) had(132) my(22) first(118) one(61) on(4) 2(65) days(278) after(99) my(22) really(160) good(69) love(92) it(16) went(280) on(4) a1(2665) yesterday(386) xx(983) irow=5374 hey(403) are(45) you(15) going(31) to(2) blue(1018) diamond(2952) tomorrow(18) and(8) are(45) you(15) driving(1758) Add zero padding to the sentence that is shorter than the maximum length. This zero padding function will be also necessary in deployment. X X has a shape (N tweets, max length of tweet) X has zero padding when the original tweet length is less than max length of tweet. In [14]: def pad_pre_sequences ( arr , maxlen ): lines = [] for iline in range ( len ( arr )): oline = arr [ iline ] lo = len ( oline ) if maxlen > lo : line = [ 0 ] * ( maxlen - lo ) + list ( oline ) else : line = oline [: maxlen ] lines . append ( line ) if len ( line ) != maxlen : print ( maxlen ) print ( line ) lines = np . array ( lines ) return ( lines ) max_length = max ([ len ( seq ) for seq in sequences ]) X = pad_pre_sequences ( sequences , maxlen = max_length ) print ( 'Max Sequence Length: %d ' % max_length ) Max Sequence Length: 33 One-hot encoding to create y y[i,0] == 1 if the i&#94;th tweet is negative or neutral otherwise 0 y[i,1] == 1 if the i&#94;th tweet is positive otherwise 0 In [15]: from keras.utils import to_categorical y = to_categorical ( data [ \"class\" ] . values , num_classes = 2 ) Using TensorFlow backend. Split between training and testing data In [16]: X_train , y_train , X_test , y_test = X [ index_train ], y [ index_train ], X [ index_test ], y [ index_test ] Model definition Here, I define a deep learning model with embedding layer + single layer LSTM followed by a single dense layer. Notice that I am using tensorflow to extract Keras's models and layers. I could have extracted these modules directly from Keras. However, somehow Heroku gave me warning when I try to import keras. Importing tensorflow did not yield the same error. This is the reason why I am using tensorflow in this blog post. In [17]: from tensorflow.contrib.keras import models from tensorflow.contrib.keras import layers def define_model ( input_length , Embedding , dim_out = 2 ): hidden_unit_LSTM = 4 main_input = layers . Input ( shape = ( input_length ,), dtype = 'int32' , name = 'main_input' ) embedding = Embedding ( main_input ) x = layers . LSTM ( hidden_unit_LSTM )( embedding ) main_output = layers . Dense ( dim_out , activation = 'softmax' )( x ) model = models . Model ( inputs = [ main_input ], outputs = [ main_output ]) # compile network model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) return ( model ) dim_dense_embedding = 50 Embedding1 = layers . Embedding ( vocab_size , dim_dense_embedding ) model1 = define_model ( X . shape [ 1 ], Embedding1 ) WARNING:tensorflow:From /home/fairy/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1456: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead WARNING:tensorflow:From /home/fairy/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 33) 0 _________________________________________________________________ embedding_1 (Embedding) (None, 33, 50) 250050 _________________________________________________________________ lstm_1 (LSTM) (None, 4) 880 _________________________________________________________________ dense_1 (Dense) (None, 2) 10 ================================================================= Total params: 250,940 Trainable params: 250,940 Non-trainable params: 0 _________________________________________________________________ None Training starts here: In [18]: import time start = time . time () hist1 = model1 . fit ( X_train , y_train , validation_data = ( X_test , y_test ), epochs = 10 , batch_size = 64 , verbose = 2 ) end = time . time () print ( \"Time took: {:3.2f}MIN\" . format (( end - start ) / 60.0 )) Train on 12938 samples, validate on 3235 samples Epoch 1/10 - 15s - loss: 0.6708 - acc: 0.5999 - val_loss: 0.7330 - val_acc: 0.4686 Epoch 2/10 - 13s - loss: 0.6289 - acc: 0.6450 - val_loss: 0.7690 - val_acc: 0.5168 Epoch 3/10 - 13s - loss: 0.5290 - acc: 0.7462 - val_loss: 0.7103 - val_acc: 0.5913 Epoch 4/10 - 13s - loss: 0.4512 - acc: 0.8008 - val_loss: 0.7766 - val_acc: 0.5938 Epoch 5/10 - 13s - loss: 0.4024 - acc: 0.8337 - val_loss: 0.8021 - val_acc: 0.5963 Epoch 6/10 - 13s - loss: 0.3726 - acc: 0.8508 - val_loss: 0.8317 - val_acc: 0.6031 Epoch 7/10 - 13s - loss: 0.3475 - acc: 0.8673 - val_loss: 0.8986 - val_acc: 0.5963 Epoch 8/10 - 13s - loss: 0.3235 - acc: 0.8818 - val_loss: 1.0821 - val_acc: 0.5598 Epoch 9/10 - 13s - loss: 0.3093 - acc: 0.8878 - val_loss: 0.9846 - val_acc: 0.5892 Epoch 10/10 - 13s - loss: 0.2869 - acc: 0.8987 - val_loss: 0.9319 - val_acc: 0.6003 Time took: 2.17MIN Validation loss and validation accuracies over epochs The validation loss increases very quickly. Model is overfitting. In [19]: def plot_loss ( hist1 ): for label in hist1 . history . keys (): plt . plot ( hist1 . history [ label ], label = label ) plt . legend () plt . show () plot_loss ( hist1 ) Attempt 2: GloVe pre-trained weights The model performance was not very good. Let's try to improve the model performance by transfer learning. The researchers behind GloVe method provide a suite of pre-trained word embeddings on their website released under a public domain license. See: GloVe: Global Vectors for Word Representation The smallest package of embeddings is called \"glove.6B.zip\". Weights are trained with 6 billion tokens (words) with 400,000 vocabularies. There are four dimensions of embedding vectors, i.e., 50d, 100d, 200d, and 300d. wget 'nlp.stanford.edu/data/glove.6B.zip' unzipping gives four .txt files: 164M Aug 4 2014 glove.6B.50d.txt 332M Aug 4 2014 glove.6B.100d.txt 662M Aug 4 2014 glove.6B.200d.txt 990M Aug 27 2014 glove.6B.300d.txt Reference nice example of how to use pre_trained word embedding with Keras In [20]: dim_embedding = 50 path_GloVe = \"../output/glove.6B.{}d.txt\" . format ( dim_embedding ) def GloVe_embedding ( path_GloVe ): embeddings_index = {} with open ( path_GloVe ) as f : for line in f : values = line . split () word = values [ 0 ] coefs = np . asarray ( values [ 1 :], dtype = 'float32' ) embeddings_index [ word ] = coefs print ( \"{} vocabs \" . format ( len ( embeddings_index ))) return ( embeddings_index ) GloVe_embedding = GloVe_embedding ( path_GloVe ) 400000 vocabs Let's see 50 example tokens in GloVe Some tokens contain digits, all the tokens are in lower case. In [21]: set_seed ( 10 ) isamples = np . random . choice ( len ( GloVe_embedding ), 50 ) def print_words ( words ): count = 1 for word in words : print ( \"{:20}\" . format ( word )), if count % 5 == 0 : print ( \"\" ) count += 1 print_words ( np . array ( GloVe_embedding . keys ())[ isamples ]) ksl kruszwica muzhakhoyeva oberscharführer toder bagbazar frasca disengaged minicomputer embraer palcy jicheng mym casariego vestments lifejackets amirian markie lemisch cuisine rocked flatterer nilin edmonson safarova biking unfavorable tpn8 cupa cobleskill laryngology stadtmuseum colleagues hưng countervailing drillers prinn jaywalker castaños 384 wtvh first-rate tennesse alania-d mazraeh-ye hentschel pollsters eluding 2xcd lapu-lapu Now let's look at the vocabularies that do not appear in the GloVe but appear in the top 5000 most frequent words in my SemEval data. In [22]: word_not_in_GloVe = np . array ( list ( set ( word_index . keys ()) - set ( GloVe_embedding . keys ()))) Nvoc = len ( tokenizer . word_index . keys ()) print ( \"Out of {} vocabulary in original data, {} exists in the vocabulary of GloVe\" . format ( Nvoc , Nvoc - len ( word_not_in_GloVe ))) print ( \"Following {} tokens ({:4.2f}%) do not exist in GloVe!!\" . format ( len ( word_not_in_GloVe ), len ( word_not_in_GloVe ) * 100 / float ( Nvoc ))) print ( \"-\" * 100 ) isamples = np . random . choice ( len ( word_not_in_GloVe ), 50 ) print_words ( word_not_in_GloVe [ isamples ]) Out of 33795 vocabulary in original data, 33613 exists in the vocabulary of GloVe Following 182 tokens (0.54%) do not exist in GloVe!! ---------------------------------------------------------------------------------------------------- you're madonna's ibm's we've samsung's i've trndnl i'm monday's ticketek justinbieber coleswindell oneil beiber cobain's amandawanxo hahaha 30pm trndnl i'm i'm federer's hillary's gears3 tomcc we've i'm monthsary sanders' grimmie 15pm pinkcity bush's tonight's they'll xfactor hillary's mufc 00am retweet federer's isn't he'll beyonce's wasn't jindal's rella don't obama's everyone's Not all the embedding vector is necessary for training. I will simply extract the embedding vectors that appear in our tweeter data. In [23]: # prepare embedding matrix num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , dim_embedding )) count = 0 for word , i in tokenizer . word_index . items (): if i >= nb_words : continue embedding_vector = GloVe_embedding . get ( word ) if embedding_vector is None : count += 1 else : # words not found in embedding index will be all-zeros. embedding_matrix [ i ] = embedding_vector print ( \" {} tokens did not exist\" . format ( count )) 182 tokens did not exist Define the model This time, I will provide embedding matrix from GloVe and freeze the parameters. Notice that the model summary shows that there are only 890 trainable weights. These are the weights from LSTM and Dense layers. In [24]: dim_dense_embedding = embedding_matrix . shape [ 1 ] Embedding2 = layers . Embedding ( vocab_size , dim_dense_embedding , weights = [ embedding_matrix ], trainable = False ) model2 = define_model ( X . shape [ 1 ], Embedding2 ) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 33) 0 _________________________________________________________________ embedding_2 (Embedding) (None, 33, 50) 250050 _________________________________________________________________ lstm_2 (LSTM) (None, 4) 880 _________________________________________________________________ dense_2 (Dense) (None, 2) 10 ================================================================= Total params: 250,940 Trainable params: 890 Non-trainable params: 250,050 _________________________________________________________________ None Model training In [25]: start = time . time () hist2 = model2 . fit ( X_train , y_train , epochs = 10 , batch_size = 64 , validation_data = ( X_test , y_test ), verbose = 2 ) end = time . time () print ( \"Time took: {:3.1f}MIN\" . format (( end - start ) / 60.0 )) Train on 12938 samples, validate on 3235 samples Epoch 1/10 - 13s - loss: 0.6900 - acc: 0.5524 - val_loss: 0.7272 - val_acc: 0.4730 Epoch 2/10 - 13s - loss: 0.6645 - acc: 0.6072 - val_loss: 0.7178 - val_acc: 0.4943 Epoch 3/10 - 13s - loss: 0.6508 - acc: 0.6244 - val_loss: 0.6940 - val_acc: 0.5459 Epoch 4/10 - 13s - loss: 0.6214 - acc: 0.6567 - val_loss: 0.6883 - val_acc: 0.5861 Epoch 5/10 - 13s - loss: 0.5978 - acc: 0.6855 - val_loss: 0.6833 - val_acc: 0.5917 Epoch 6/10 - 13s - loss: 0.5831 - acc: 0.6985 - val_loss: 0.6498 - val_acc: 0.6278 Epoch 7/10 - 13s - loss: 0.5738 - acc: 0.7050 - val_loss: 0.6779 - val_acc: 0.6068 Epoch 8/10 - 12s - loss: 0.5641 - acc: 0.7143 - val_loss: 0.6725 - val_acc: 0.6207 Epoch 9/10 - 12s - loss: 0.5586 - acc: 0.7201 - val_loss: 0.6688 - val_acc: 0.6213 Epoch 10/10 - 13s - loss: 0.5537 - acc: 0.7211 - val_loss: 0.6696 - val_acc: 0.6142 Time took: 2.1MIN Validation loss and validation accuracies over epochs The validation loss decreases more by using weights from GloVe. In [26]: plot_loss ( hist2 ) Save model 2 Save the model weights. In the deployment, the model weights together with the tokenizer's word index are extracted during the prediction. In [27]: model2 . save_weights ( 'sentiment_weights.h5' ) Model validation with example tweets Hummm the happiness level makes some sense??! In the next blog post, I will deploy this model to web app. In [28]: def predict ( line , model ): encoded = texts_to_sequences ( line ) sequences = pad_pre_sequences ([ encoded ], maxlen = max_length ) probs = model . predict ( sequences )[ 0 ] ## The probability of positive tweet is recorded in the 1st position return ( probs [ 1 ]) model = define_model ( X . shape [ 1 ], Embedding1 ) model . load_weights ( 'sentiment_weights.h5' ) word_index , _ = get_word_index_from_csv () texts = [ \"I feel happy!\" , \"What a great day!\" , \"Going to work.\" , \"I broke up with my boyfriend.\" , \"happy happy\" , \"happy happy happy happy so happy\" , \"Life sucks\" , \"I want to kill myself\" , \"kill\" , \"kill hate hate hate kill\" ] for text in texts : print ( \"Prob(Happy Tweet)={:5.3f}, {:20}\" . format ( predict ( text , model2 ), text )) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 33) 0 _________________________________________________________________ embedding_1 (Embedding) (None, 33, 50) 250050 _________________________________________________________________ lstm_3 (LSTM) (None, 4) 880 _________________________________________________________________ dense_3 (Dense) (None, 2) 10 ================================================================= Total params: 250,940 Trainable params: 250,940 Non-trainable params: 0 _________________________________________________________________ None Prob(Happy Tweet)=0.632, I feel happy! Prob(Happy Tweet)=0.665, What a great day! Prob(Happy Tweet)=0.350, Going to work. Prob(Happy Tweet)=0.343, I broke up with my boyfriend. Prob(Happy Tweet)=0.914, happy happy Prob(Happy Tweet)=0.962, happy happy happy happy so happy Prob(Happy Tweet)=0.447, Life sucks Prob(Happy Tweet)=0.426, I want to kill myself Prob(Happy Tweet)=0.198, kill Prob(Happy Tweet)=0.073, kill hate hate hate kill if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Semantic model built with tensorflow deployed with Flask and Heroku - Part 1 -"},{"url":"deployment.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Simple Semantic Analysis ~ how happy are you? ~ In [ ]: 1 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Deployment","title":"Simple Semantic Analysis ~ how happy are you? ~"},{"url":".html","text":"Simple Semantic Analysis ~ how happy are you? ~","tags":"Deployment","title":""},{"url":"Develop_an_image_captioning_deep_learning_model_using_Flickr_8K_data.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Image captioning is an interesting problem, where you can learn both computer vision techniques and natural language processing techniques. In this blog post, I will follow How to Develop a Deep Learning Photo Caption Generator from Scratch and create an image caption generation model using Flicker 8K data. This model takes a single image as input and output the caption to this image. To evaluate the model performance, I will use bilingual evaluation understudy BLEU . For this purpose, I will review the calculation of BLEU by going through its calculation step by step. Reference How to Develop a Deep Learning Photo Caption Generator from Scratch In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys , time , os , warnings import numpy as np import pandas as pd from collections import Counter warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )); del keras print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 Download the Flickr8K Dataset Flilckr8K contains 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations. The dataset can be downloaded by submitting the request form . In [2]: ## The location of the Flickr8K_ photos dir_Flickr_jpg = \"../Flickr8k/Flicker8k_Dataset/\" ## The location of the caption file dir_Flickr_text = \"../Flickr8k/Flickr8k.token.txt\" jpgs = os . listdir ( dir_Flickr_jpg ) print ( \"The number of jpg flies in Flicker8k: {}\" . format ( len ( jpgs ))) The number of jpg flies in Flicker8k: 8091 Preliminary analysis Import caption data Load the text data and save it into a panda dataframe df_txt. filename : jpg file name index : unique ID for each caption for the same image caption : string of caption, all in lower case In [3]: ## read in the Flickr caption data file = open ( dir_Flickr_text , 'r' ) text = file . read () file . close () datatxt = [] for line in text . split ( ' \\n ' ): col = line . split ( ' \\t ' ) if len ( col ) == 1 : continue w = col [ 0 ] . split ( \"#\" ) datatxt . append ( w + [ col [ 1 ] . lower ()]) df_txt = pd . DataFrame ( datatxt , columns = [ \"filename\" , \"index\" , \"caption\" ]) uni_filenames = np . unique ( df_txt . filename . values ) print ( \"The number of unique file names : {}\" . format ( len ( uni_filenames ))) print ( \"The distribution of the number of captions for each image:\" ) Counter ( Counter ( df_txt . filename . values ) . values ()) The number of unique file names : 8092 The distribution of the number of captions for each image: Out[3]: Counter({5: 8092}) Let's have a look at some of the pictures together with the captions. The 5 captions for each image share many common words and very similar meaning. Some sentences finish with \".\" but not all. In [4]: from keras.preprocessing.image import load_img , img_to_array npic = 5 npix = 224 target_size = ( npix , npix , 3 ) count = 1 fig = plt . figure ( figsize = ( 10 , 20 )) for jpgfnm in uni_filenames [: npic ]: filename = dir_Flickr_jpg + '/' + jpgfnm captions = list ( df_txt [ \"caption\" ] . loc [ df_txt [ \"filename\" ] == jpgfnm ] . values ) image_load = load_img ( filename , target_size = target_size ) ax = fig . add_subplot ( npic , 2 , count , xticks = [], yticks = []) ax . imshow ( image_load ) count += 1 ax = fig . add_subplot ( npic , 2 , count ) plt . axis ( 'off' ) ax . plot () ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , len ( captions )) for i , caption in enumerate ( captions ): ax . text ( 0 , i , caption , fontsize = 20 ) count += 1 plt . show () Data preparation We prepare text and image data separately. Text preparation We create a new dataframe dfword to visualize distribution of the words. It contains each word and its frequency in the entire tokens in decreasing order. In [5]: def df_word ( df_txt ): vocabulary = [] for txt in df_txt . caption . values : vocabulary . extend ( txt . split ()) print ( 'Vocabulary Size: %d ' % len ( set ( vocabulary ))) ct = Counter ( vocabulary ) dfword = pd . DataFrame ({ \"word\" : ct . keys (), \"count\" : ct . values ()}) dfword = dfword . sort ( \"count\" , ascending = False ) dfword = dfword . reset_index ()[[ \"word\" , \"count\" ]] return ( dfword ) dfword = df_word ( df_txt ) dfword . head ( 3 ) Vocabulary Size: 8918 Out[5]: word count 0 a 62989 1 . 36581 2 in 18975 The most and least frequently appearing words The most common words are articles such as \"a\", or \"the\", or punctuations. These words do not have much infomation about the data. In [6]: topn = 50 def plthist ( dfsub , title = \"The top 50 most frequently appearing words\" ): plt . figure ( figsize = ( 20 , 3 )) plt . bar ( dfsub . index , dfsub [ \"count\" ]) plt . yticks ( fontsize = 20 ) plt . xticks ( dfsub . index , dfsub [ \"word\" ], rotation = 90 , fontsize = 20 ) plt . title ( title , fontsize = 20 ) plt . show () plthist ( dfword . iloc [: topn ,:], title = \"The top 50 most frequently appearing words\" ) plthist ( dfword . iloc [ - topn :,:], title = \"The least 50 most frequently appearing words\" ) In order to clean the caption, I will create three functions that: remove punctuation remove single character remove numeric characters To see how these functions work, I will process a single example string using these three functions. In [7]: import string text_original = \"I ate 1000 apples and a banana. I have python v2.7. It's 2:30 pm. Could you buy me iphone7?\" print ( text_original ) print ( \" \\n Remove punctuations..\" ) def remove_punctuation ( text_original ): text_no_punctuation = text_original . translate ( None , string . punctuation ) return ( text_no_punctuation ) text_no_punctuation = remove_punctuation ( text_original ) print ( text_no_punctuation ) print ( \" \\n Remove a single character word..\" ) def remove_single_character ( text ): text_len_more_than1 = \"\" for word in text . split (): if len ( word ) > 1 : text_len_more_than1 += \" \" + word return ( text_len_more_than1 ) text_len_more_than1 = remove_single_character ( text_no_punctuation ) print ( text_len_more_than1 ) print ( \" \\n Remove words with numeric values..\" ) def remove_numeric ( text , printTF = False ): text_no_numeric = \"\" for word in text . split (): isalpha = word . isalpha () if printTF : print ( \" {:10} : {:}\" . format ( word , isalpha )) if isalpha : text_no_numeric += \" \" + word return ( text_no_numeric ) text_no_numeric = remove_numeric ( text_len_more_than1 , printTF = True ) print ( text_no_numeric ) I ate 1000 apples and a banana. I have python v2.7. It's 2:30 pm. Could you buy me iphone7? Remove punctuations.. I ate 1000 apples and a banana I have python v27 Its 230 pm Could you buy me iphone7 Remove a single character word.. ate 1000 apples and banana have python v27 Its 230 pm Could you buy me iphone7 Remove words with numeric values.. ate : True 1000 : False apples : True and : True banana : True have : True python : True v27 : False Its : True 230 : False pm : True Could : True you : True buy : True me : True iphone7 : False ate apples and banana have python Its pm Could you buy me Clean all captions Using the three functions, I will clean all captions. In [8]: def text_clean ( text_original ): text = remove_punctuation ( text_original ) text = remove_single_character ( text ) text = remove_numeric ( text ) return ( text ) for i , caption in enumerate ( df_txt . caption . values ): newcaption = text_clean ( caption ) df_txt [ \"caption\" ] . iloc [ i ] = newcaption After cleaning, the vocabularly size get reduced by about 200. In [9]: dfword = df_word ( df_txt ) plthist ( dfword . iloc [: topn ,:], title = \"The top 50 most frequently appearing words\" ) plthist ( dfword . iloc [ - topn :,:], title = \"The least 50 most frequently appearing words\" ) Vocabulary Size: 8763 Add start and end sequence tokens In [10]: from copy import copy def add_start_end_seq_token ( captions ): caps = [] for txt in captions : txt = 'startseq ' + txt + ' endseq' caps . append ( txt ) return ( caps ) df_txt0 = copy ( df_txt ) df_txt0 [ \"caption\" ] = add_start_end_seq_token ( df_txt [ \"caption\" ]) df_txt0 . head ( 5 ) del df_txt Image preparation Create features for each image using VGG16's pre-trained networks Read in the pre-trained network. Notice that this network takes input of size (224,224,3). The output layer contains 1,000 nodes. I downloaded weights from here to my local computer at ../output/ by: wget 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5' --no-check-certificate In [11]: from keras.applications import VGG16 modelvgg = VGG16 ( include_top = True , weights = None ) ## load the locally saved weights modelvgg . load_weights ( \"../output/vgg16_weights_tf_dim_ordering_tf_kernels.h5\" ) modelvgg . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 224, 224, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 _________________________________________________________________ predictions (Dense) (None, 1000) 4097000 ================================================================= Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 _________________________________________________________________ VGG16 is developed to classify images into 1,000 different classes. As I am not using VGG16 for the sake of the classification but I just need it for extracting features, I will remove the last layer from the network. In [12]: from keras import models modelvgg . layers . pop () modelvgg = models . Model ( inputs = modelvgg . inputs , outputs = modelvgg . layers [ - 1 ] . output ) ## show the deep learning model modelvgg . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 224, 224, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 ================================================================= Total params: 134,260,544 Trainable params: 134,260,544 Non-trainable params: 0 _________________________________________________________________ Images In [13]: from keras.preprocessing.image import load_img , img_to_array from keras.applications.vgg16 import preprocess_input from collections import OrderedDict images = OrderedDict () npix = 224 target_size = ( npix , npix , 3 ) data = np . zeros (( len ( jpgs ), npix , npix , 3 )) for i , name in enumerate ( jpgs ): # load an image from file filename = dir_Flickr_jpg + '/' + name image = load_img ( filename , target_size = target_size ) # convert the image pixels to a numpy array image = img_to_array ( image ) nimage = preprocess_input ( image ) y_pred = modelvgg . predict ( nimage . reshape ( ( 1 ,) + nimage . shape [: 3 ])) images [ name ] = y_pred . flatten () Visualization of the VGG16 features For each image, 4096 features are created. As I cannot visualize the 4096 dimensional space, I will create 2 dimentional representation of the space using PCA and visualize the distribution of the sample images. In [14]: from sklearn.decomposition import PCA encoder = np . array ( images . values ()) pca = PCA ( n_components = 2 ) y_pca = pca . fit_transform ( encoder ) Do photo features make sense? I manually selected similar images that are creating clusters. Namely I created red, green, magenta, blue, yellow and purple clusters. From each cluster, I plotted the original images. As you see, images from the same clusters tend to be very similar: red: many people are in one image green: dogs on green, yard or on bed magenta: dogs in snow or with water splash blue: guys doing sports with helmets yellow: not sure what these are?? I see many pictures are densely clustered around this area and Photo features seem to make sense! In [15]: ## some selected pictures that are creating clusters picked_pic = OrderedDict () picked_pic [ \"red\" ] = [ 1502 , 5298 , 2070 , 7545 , 1965 ] picked_pic [ \"green\" ] = [ 746 , 7627 , 6640 , 2733 , 4997 ] picked_pic [ \"magenta\" ] = [ 5314 , 5879 , 310 , 5303 , 3784 ] picked_pic [ \"blue\" ] = [ 4644 , 4209 , 7326 , 7290 , 4394 ] picked_pic [ \"yellow\" ] = [ 5895 , 9 , 27 , 62 , 123 ] picked_pic [ \"purple\" ] = [ 5087 ] fig , ax = plt . subplots ( figsize = ( 15 , 15 )) ax . scatter ( y_pca [:, 0 ], y_pca [:, 1 ], c = \"white\" ) for irow in range ( y_pca . shape [ 0 ]): ax . annotate ( irow , y_pca [ irow ,:], color = \"black\" , alpha = 0.5 ) for color , irows in picked_pic . items (): for irow in irows : ax . annotate ( irow , y_pca [ irow ,:], color = color ) ax . set_xlabel ( \"pca embedding 1\" , fontsize = 30 ) ax . set_ylabel ( \"pca embedding 2\" , fontsize = 30 ) plt . show () ## plot of images fig = plt . figure ( figsize = ( 16 , 20 )) count = 1 for color , irows in picked_pic . items (): for ivec in irows : name = jpgs [ ivec ] filename = dir_Flickr_jpg + '/' + name image = load_img ( filename , target_size = target_size ) ax = fig . add_subplot ( len ( picked_pic ), 5 , count , xticks = [], yticks = []) count += 1 plt . imshow ( image ) plt . title ( \"{} ({})\" . format ( ivec , color )) plt . show () Link the text and image data In this dataset, a single image has 5 captions. I will only use one caption out of 5 for simplicity. Each row of the dtexts and dimages contain the same info. Remove captions (or images) that do not have corresponding images (or captions). In [16]: dimages , keepindex = [],[] df_txt0 = df_txt0 . loc [ df_txt0 [ \"index\" ] . values == \"0\" ,: ] for i , fnm in enumerate ( df_txt0 . filename ): if fnm in images . keys (): dimages . append ( images [ fnm ]) keepindex . append ( i ) fnames = df_txt0 [ \"filename\" ] . iloc [ keepindex ] . values dcaptions = df_txt0 [ \"caption\" ] . iloc [ keepindex ] . values dimages = np . array ( dimages ) Tokenizer Change character vector to integer vector using Tokenizer In [17]: from keras.preprocessing.text import Tokenizer ## the maximum number of words in dictionary nb_words = 8000 tokenizer = Tokenizer ( nb_words = nb_words ) tokenizer . fit_on_texts ( dcaptions ) vocab_size = len ( tokenizer . word_index ) + 1 print ( \"vocabulary size : {}\" . format ( vocab_size )) dtexts = tokenizer . texts_to_sequences ( dcaptions ) print ( dtexts [: 5 ]) vocabulary size : 4476 [[1, 38, 3, 66, 144, 7, 124, 52, 406, 9, 367, 3, 24, 2351, 522, 2], [1, 12, 8, 5, 752, 8, 17, 368, 2], [1, 48, 15, 170, 3, 584, 101, 3, 41, 9, 551, 1198, 11, 55, 213, 3, 1076, 2], [1, 10, 621, 6, 150, 27, 23, 8, 101, 46, 112, 2], [1, 10, 3, 24, 82, 96, 1199, 19, 162, 2]] Split between training and testing data In [18]: prop_test , prop_val = 0.2 , 0.2 N = len ( dtexts ) Ntest , Nval = int ( N * prop_test ), int ( N * prop_val ) def split_test_val_train ( dtexts , Ntest , Nval ): return ( dtexts [: Ntest ], dtexts [ Ntest : Ntest + Nval ], dtexts [ Ntest + Nval :]) dt_test , dt_val , dt_train = split_test_val_train ( dtexts , Ntest , Nval ) di_test , di_val , di_train = split_test_val_train ( dimages , Ntest , Nval ) fnm_test , fnm_val , fnm_train = split_test_val_train ( fnames , Ntest , Nval ) The maximume length of captions In [19]: maxlen = np . max ([ len ( text ) for text in dtexts ]) The final preprocessing so that the data can be used as input and output of the Keras model. In [20]: from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical def preprocessing ( dtexts , dimages ): N = len ( dtexts ) print ( \"# captions/images = {}\" . format ( N )) assert ( N == len ( dimages )) Xtext , Ximage , ytext = [],[],[] for text , image in zip ( dtexts , dimages ): for i in range ( 1 , len ( text )): in_text , out_text = text [: i ], text [ i ] in_text = pad_sequences ([ in_text ], maxlen = maxlen ) . flatten () out_text = to_categorical ( out_text , num_classes = vocab_size ) Xtext . append ( in_text ) Ximage . append ( image ) ytext . append ( out_text ) Xtext = np . array ( Xtext ) Ximage = np . array ( Ximage ) ytext = np . array ( ytext ) print ( \" {} {} {}\" . format ( Xtext . shape , Ximage . shape , ytext . shape )) return ( Xtext , Ximage , ytext ) Xtext_train , Ximage_train , ytext_train = preprocessing ( dt_train , di_train ) Xtext_val , Ximage_val , ytext_val = preprocessing ( dt_val , di_val ) # pre-processing is not necessary for testing data #Xtext_test, Ximage_test, ytext_test = preprocessing(dt_test,di_test) # captions/images = 4855 (49631, 30) (49631, 4096) (49631, 4476) # captions/images = 1618 (16353, 30) (16353, 4096) (16353, 4476) Model This model takes two inputs: 4096-dimensional image features from pre-trained VGG model tokenized captions up to $t$th word. The single output is: tokenized $t+1$th word of caption Prediction Given the caption prediction up to the $t$th word, the model can predict the $t+1$st word in the caption, and then the input caption can be augmented with the predicted word to contain the caption up to the $t+1$th word. The augmented caption up to the $t+1$st word can, in turn, be used as input to predict the $t+2$nd word in caption. The process is repeated until the \"endseq\" is predicted. A bit more detail: [Image] 4096-dimensional image features from pre-trained VGG model The image feature is passed to fully connected layer with 256 hidden units. [Caption up to $t$] tokenized captions up to $t$th word. The tokenized caption up to $t$th time point is passed to embedding layer where each word is represented with a \"dim_embedding\" dimensional vector. This means that a single caption is represented as many time series of length \"max_len\". The time series are passed to LSTM with 256 hidden states, and then a single output at the final time point is passed to the higher layer. The networks are merged by simply adding the two vectors of size 256. The vector of length 256 is passed to two dense layers and the final dense layer return probability that the $t+1$st word is $k$th word in the vocabulary ($k=1,...,4476$). In [21]: from keras import layers print ( vocab_size ) ## image feature dim_embedding = 64 input_image = layers . Input ( shape = ( Ximage_train . shape [ 1 ],)) fimage = layers . Dense ( 256 , activation = 'relu' , name = \"ImageFeature\" )( input_image ) ## sequence model input_txt = layers . Input ( shape = ( maxlen ,)) ftxt = layers . Embedding ( vocab_size , dim_embedding , mask_zero = True )( input_txt ) ftxt = layers . LSTM ( 256 , name = \"CaptionFeature\" )( ftxt ) ## combined model for decoder decoder = layers . add ([ ftxt , fimage ]) decoder = layers . Dense ( 256 , activation = 'relu' )( decoder ) output = layers . Dense ( vocab_size , activation = 'softmax' )( decoder ) model = models . Model ( inputs = [ input_image , input_txt ], outputs = output ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' ) print ( model . summary ()) 4476 __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) (None, 30) 0 __________________________________________________________________________________________________ embedding_1 (Embedding) (None, 30, 64) 286464 input_3[0][0] __________________________________________________________________________________________________ input_2 (InputLayer) (None, 4096) 0 __________________________________________________________________________________________________ CaptionFeature (LSTM) (None, 256) 328704 embedding_1[0][0] __________________________________________________________________________________________________ ImageFeature (Dense) (None, 256) 1048832 input_2[0][0] __________________________________________________________________________________________________ add_1 (Add) (None, 256) 0 CaptionFeature[0][0] ImageFeature[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 256) 65792 add_1[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 4476) 1150332 dense_1[0][0] ================================================================================================== Total params: 2,880,124 Trainable params: 2,880,124 Non-trainable params: 0 __________________________________________________________________________________________________ None Model training In [22]: # fit model start = time . time () hist = model . fit ([ Ximage_train , Xtext_train ], ytext_train , epochs = 5 , verbose = 2 , batch_size = 64 , validation_data = ([ Ximage_val , Xtext_val ], ytext_val )) end = time . time () print ( \"TIME TOOK {:3.2f}MIN\" . format (( end - start ) / 60 )) Train on 49631 samples, validate on 16353 samples Epoch 1/5 - 60s - loss: 5.3167 - val_loss: 4.7659 Epoch 2/5 - 58s - loss: 4.3323 - val_loss: 4.4667 Epoch 3/5 - 58s - loss: 3.8796 - val_loss: 4.3870 Epoch 4/5 - 58s - loss: 3.5683 - val_loss: 4.3412 Epoch 5/5 - 58s - loss: 3.3066 - val_loss: 4.4118 TIME TOOK 4.90MIN In [23]: print ( Ximage_train . shape , Xtext_train . shape , ytext_train . shape ) ((49631, 4096), (49631, 30), (49631, 4476)) Validation loss and training loss over epochs The model over fit very quickly. This makes sense considering the small size of our data. In [24]: for label in [ \"loss\" , \"val_loss\" ]: plt . plot ( hist . history [ label ], label = label ) plt . legend () plt . xlabel ( \"epochs\" ) plt . ylabel ( \"loss\" ) plt . show () Prediction Prediction of testing image makes sense! In [25]: index_word = dict ([( index , word ) for word , index in tokenizer . word_index . items ()]) def predict_caption ( image ): ''' image.shape = (1,4462) ''' in_text = 'startseq' for iword in range ( maxlen ): sequence = tokenizer . texts_to_sequences ([ in_text ])[ 0 ] sequence = pad_sequences ([ sequence ], maxlen ) yhat = model . predict ([ image , sequence ], verbose = 0 ) yhat = np . argmax ( yhat ) newword = index_word [ yhat ] in_text += \" \" + newword if newword == \"endseq\" : break return ( in_text ) npic = 5 npix = 224 target_size = ( npix , npix , 3 ) count = 1 fig = plt . figure ( figsize = ( 10 , 20 )) for jpgfnm , image_feature in zip ( fnm_test [: npic ], di_test [: npic ]): ## images filename = dir_Flickr_jpg + '/' + jpgfnm image_load = load_img ( filename , target_size = target_size ) ax = fig . add_subplot ( npic , 2 , count , xticks = [], yticks = []) ax . imshow ( image_load ) count += 1 ## captions caption = predict_caption ( image_feature . reshape ( 1 , len ( image_feature ))) ax = fig . add_subplot ( npic , 2 , count ) plt . axis ( 'off' ) ax . plot () ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , 1 ) ax . text ( 0 , 0.5 , caption , fontsize = 20 ) count += 1 plt . show () Bilingual evaluation understudy (BLEU) I want to evaluate the model performance in the prediction. Is there any good metric? BLEU is a well-acknowledged metric to measure the similarly of one hypothesis sentence to multiple reference sentences. Given a single hypothesis sentence and multiple reference sentences, it returns value between 0 and 1. The metric close to 1 means that the two are very similar. The metric was introduced in 2002 BLEU: a Method for Automatic Evaluation of Machine Translation . Although there are many problems in this metric, for example grammatical correctness are not taken into account, BLEU is very well accepted partly because it is easy to calculate. Basic idea of BLEU The authors of the paper say that: The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is. For example let's consider the scenario where I want to compare the similarly between the hypothesis sentence and the reference sentence. The hypothesis sentence is I like dogs and the reference sentence is: I do like dogs From the hypothesis sentence, I can create 2 2-grams: (I, like) (like, dogs) From the reference sentence, I can create 3 2-grams: (I, do) (do, like) (like, dogs) Which 2-gram from hypothesis exists in reference? The reference sentence has (like, dog) that is one of the 2-gram from hypothesis. This means that out of the two 2-grams from hypothesis, 1 exists in the reference i.e., 50% of 2-gram from the hypothesis exists in reference. This 50% is what's called \"modified precision\". This modified precision has the problem that when the number of words is larger than the hypothesis length, then the modified precision tends to be larger. For example, if the reference sentence contains 10000 words and its sub-sentence happened to contain \"I do like dogs\" sentence, this also gives the modified precision of 50%. However, the reference sentence and the hypothesis sentence should be considered less similar. To account for this, BLEU calculation penalizes the long reference sentence. The penalty is simply $$ exp\\left( 1- \\frac{\\textrm{lengh of reference} }{\\textrm{length of hypothesis} } \\right) $$ Modified precision is multiplied by this reference factor to yield 2-gram BLEU. In this particular example, the penalty is $exp(1 - 4/3)=0.717$ so the 2-gram BLEU is 0.717*0.5 = 0.3585. Finally, 1-gram BLEU, 2-gram BLEU, 3-gram BLEU and 4-gram BLEU are calculated and its average is reported as BLEU. Implementation BLEU score calculation is implemented in nltk.util and the source code is available . I found this source code is the easiest way to understand this metric. So I will go over the script to understand how the calculation works. Let's introduce one hypothesis string and one reference string, and see how similar they are according to BLEU. In [26]: hypothesis = \"I like dog\" hypothesis = hypothesis . split () reference = \"I do like dog\" references = [ reference . split ()] ## references must be a list containing list. According to BLEU the two sentences are reasonably similar. In [27]: from nltk.translate.bleu_score import sentence_bleu print ( \"BLEU={:4.3f}\" . format ( sentence_bleu ( references , hypothesis ))) BLEU=0.603 If I change the hypothesis sentence a bit, this worsen the BLEU. In [28]: hypothesis2 = \"I love dog!\" . split () print ( \"BLEU={:4.3f}\" . format ( sentence_bleu ( references , hypothesis2 ))) BLEU=0.544 2-gram BLEU calculation step by step The main calculation of BLEU is done in modified_precision(references, hypothesis, n) . So Let's try to understand this method. As the first step, given \"n\" of n-gram, compute n-gram and count the frequency of each n-gram. In [29]: from nltk.util import ngrams n = 2 # Extracts all ngrams in hypothesis # Set an empty Counter if hypothesis is empty. counts = Counter ( ngrams ( hypothesis , n )) if len ( hypothesis ) >= n else Counter () # Extract a union of references' counts. counts Out[29]: Counter({('I', 'like'): 1, ('like', 'dog'): 1}) max_counts is a dictionary containing the same key as counts; i.e. the n-gram from the hypothesis is a key. It records how many of the ngram from hypothesis exists in each of the reference sentences. In [30]: max_counts = {} for reference in references : reference_counts = Counter ( ngrams ( reference , n )) if len ( reference ) >= n else Counter () for ngram in counts : ## ngram from hypothesis max_counts [ ngram ] = max ( max_counts . get ( ngram , 0 ), reference_counts [ ngram ]) max_counts Out[30]: {('I', 'like'): 0, ('like', 'dog'): 1} Modified precision is: In [31]: # Assigns the intersection between hypothesis and references' counts. clipped_counts = { ngram : min ( count , max_counts [ ngram ]) for ngram , count in counts . items ()} numerator = sum ( clipped_counts . values ()) # Ensures that denominator is minimum 1 to avoid ZeroDivisionError. # Usually this happens when the ngram order is > len(reference). denominator = max ( 1 , sum ( counts . values ())) modified_precision = numerator / float ( denominator ) print ( modified_precision ) 0.5 Compute the penalty: In [32]: ref_len = len ( reference ) hyp_len = float ( len ( hypothesis )) brevity_penalty = np . exp ( 1 - ref_len / hyp_len ) print ( \"reference length = {:1.0f}, hypothesis length = {:1.0f}, penalty = {:4.3f}\" . format ( ref_len , hyp_len , brevity_penalty )) reference length = 4, hypothesis length = 3, penalty = 0.717 2-gram BLEU is: In [33]: brevity_penalty * modified_precision Out[33]: 0.35826565528689464 We can also calculate the 2-gram BLEU using the sentence_bleu function by setting only the second position in weight as 1. In [34]: print ( \"2-gram result:{}\" . format ( sentence_bleu ( references , hypothesis , weights = [ 0 , 1 , 0 , 0 ]))) 2-gram result:0.358265655287 Back to image captioning problem Now we understand what BLEU does and we are ready to calculate the BLEU for our test set. In [35]: index_word = dict ([( index , word ) for word , index in tokenizer . word_index . items ()]) nkeep = 5 pred_good , pred_bad , bleus = [], [], [] count = 0 for jpgfnm , image_feature , tokenized_text in zip ( fnm_test , di_test , dt_test ): count += 1 if count % 200 == 0 : print ( \" {:4.2f} % i s done..\" . format ( 100 * count / float ( len ( fnm_test )))) caption_true = [ index_word [ i ] for i in tokenized_text ] caption_true = caption_true [ 1 : - 1 ] ## remove startreg, and endreg ## captions caption = predict_caption ( image_feature . reshape ( 1 , len ( image_feature ))) caption = caption . split () caption = caption [ 1 : - 1 ] ## remove startreg, and endreg bleu = sentence_bleu ([ caption_true ], caption ) bleus . append ( bleu ) if bleu > 0.7 and len ( pred_good ) < nkeep : pred_good . append (( bleu , jpgfnm , caption_true , caption )) elif bleu < 0.3 and len ( pred_bad ) < nkeep : pred_bad . append (( bleu , jpgfnm , caption_true , caption )) 12.36% is done.. 24.72% is done.. 37.08% is done.. 49.44% is done.. 61.80% is done.. 74.17% is done.. 86.53% is done.. 98.89% is done.. The mean BLEU value for testing data In [36]: print ( \"Mean BLEU {:4.3f}\" . format ( np . mean ( bleus ))) Mean BLEU 0.370 Plot the images with good captions (BLEU > 0.9) and bad captions (BLEU < 0.1) In [37]: def plot_images ( pred_bad ): def create_str ( caption_true ): strue = \"\" for s in caption_true : strue += \" \" + s return ( strue ) npix = 224 target_size = ( npix , npix , 3 ) count = 1 fig = plt . figure ( figsize = ( 10 , 20 )) npic = len ( pred_bad ) for pb in pred_bad : bleu , jpgfnm , caption_true , caption = pb ## images filename = dir_Flickr_jpg + '/' + jpgfnm image_load = load_img ( filename , target_size = target_size ) ax = fig . add_subplot ( npic , 2 , count , xticks = [], yticks = []) ax . imshow ( image_load ) count += 1 caption_true = create_str ( caption_true ) caption = create_str ( caption ) ax = fig . add_subplot ( npic , 2 , count ) plt . axis ( 'off' ) ax . plot () ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , 1 ) ax . text ( 0 , 0.7 , \"true:\" + caption_true , fontsize = 20 ) ax . text ( 0 , 0.4 , \"pred:\" + caption , fontsize = 20 ) ax . text ( 0 , 0.1 , \"BLEU: {}\" . format ( bleu ), fontsize = 20 ) count += 1 plt . show () print ( \"Bad Caption\" ) plot_images ( pred_bad ) print ( \"Good Caption\" ) plot_images ( pred_good ) Bad Caption Good Caption if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Develop an image captioning deep learning model using Flickr 8K data"},{"url":"Stateful-LSTM-model-training-in-Keras.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In the previous post, titled Extract weights from Keras's LSTM and calcualte hidden and cell states , I discussed LSTM model. In this blog post, I would like to discuss the stateful flag in Keras's recurrent model. If you google \"stateful LSTM\" or \"stateful RNN\", google shows many blog posts discussing and puzzling about this notorious parameter, e.g.: Don't use stateful LSTM unless you know what it does Simple stateful LSTM example Keras - stateful vs stateless LSTMs Convert LSTM model from stateless to stateful I hope to give some understanding of stateful prediction through this blog. Stateful flag is Keras All the RNN or LSTM models are stateful in theory . These models are meant to remember the entire sequence for prediction or classification tasks. However, in practice, you need to create a batch to train a model with backprogation algorithm, and the gradient can't backpropagate between batches. This means that if you have a long time series which does not fit into a single batch, you need to divide the time series into multiple sub-time series and each sub time series goes to separate batch. Then LSTM only remember what happened within a batch. At the initial time point of every batch, states are initialized and set to 0. No previous information. This is very unfortunate because RNN or LSTM are introduced to remember all the past history to predict the next time point. Nope, it only remembers what happened within the batch time series length! Stateful flag in Keras is introduced to circumvent these problems during training, and make the model remember what happened in the previous batch by passing states from the previous batch to the next batch. Also, stateful = True makes a lot of sense during the prediction phase because otherwise the RNN trained with batch time series length $T$ assumes that the hidden states are initialized to zero at every $T$ steps. This means that in order to predict the value at time point $t$, we need to feed foward the network assuming that $h_{t-T}=0$ for $T$ times and in order to predict the value at time point $t+1$, we AGAIN need to feed forward the network for $T$ times assuming that $h_{t-T+1}=0$. This point was discussed greatly in my previous blog. History of Stateful flag Training with Stateful = True Here, I introduce two nice blogs discussing stateful flag. Philippe Remy's blog post provided nice introduction to understand the stateful flag of Keras's RNN model. It discusses how to train a model with stateful = True. However, although I like this blog post, it also contributed for the confusion because the example uses batch_input_shape=(1, 1, 1) which simply does not work for most of the time series examples even when you train a model with stateful = True. This means that your batch only contains a single time point from a single time series. Jason Brownlee's blog is another great blog post that gives good stateful examples. Jason says: \" [Stateful = True] is truly how the LSTM networks are intended to be used. We find that by allowing the network itself to learn the dependencies between the characters, that we need a smaller network (half the number of units) and fewer training epochs (almost half). \" As he only try fitting stateful LSTM to a small simple example, I cannot generalize his comment, but it is nice to learn that training with stateful=True works for his example. In this blog post, I would like to also discuss the training with stateful = True. Training with Stateful = False and Prediction with Stateful = True. The usefulness of stateful method during the prediction is also discussed. There are lots of discussions online that try to use stateful = True only during the prediction phase: \" Why anybody would want to have a stateful model during training is beyond me, so this part I can agree with. But during testing, when you want to let the model predict some output on some data, then stateful makes a lot more sense. For example, it might be a part of a larger system that works on video frames. It might be required to perform some action instantly after each frame, instead of waiting for a sufficiently long sequence of video frames before being fed to the network. It would be really nice if you could train the network stateless with a time-depth of X (say 16), and then use those weights on a stateful network with a time-depth of 1 during prediction. In my experience however, this does not work in Keras. \" -- ahrnbom commented on Sep 19 2017 \" The length of my input sequences is variable and has a quite high variance (from very short sequences to nearly 1000 long sequences). At training time I can just divide the sequences in batches of fixed sizes but at test time it would be useful to feed the whole sequence and possibly get prediction at every time step.Is it possible to train the model in stateless mode feeding fixed length sequences and then make predictions in a stateful fashion? Does this even make sense or am I missing something? \"-- fedebecat commented on Mar 11 2017 I was also fascinated with these ideas. In fact, writing scripts for stateful training is a bit cumbersome because you have to reset sequence by yourself. However, after what I have seen in my previous post titled Understand Keras's RNN behind the scenes with a sin wave example - Stateful and Stateless prediction - , I am very skeptical about this. Remember, stateful prediction and stateless prediction returns different results when model is trained stateless! And it was such a simple data (sin wave). Therefore, in this blog post, I will train model in stateful setting and show how the results are different from a model trained in stateless setting. Reference in this blog Understand Keras's RNN behind the scenes with a sin wave example - Stateful and Stateless prediction - Extract weights from Keras's LSTM and calcualte hidden and cell states In [121]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys , time import numpy as np import seaborn as sns import warnings warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 I will create a synthetic long time series data just as in Extract weights from Keras's LSTM and calcualte hidden and cell states . But this time, I will make the data more complex by using much larger $D$. Create synthetic long time series data I will generate a time series $X_{t}$ ($t=1,...,T$) as my independent feature. As the target, or dependent time series, I will create a time series $Y_{t}$ as a function of a single time series $\\{ X_{k} \\}_{k=1}&#94;t$. Given integers $D$ and $T$, the time series $X_{t}$ an $Y_t$ are generated as: $$ C \\sim \\textrm{Multinomial}(5,6,...,99)\\\\ U \\sim \\textrm{Unif}([0,1])\\\\ X_{t} = -\\frac{t}{T}U\\left[ \\textrm{Cos}\\left(\\frac{t}{1+C}\\right) \\right]\\\\ Y_{t} = X_{t-2} X_{t-\\textrm{D}} \\textrm{ for t > D else } Y_t = 0 $$ We consider a long time series $T = 1,000$. The parameter $D$ determines the time lag. The larger $D$ is, the longer it takes for the $Y_{t}$ to show the effect of $X_{t}$, and the longer memory that the deep learning model needs to remember. For this exercise, I will consider $D=100$, and generate 1,000 sets of time series, independently. Generate training data In [122]: def random_sample ( len_ts = 3000 , D = 1001 ): c_range = range ( 5 , 100 ) c1 = np . random . choice ( c_range ) u = np . random . random ( 1 ) const = - 1.0 / len_ts ts = np . arange ( 0 , len_ts ) x1 = np . cos ( ts / float ( 1.0 + c1 )) x1 = x1 * ts * u * const y1 = np . zeros ( len_ts ) for t in range ( D , len_ts ): ## the output time series depend on input as follows: y1 [ t ] = x1 [ t - 2 ] * x1 [ t - D ] y = np . array ([ y1 ]) . T X = np . array ([ x1 ]) . T return y , X def generate_data ( D = 1001 , Nsequence = 1000 , T = 4000 , seed = 123 ): X_train = [] y_train = [] set_seed ( sd = seed ) for isequence in range ( Nsequence ): y , X = random_sample ( T , D = D ) X_train . append ( X ) y_train . append ( y ) return np . array ( X_train ), np . array ( y_train ) D = 100 T = 1000 X , y = generate_data ( D = D , T = T , Nsequence = 1000 ) print ( X . shape , y . shape ) ((1000, 1000, 1), (1000, 1000, 1)) Plot examples of the generated time series Notice that in every time series, the later 500 seconds are more variable. In [160]: def plot_examples ( X , y , ypreds = None , nm_ypreds = None ): fig = plt . figure ( figsize = ( 16 , 10 )) fig . subplots_adjust ( hspace = 0.32 , wspace = 0.15 ) count = 1 n_ts = 16 for irow in range ( n_ts ): ax = fig . add_subplot ( n_ts / 4 , 4 , count ) ax . set_ylim ( - 0.5 , 0.5 ) ax . plot ( X [ irow ,:, 0 ], \"--\" , label = \"x1\" ) ax . plot ( y [ irow ,:,:], label = \"y\" , linewidth = 3 , alpha = 0.5 ) ax . set_title ( \"{:}th time series sample\" . format ( irow )) if ypreds is not None : for ypred , nm in zip ( ypreds , nm_ypreds ): ax . plot ( ypred [ irow ,:,:], label = nm ) count += 1 plt . legend () plt . show () if ypreds is not None : for y_pred , nm_ypred in zip ( ypreds , nm_ypreds ): loss = np . mean ( ( y_pred [:, D :,:] . flatten () - y [:, D :,:] . flatten ()) ** 2 ) print ( \"The final validation loss of {} is {:7.6f}\" . format ( nm_ypred , loss )) plot_examples ( X , y , ypreds = None , nm_ypreds = None ) Split between training and testing Define weights just like Extract weights from Keras's LSTM and calcualte hidden and cell states . In [124]: prop_train = 0.8 ntrain = int ( X . shape [ 0 ] * prop_train ) w = np . zeros ( y . shape [: 2 ]) w [:, D :] = 1 w_train = w X_train , X_val = X [: ntrain ], X [ ntrain :] y_train , y_val = y [: ntrain ], y [ ntrain :] w_train , w_val = w [: ntrain ], w [ ntrain :] Define LSTM model (stateful and stateless) Now I define stateful LSTM model. In [125]: from keras import models , layers def define_model ( len_ts , hidden_neurons = 10 , nfeature = 1 , batch_size = None , stateful = False ): in_out_neurons = 1 inp = layers . Input ( batch_shape = ( batch_size , len_ts , nfeature ), name = \"input\" ) rnn = layers . LSTM ( hidden_neurons , return_sequences = True , stateful = stateful , name = \"RNN\" )( inp ) dens = layers . TimeDistributed ( layers . Dense ( in_out_neurons , name = \"dense\" ))( rnn ) model = models . Model ( inputs = [ inp ], outputs = [ dens ]) model . compile ( loss = \"mean_squared_error\" , sample_weight_mode = \"temporal\" , optimizer = \"rmsprop\" ) return ( model ,( inp , rnn , dens )) Stateless model as a reference Define a model. In the Extract weights from Keras's LSTM and calcualte hidden and cell states , the number of hidden units was as less as 3. However, this time, the dependencies between $Y_t$ and $X_t$ are more complex because of the magnitude of $D$. The LSTM model needs to remember longer history of $X_t$. In [152]: hunits = 64 model_stateless , _ = define_model ( hidden_neurons = hunits , len_ts = X_train . shape [ 1 ]) model_stateless . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (None, 1000, 1) 0 _________________________________________________________________ RNN (LSTM) (None, 1000, 64) 16896 _________________________________________________________________ time_distributed_18 (TimeDis (None, 1000, 1) 65 ================================================================= Total params: 16,961 Trainable params: 16,961 Non-trainable params: 0 _________________________________________________________________ Model training In [153]: start = time . time () history = model_stateless . fit ( X_train , y_train , batch_size = 400 , epochs = 100 , verbose = 2 , sample_weight = w_train , validation_data = ( X_val , y_val , w_val )) end = time . time () print ( \"Time Took :{:3.2f} min\" . format ( ( end - start ) / 60 )) Train on 800 samples, validate on 200 samples Epoch 1/100 - 6s - loss: 0.0076 - val_loss: 0.0072 Epoch 2/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 3/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 4/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 5/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 6/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 7/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 8/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 9/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 10/100 - 3s - loss: 0.0070 - val_loss: 0.0071 Epoch 11/100 - 4s - loss: 0.0069 - val_loss: 0.0071 Epoch 12/100 - 4s - loss: 0.0069 - val_loss: 0.0071 Epoch 13/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 14/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 15/100 - 4s - loss: 0.0069 - val_loss: 0.0073 Epoch 16/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 17/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 18/100 - 4s - loss: 0.0069 - val_loss: 0.0071 Epoch 19/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 20/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 21/100 - 4s - loss: 0.0070 - val_loss: 0.0071 Epoch 22/100 - 4s - loss: 0.0069 - val_loss: 0.0071 Epoch 23/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 24/100 - 3s - loss: 0.0069 - val_loss: 0.0072 Epoch 25/100 - 3s - loss: 0.0069 - val_loss: 0.0071 Epoch 26/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 27/100 - 4s - loss: 0.0069 - val_loss: 0.0070 Epoch 28/100 - 3s - loss: 0.0067 - val_loss: 0.0071 Epoch 29/100 - 4s - loss: 0.0066 - val_loss: 0.0069 Epoch 30/100 - 4s - loss: 0.0063 - val_loss: 0.0068 Epoch 31/100 - 4s - loss: 0.0067 - val_loss: 0.0070 Epoch 32/100 - 4s - loss: 0.0067 - val_loss: 0.0064 Epoch 33/100 - 4s - loss: 0.0060 - val_loss: 0.0060 Epoch 34/100 - 4s - loss: 0.0058 - val_loss: 0.0062 Epoch 35/100 - 3s - loss: 0.0062 - val_loss: 0.0059 Epoch 36/100 - 4s - loss: 0.0057 - val_loss: 0.0056 Epoch 37/100 - 4s - loss: 0.0055 - val_loss: 0.0059 Epoch 38/100 - 4s - loss: 0.0065 - val_loss: 0.0062 Epoch 39/100 - 3s - loss: 0.0058 - val_loss: 0.0056 Epoch 40/100 - 4s - loss: 0.0054 - val_loss: 0.0053 Epoch 41/100 - 4s - loss: 0.0053 - val_loss: 0.0070 Epoch 42/100 - 4s - loss: 0.0072 - val_loss: 0.0062 Epoch 43/100 - 4s - loss: 0.0058 - val_loss: 0.0048 Epoch 44/100 - 4s - loss: 0.0048 - val_loss: 0.0046 Epoch 45/100 - 4s - loss: 0.0048 - val_loss: 0.0059 Epoch 46/100 - 3s - loss: 0.0056 - val_loss: 0.0044 Epoch 47/100 - 4s - loss: 0.0045 - val_loss: 0.0046 Epoch 48/100 - 4s - loss: 0.0048 - val_loss: 0.0052 Epoch 49/100 - 4s - loss: 0.0052 - val_loss: 0.0047 Epoch 50/100 - 4s - loss: 0.0052 - val_loss: 0.0046 Epoch 51/100 - 3s - loss: 0.0045 - val_loss: 0.0044 Epoch 52/100 - 4s - loss: 0.0048 - val_loss: 0.0051 Epoch 53/100 - 4s - loss: 0.0052 - val_loss: 0.0042 Epoch 54/100 - 4s - loss: 0.0047 - val_loss: 0.0040 Epoch 55/100 - 3s - loss: 0.0043 - val_loss: 0.0050 Epoch 56/100 - 3s - loss: 0.0049 - val_loss: 0.0041 Epoch 57/100 - 4s - loss: 0.0047 - val_loss: 0.0042 Epoch 58/100 - 4s - loss: 0.0048 - val_loss: 0.0043 Epoch 59/100 - 4s - loss: 0.0044 - val_loss: 0.0046 Epoch 60/100 - 4s - loss: 0.0048 - val_loss: 0.0046 Epoch 61/100 - 4s - loss: 0.0044 - val_loss: 0.0040 Epoch 62/100 - 4s - loss: 0.0050 - val_loss: 0.0045 Epoch 63/100 - 4s - loss: 0.0045 - val_loss: 0.0037 Epoch 64/100 - 4s - loss: 0.0042 - val_loss: 0.0053 Epoch 65/100 - 4s - loss: 0.0052 - val_loss: 0.0034 Epoch 66/100 - 4s - loss: 0.0037 - val_loss: 0.0033 Epoch 67/100 - 4s - loss: 0.0045 - val_loss: 0.0043 Epoch 68/100 - 4s - loss: 0.0048 - val_loss: 0.0034 Epoch 69/100 - 4s - loss: 0.0039 - val_loss: 0.0048 Epoch 70/100 - 4s - loss: 0.0044 - val_loss: 0.0029 Epoch 71/100 - 4s - loss: 0.0034 - val_loss: 0.0050 Epoch 72/100 - 4s - loss: 0.0060 - val_loss: 0.0054 Epoch 73/100 - 4s - loss: 0.0053 - val_loss: 0.0047 Epoch 74/100 - 4s - loss: 0.0048 - val_loss: 0.0042 Epoch 75/100 - 4s - loss: 0.0043 - val_loss: 0.0037 Epoch 76/100 - 4s - loss: 0.0039 - val_loss: 0.0035 Epoch 77/100 - 4s - loss: 0.0041 - val_loss: 0.0043 Epoch 78/100 - 4s - loss: 0.0047 - val_loss: 0.0037 Epoch 79/100 - 4s - loss: 0.0040 - val_loss: 0.0032 Epoch 80/100 - 4s - loss: 0.0035 - val_loss: 0.0031 Epoch 81/100 - 4s - loss: 0.0037 - val_loss: 0.0039 Epoch 82/100 - 4s - loss: 0.0048 - val_loss: 0.0040 Epoch 83/100 - 4s - loss: 0.0039 - val_loss: 0.0026 Epoch 84/100 - 4s - loss: 0.0030 - val_loss: 0.0026 Epoch 85/100 - 3s - loss: 0.0032 - val_loss: 0.0038 Epoch 86/100 - 4s - loss: 0.0046 - val_loss: 0.0032 Epoch 87/100 - 4s - loss: 0.0034 - val_loss: 0.0028 Epoch 88/100 - 4s - loss: 0.0034 - val_loss: 0.0051 Epoch 89/100 - 4s - loss: 0.0047 - val_loss: 0.0034 Epoch 90/100 - 4s - loss: 0.0035 - val_loss: 0.0029 Epoch 91/100 - 4s - loss: 0.0036 - val_loss: 0.0029 Epoch 92/100 - 4s - loss: 0.0035 - val_loss: 0.0050 Epoch 93/100 - 4s - loss: 0.0044 - val_loss: 0.0027 Epoch 94/100 - 4s - loss: 0.0031 - val_loss: 0.0027 Epoch 95/100 - 4s - loss: 0.0034 - val_loss: 0.0030 Epoch 96/100 - 4s - loss: 0.0033 - val_loss: 0.0027 Epoch 97/100 - 4s - loss: 0.0033 - val_loss: 0.0036 Epoch 98/100 - 4s - loss: 0.0047 - val_loss: 0.0035 Epoch 99/100 - 4s - loss: 0.0035 - val_loss: 0.0025 Epoch 100/100 - 4s - loss: 0.0032 - val_loss: 0.0035 Time Took :6.36 min Validation and training loss In [154]: for label in [ \"loss\" , \"val_loss\" ]: plt . plot ( histroy . history [ label ], label = label ) plt . legend () plt . show () Generate testing data In [145]: X_test , y_test = generate_data ( D = D , T = T , Nsequence = 5000 ) In [161]: y_pred_stateless = model_stateless . predict ( X_test ) plot_examples ( X_test , y_test , ypreds = [ y_pred_stateless ], nm_ypreds = [ \"y_pred stateless\" ]) The final validation loss of y_pred stateless is 0.004058 Stateful Model Training The stateful model gives flexibility of resetting states so you can pass states from batch to batch. However, as a consequence, stateful model requires some book keeping during the training: a set of original time series needs to be trained in the sequential manner and you need to specify when the batch with new sequence starts. For example, consider the scenario: batch_size = 250, time_series_length_in_batch = 500. Then the original 250 time series of length 1,000 sec are divided into two groups: the first 500 sec of all the 250 time series goes to batch 1 and the remaining 500 sec of all the 250 time series goes to the batch 2. Batch 3 will contain the first 500 sec of the next 250 time series and the remaining 500 sec goes to the batch 4. As batch 2 contains the continuation of time series in batch 1, states at the 500th time point from batch 1 needs to be passed to the states in batch 2. On the other hand, batch 3 contains completely new time series so states should not be passed from batch 2 to batch 3. [batch 1]--> pass states --> [batch 2] --> reset states [batch 3]--> pass states --> [batch 4] --> reset states [batch 5]--> pass states --> [batch 6] --> reset states [batch 7]--> pass states --> [batch 8] --> reset states In this formulation, the random batch creation has some constraint: sub time series in batch 2 cannot be trained with sub time series in batch 1. And batch 1 needs to be trained before batch 3. The following class define the methods for training procedure. (Note that this training script save the weights at every back propagation for the sake of algorithm convergence analysis.) In [131]: class statefulModel ( object ): def __init__ ( self , model , print_val_every = 500 ): ''' model must be stateful keras model object batch_input_shape must be specified ''' bis = model . layers [ 0 ] . get_config ()[ \"batch_input_shape\" ] print ( \"batch_input_shape={}\" . format ( bis )) self . batch_size = bis [ 0 ] self . ts = bis [ 1 ] self . Nfeat = bis [ 2 ] self . model = model self . print_val_every = print_val_every def get_mse ( self , true , est , w = None ): ''' calculate MSE for weights == 1 ''' if w is None : w = np . zeros ( true . shape ) w [:] = 1 ytrue = true [ w == 1 ] . flatten () yest = est [ w == 1 ] . flatten () SSE = np . sum (( ytrue - yest ) ** 2 ) N = np . sum ( w == 1 ) MSE = SSE / N return MSE , ( SSE , N ) def X_val_shape_adj ( self , X , X_val_orig , y_val_orig , w_val_orig ): ''' Make the dimension of X_val the same as the dimension of X by adding zeros. It is assumed that: X_val.shape[i] < X.shape[i] i = 0, 1, 2 ''' X_val = np . zeros ( X . shape ) X_val [: X_val_orig . shape [ 0 ]] = X_val_orig myshape = list ( y_val_orig . shape ) myshape [ 0 ] = X . shape [ 0 ] y_val = np . zeros ( myshape ) y_val [: y_val_orig . shape [ 0 ]] = y_val_orig myshape = list ( w_val_orig . shape ) myshape [ 0 ] = X . shape [ 0 ] w_val = np . zeros ( myshape ) w_val [: w_val_orig . shape [ 0 ]] = w_val_orig return X_val , y_val , w_val def train1epoch ( self , X , y , w , epoch = None ): ''' devide the training set of time series into batches. ''' print \" Training..\" batch_index = np . arange ( X . shape [ 0 ]) ## shuffle to create batch containing different time series np . random . shuffle ( batch_index ) count = 1 for ibatch in range ( self . batch_size , X . shape [ 0 ] + 1 , self . batch_size ): print \" Batch {:02d}\" . format ( count ) pick = batch_index [( ibatch - self . batch_size ): ibatch ] if len ( pick ) < self . batch_size : continue X_batch = X [ pick ] y_batch = y [ pick ] w_batch = w [ pick ] self . fit_across_time ( X_batch , y_batch , w_batch , epoch , ibatch ) count += 1 def fit_across_time ( self , X , y , w , epoch = None , ibatch = None ): ''' training for the given set of time series It always starts at the time point 0 so we need to reset states to zero. ''' self . model . reset_states () for itime in range ( self . ts , X . shape [ 1 ] + 1 , self . ts ): ## extract sub time series Xtime = X [:, itime - self . ts : itime ,:] ytime = y [:, itime - self . ts : itime ,:] wtime = w [:, itime - self . ts : itime ] if np . all ( wtime == 0 ): continue val = self . model . fit ( Xtime , ytime , nb_epoch = 1 , ## no shuffling across rows (i.e. time series) shuffle = False , ## use all the samples in one epoch batch_size = X . shape [ 0 ], sample_weight = wtime , verbose = False ) if itime % self . print_val_every == 0 : print \" {start:4d}:{end:4d} loss={val:.3f}\" . format ( start = itime - self . ts , end = itime , val = val . history [ \"loss\" ][ 0 ]) sys . stdout . flush () ## uncomment below if you do not want to save weights for every epoch every batch and every time if epoch is not None : self . model . save_weights ( \"weights_epoch{:03d}_batch{:01d}_time{:04d}.hdf5\" . format ( epoch , ibatch , itime )) def validate1epoch ( self , X_val_adj , y_val_adj , w_val_adj ): batch_index = np . arange ( X_val_adj . shape [ 0 ]) print \" Validating..\" val_loss = 0 count = 1 for ibatch in range ( self . batch_size , X_val_adj . shape [ 0 ] + 1 , self . batch_size ): pick = batch_index [( ibatch - self . batch_size ): ibatch ] if len ( pick ) < self . batch_size : continue X_val_adj_batch = X_val_adj [ pick ] y_val_adj_batch = y_val_adj [ pick ] w_val_adj_batch = w_val_adj [ pick ] if np . all ( w_val_adj_batch == 0 ): continue print \" Batch {}\" . format ( count ) SSE , N = self . validate_across_time ( X_val_adj_batch , y_val_adj_batch , w_val_adj_batch ) val_loss += SSE count += N val_loss /= count return val_loss def validate_across_time ( self , X_val_adj , y_val_adj , w_val_adj ): y_pred_adj = np . zeros ( y_val_adj . shape ) y_pred_adj [:] = np . NaN self . model . reset_states () for itime in range ( self . ts , X_val_adj . shape [ 1 ] + 1 , self . ts ): y_pred_adj [:, itime - self . ts : itime ,:] = self . model . predict ( X_val_adj [:, itime - self . ts : itime ,:], batch_size = X_val_adj . shape [ 0 ]) loss , _ = self . get_mse ( y_pred_adj [:, itime - self . ts : itime ,:], y_val_adj [:, itime - self . ts : itime ,:], w_val_adj [:, itime - self . ts : itime ]) if itime % self . print_val_every == 0 : print \" {start:4d}:{end:4d} val_loss={a:.3f}\" . format ( start = itime - self . ts , end = itime , a = loss ) sys . stdout . flush () ## comment out the lines below if you do not want to see the trajectory plots fig = plt . figure ( figsize = ( 12 , 2 )) nplot = 3 for i in range ( nplot ): ax = fig . add_subplot ( 1 , nplot , i + 1 ) ax . plot ( y_pred_adj [ i ,:, 0 ], label = \"ypred\" ) ax . plot ( y_val_adj [ i ,:, 0 ], label = \"yval\" ) ax . set_ylim ( - 0.5 , 0.5 ) plt . legend () plt . show () _ , ( SSE , N ) = self . get_mse ( y_pred_adj [:,: itime ,:], y_val_adj [:,: itime ,:], w_val_adj [:,: itime ]) return SSE , N def fit ( self , X , y , w , X_val , y_val , w_val , Nepoch = 300 ): X_val_adj , y_val_adj , w_val_adj = self . X_val_shape_adj ( X , X_val , y_val , w_val ) past_val_loss = np . Inf history = [] for iepoch in range ( Nepoch ): self . model . reset_states () print \"__________________________________\" print \"Epoch {}\" . format ( iepoch + 1 ) self . train1epoch ( X , y , w , iepoch ) val_loss = self . validate1epoch ( X_val_adj , y_val_adj , w_val_adj ) print \"-----------------> Epoch {iepoch:d} overall valoss={loss:.6f}\" . format ( iepoch = iepoch + 1 , loss = val_loss ), ## uncomments here if you want to save weights only when the weights resulted in lower validation loss ##if val_loss < past_val_loss: ## print \">>SAVED<<\" ## self.model.save_weights(\"/model.h5\") ## past_val_loss = val_loss ## print \"\" history . append ( val_loss ) return history Define a stateful model I consider batch size = 400 just as in the stateless model. In [132]: model_stateful , _ = define_model ( hidden_neurons = hunits , batch_size = 400 , stateful = True , len_ts = 500 ) model_stateful . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (400, 500, 1) 0 _________________________________________________________________ RNN (LSTM) (400, 500, 64) 16896 _________________________________________________________________ time_distributed_16 (TimeDis (400, 500, 1) 65 ================================================================= Total params: 16,961 Trainable params: 16,961 Non-trainable params: 0 _________________________________________________________________ Training The log shows the training loss and validation loss for the first 500 sec of time series and the next 500 sec of time series for each batch separately. It is clear that the model performance is lower in the last 500 sec in every epoch. This makes sense because the data shows more variability in that region. Three example validation time series are also plotted. In [133]: smodel = statefulModel ( model = model_stateful , print_val_every = 500 ) start = time . time () history_stateful = smodel . fit ( X_train , y_train , w_train , X_val , y_val , w_val , Nepoch = 100 ) end = time . time () print ( \"Time Took {:3.2f} min\" . format (( end - start ) / 60 )) batch_input_shape=(400, 500, 1) __________________________________ Epoch 1 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.015 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 1 overall valoss=0.007259 __________________________________ Epoch 2 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 2 overall valoss=0.007213 __________________________________ Epoch 3 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 3 overall valoss=0.007170 __________________________________ Epoch 4 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 4 overall valoss=0.007212 __________________________________ Epoch 5 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 5 overall valoss=0.007183 __________________________________ Epoch 6 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 6 overall valoss=0.007309 __________________________________ Epoch 7 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 7 overall valoss=0.007225 __________________________________ Epoch 8 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 8 overall valoss=0.007235 __________________________________ Epoch 9 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 9 overall valoss=0.007191 __________________________________ Epoch 10 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 10 overall valoss=0.007153 __________________________________ Epoch 11 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 11 overall valoss=0.007200 __________________________________ Epoch 12 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 12 overall valoss=0.007400 __________________________________ Epoch 13 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 13 overall valoss=0.007281 __________________________________ Epoch 14 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 14 overall valoss=0.007183 __________________________________ Epoch 15 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 15 overall valoss=0.007143 __________________________________ Epoch 16 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 16 overall valoss=0.007282 __________________________________ Epoch 17 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 17 overall valoss=0.007223 __________________________________ Epoch 18 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 18 overall valoss=0.007235 __________________________________ Epoch 19 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 19 overall valoss=0.007136 __________________________________ Epoch 20 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 20 overall valoss=0.007123 __________________________________ Epoch 21 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 21 overall valoss=0.007147 __________________________________ Epoch 22 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 22 overall valoss=0.007098 __________________________________ Epoch 23 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 23 overall valoss=0.007309 __________________________________ Epoch 24 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 24 overall valoss=0.007086 __________________________________ Epoch 25 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.013 -----------------> Epoch 25 overall valoss=0.007330 __________________________________ Epoch 26 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.014 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 26 overall valoss=0.007194 __________________________________ Epoch 27 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 27 overall valoss=0.007048 __________________________________ Epoch 28 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 28 overall valoss=0.006954 __________________________________ Epoch 29 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 29 overall valoss=0.006786 __________________________________ Epoch 30 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.011 -----------------> Epoch 30 overall valoss=0.006396 __________________________________ Epoch 31 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 31 overall valoss=0.005889 __________________________________ Epoch 32 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.020 -----------------> Epoch 32 overall valoss=0.011183 __________________________________ Epoch 33 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.014 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 33 overall valoss=0.006942 __________________________________ Epoch 34 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 34 overall valoss=0.006642 __________________________________ Epoch 35 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.011 -----------------> Epoch 35 overall valoss=0.006194 __________________________________ Epoch 36 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 36 overall valoss=0.005879 __________________________________ Epoch 37 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 37 overall valoss=0.006595 __________________________________ Epoch 38 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.011 -----------------> Epoch 38 overall valoss=0.006454 __________________________________ Epoch 39 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 39 overall valoss=0.005579 __________________________________ Epoch 40 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 40 overall valoss=0.006312 __________________________________ Epoch 41 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.013 -----------------> Epoch 41 overall valoss=0.007898 __________________________________ Epoch 42 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.014 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 42 overall valoss=0.005934 __________________________________ Epoch 43 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 43 overall valoss=0.005574 __________________________________ Epoch 44 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 44 overall valoss=0.006649 __________________________________ Epoch 45 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.010 -----------------> Epoch 45 overall valoss=0.006031 __________________________________ Epoch 46 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 46 overall valoss=0.006568 __________________________________ Epoch 47 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 47 overall valoss=0.006723 __________________________________ Epoch 48 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.010 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 48 overall valoss=0.006586 __________________________________ Epoch 49 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 49 overall valoss=0.006294 __________________________________ Epoch 50 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 50 overall valoss=0.006118 __________________________________ Epoch 51 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.011 -----------------> Epoch 51 overall valoss=0.006206 __________________________________ Epoch 52 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 52 overall valoss=0.004871 __________________________________ Epoch 53 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 53 overall valoss=0.005124 __________________________________ Epoch 54 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 54 overall valoss=0.006700 __________________________________ Epoch 55 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 55 overall valoss=0.005238 __________________________________ Epoch 56 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 56 overall valoss=0.006500 __________________________________ Epoch 57 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 57 overall valoss=0.004975 __________________________________ Epoch 58 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.001 500:1000 loss=0.016 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.012 -----------------> Epoch 58 overall valoss=0.006789 __________________________________ Epoch 59 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 59 overall valoss=0.005800 __________________________________ Epoch 60 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 60 overall valoss=0.005546 __________________________________ Epoch 61 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 61 overall valoss=0.005259 __________________________________ Epoch 62 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 62 overall valoss=0.005019 __________________________________ Epoch 63 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 63 overall valoss=0.005753 __________________________________ Epoch 64 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 64 overall valoss=0.004636 __________________________________ Epoch 65 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 65 overall valoss=0.004421 __________________________________ Epoch 66 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 66 overall valoss=0.005629 __________________________________ Epoch 67 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 67 overall valoss=0.004751 __________________________________ Epoch 68 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 68 overall valoss=0.005471 __________________________________ Epoch 69 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 69 overall valoss=0.004975 __________________________________ Epoch 70 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 70 overall valoss=0.004494 __________________________________ Epoch 71 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 71 overall valoss=0.005566 __________________________________ Epoch 72 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 72 overall valoss=0.004722 __________________________________ Epoch 73 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 73 overall valoss=0.005049 __________________________________ Epoch 74 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 74 overall valoss=0.005097 __________________________________ Epoch 75 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 75 overall valoss=0.005747 __________________________________ Epoch 76 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 76 overall valoss=0.005440 __________________________________ Epoch 77 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.018 -----------------> Epoch 77 overall valoss=0.010407 __________________________________ Epoch 78 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 78 overall valoss=0.005452 __________________________________ Epoch 79 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 79 overall valoss=0.005298 __________________________________ Epoch 80 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 80 overall valoss=0.005597 __________________________________ Epoch 81 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 81 overall valoss=0.005489 __________________________________ Epoch 82 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 82 overall valoss=0.005087 __________________________________ Epoch 83 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 83 overall valoss=0.005339 __________________________________ Epoch 84 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 84 overall valoss=0.004870 __________________________________ Epoch 85 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 85 overall valoss=0.004850 __________________________________ Epoch 86 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.007 -----------------> Epoch 86 overall valoss=0.004247 __________________________________ Epoch 87 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.006 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.007 -----------------> Epoch 87 overall valoss=0.004450 __________________________________ Epoch 88 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.006 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 88 overall valoss=0.004752 __________________________________ Epoch 89 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 89 overall valoss=0.003650 __________________________________ Epoch 90 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.006 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 90 overall valoss=0.003619 __________________________________ Epoch 91 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.000 500:1000 loss=0.006 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 91 overall valoss=0.006676 __________________________________ Epoch 92 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 92 overall valoss=0.005037 __________________________________ Epoch 93 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 93 overall valoss=0.003392 __________________________________ Epoch 94 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.000 500:1000 loss=0.006 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 94 overall valoss=0.004478 __________________________________ Epoch 95 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.006 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.012 -----------------> Epoch 95 overall valoss=0.007137 __________________________________ Epoch 96 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.014 -----------------> Epoch 96 overall valoss=0.008292 __________________________________ Epoch 97 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.010 -----------------> Epoch 97 overall valoss=0.005924 __________________________________ Epoch 98 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.000 500:1000 loss=0.005 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 98 overall valoss=0.003494 __________________________________ Epoch 99 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.006 Batch 02 0: 500 loss=0.001 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 99 overall valoss=0.004981 __________________________________ Epoch 100 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 100 overall valoss=0.003355 Time Took 7.10 min The validation loss plot, stateful and stateless models The validation loss plot shows that the back propagation algorithm of the stateless model seems less stable. In [178]: for label in [ \"loss\" , \"val_loss\" ]: plt . plot ( histroy . history [ label ], label = label + \" - stateless\" ) plt . plot ( history_stateful , label = \"val loss - stateful\" ) plt . legend () plt . show () Prediction phase For model testing, I want to predict $Y_t$ at every time point, one at a time. Therefore, I re-define a model with batch_size = (1000,1,2). As I am concerned about the unstable behavior of the back propagation algorithm in the stateful training, I will provide the last 4 training weights obtained during the final epoch. In [174]: model_pred1 , _ = define_model ( len_ts = 1 , hidden_neurons = hunits , batch_size = X_test . shape [ 0 ], stateful = True ) model_pred2 , _ = define_model ( len_ts = 1 , hidden_neurons = hunits , batch_size = X_test . shape [ 0 ], stateful = True ) model_pred3 , _ = define_model ( len_ts = 1 , hidden_neurons = hunits , batch_size = X_test . shape [ 0 ], stateful = True ) model_pred4 , _ = define_model ( len_ts = 1 , hidden_neurons = hunits , batch_size = X_test . shape [ 0 ], stateful = True ) ## load the final weights model_pred1 . load_weights ( \"weights_epoch099_batch800_time1000.hdf5\" ) model_pred2 . load_weights ( \"weights_epoch099_batch800_time0500.hdf5\" ) model_pred3 . load_weights ( \"weights_epoch099_batch400_time1000.hdf5\" ) model_pred4 . load_weights ( \"weights_epoch099_batch400_time0500.hdf5\" ) model_pred1 . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (5000, 1, 1) 0 _________________________________________________________________ RNN (LSTM) (5000, 1, 64) 16896 _________________________________________________________________ time_distributed_25 (TimeDis (5000, 1, 1) 65 ================================================================= Total params: 16,961 Trainable params: 16,961 Non-trainable params: 0 _________________________________________________________________ Define the prediction function for the stateful model. In [175]: def stateful_prediction ( mm , X_test , ntarget = 1 ): #expecting.. bis = mm . layers [ 0 ] . get_config ()[ \"batch_input_shape\" ] batch_size , ts , nfeat = bis assert ( X_test . shape [ 0 ] % batch_size == 0 ) assert ( X_test . shape [ 1 ] % ts == 0 ) y_pred = np . zeros (( X_test . shape [ 0 ], X_test . shape [ 1 ], ntarget )) y_pred [:] = np . NaN for ipat in range ( 0 , X_test . shape [ 0 ], batch_size ): mm . reset_states () for itime in range ( 0 , X_test . shape [ 1 ], ts ): X_testi = X_test [ ipat :( ipat + batch_size ), itime :( itime + ts ),:] y_pred [ ipat :( ipat + batch_size ), itime :( itime + ts ),:] = mm . predict ( X_testi , batch_size = batch_size ) return y_pred Prediction In [176]: y_pred_stateful1 = stateful_prediction ( mm = model_pred1 , X_test = X_test ) y_pred_stateful2 = stateful_prediction ( mm = model_pred2 , X_test = X_test ) y_pred_stateful3 = stateful_prediction ( mm = model_pred3 , X_test = X_test ) y_pred_stateful4 = stateful_prediction ( mm = model_pred4 , X_test = X_test ) Evaluate the model performance on testing data The stateful model outperform stateless twice, and perform much worse once. In [177]: plot_examples ( X_test , y_test , ypreds = [ y_pred_stateful1 , y_pred_stateful2 , y_pred_stateful3 , y_pred_stateful4 , y_pred_stateless ], nm_ypreds = [ \"ypred stateful1\" , \"ypred stateful2\" , \"ypred stateful3\" , \"ypred stateful4\" , \"ypred stateless\" ]) The final validation loss of ypred stateful1 is 0.003775 The final validation loss of ypred stateful2 is 0.004036 The final validation loss of ypred stateful3 is 0.004058 The final validation loss of ypred stateful4 is 0.004992 The final validation loss of ypred stateless is 0.004058 Let's see how the weights are changing over epochs. In [139]: def get_LSTM_UWb ( weight ): ''' weight must be output of LSTM's layer.get_weights() W: weights for input U: weights for hidden states b: bias ''' warr , uarr , barr = weight gates = [ \"i\" , \"f\" , \"c\" , \"o\" ] hunit = uarr . shape [ 0 ] U , W , b = {},{},{} for i1 , i2 in enumerate ( range ( 0 , len ( barr ), hunit )): W [ gates [ i1 ]] = warr [:, i2 : i2 + hunit ] U [ gates [ i1 ]] = uarr [:, i2 : i2 + hunit ] b [ gates [ i1 ]] = barr [ i2 : i2 + hunit ] . reshape ( hunit , 1 ) return ( W , U , b ) def get_LSTMweights ( model1 ): for layer in model1 . layers : if \"LSTM\" in str ( layer ): w = layer . get_weights () W , U , b = get_LSTM_UWb ( w ) break return W , U , b def vectorize_with_labels ( W , U , b ): bs , bs_label , ws , ws_label , us , us_label = [],[],[],[],[],[] for k in [ \"i\" , \"f\" , \"c\" , \"o\" ]: temp = list ( W [ k ] . flatten ()) ws_label . extend ([ \"W_\" + k ] * len ( temp )) ws . extend ( temp ) temp = list ( U [ k ] . flatten ()) us_label . extend ([ \"U_\" + k ] * len ( temp )) us . extend ( temp ) temp = list ( b [ k ] . flatten ()) bs_label . extend ([ \"b_\" + k ] * len ( temp )) bs . extend ( temp ) weight = ws + us + bs wlabel = ws_label + us_label + bs_label return ( weight , wlabel ) In [210]: from copy import copy import pandas as pd df = {} ibatch = 800 for epoch in np . arange ( 50 , 100 , 1 ): for itime in [ 500 , 1000 ]: title = \"weights_epoch{:03d}_batch{:01d}_time{:04d}.hdf5\" . format ( epoch , ibatch , itime ) model_stateful . load_weights ( title ) WUb = get_LSTMweights ( model_stateful ) weight , wlabel = vectorize_with_labels ( * WUb ) df [ \"epoch{:03d} time{:04d}\" . format ( epoch , itime )] = copy ( weight ) df = pd . DataFrame ( df , index = [ lab + \"_\" + str ( i ) for i , lab in enumerate ( wlabel )]) dfT = df . T Plot the weight change over every back propagation Observation and conclusions: Stateful procedure seems to be working in the sense that it returns comparable results as stateless procedure. The bias for cell states, forget gate, input gate and output gate seem to be oscillating between the back propagation of the batch with the first 500 sec and the next 500 sec. This observation again confirms the the unstable convergence of back propagation algorithm. Stateful training does not allow shuffling between the sub time series of the first 500 sec and the next 500 sec. This may be causing the back propagation algorithm unstable. Unless a single time series is too large to fit in to a single batch, I do not recommend to use the stateful training method as stateless model outperforms the stateful model. In [211]: unilabel = np . unique ( wlabel ) fig = plt . figure ( figsize = ( 25 , 105 )) fig . subplots_adjust ( hspace = 0.5 , wspace = 0.01 ) count = 1 for weight_type in unilabel : ax = fig . add_subplot ( 12 , 1 , count ) nweight = 0 for colnm in dfT . columns : if weight_type in colnm : ax . plot ( dfT [ colnm ] . values , label = weight_type + \"_\" + str ( count )) ax . set_title ( weight_type ) ax . set_xticks ( range ( dfT . shape [ 0 ]) ) ax . set_xticklabels ( list ( dfT . index . values ), rotation = \"vertical\" ) nweight += 1 if nweight > 5 : break count += 1 plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Stateful LSTM model training in Keras"},{"url":"Extract-weights-from-Keras's-LSTM-and-calcualte-hidden-and-cell-states.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog post, I will review the famous long short-term memory (LSTM) model and try to understand how it is implemented in Keras. If you know nothing about recurrent deep learning model, please read my previous post about recurrent neural network . If you know reccurent neural network (RNN) but not LSTM, you should first read Colah's great blog post . The LSTM outperforms Simple RNN model because it is designed to remember longer time series. In this blog, I will discuss: how to fit a LSTM model to predict a point in time series given another time series. how to extract weights for forget gates, input gates and output gates from the LSTM's model. how to calculate hidden and cell states from the weight outputs. To start learning LSTM, let's create a synthetic time series data. Having a data in front help understood the meaning of each parameter in LSTM. In [39]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import seaborn as sns import pandas as pd import sys , time import numpy as np import warnings warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"1\" set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 Create synthetic long time series data I will generate a time series $X_{t}$ ($t=1,...,T$) as my independent feature. As the target, or dependent time series, I will create a time series $Y_{t}$ as a function of a single time series $\\{ X_{k} \\}_{k=1}&#94;t$. Given integers $D$ and $T$, the time series $X_{t}$ an $Y_t$ are generated as: $$ C \\sim \\textrm{Multinomial}(5,6,...,99)\\\\ U \\sim \\textrm{Unif}([0,1])\\\\ X_{t} = -\\frac{t}{T}U\\left[ \\textrm{Cos}\\left(\\frac{t}{1+C}\\right) \\right]\\\\ Y_{t} = X_{t-2} X_{t-\\textrm{D}} \\textrm{ for t > D else } Y_t = 0 $$ We consider a long time series $T = 1,000$. The parameter $D$ determines the time lag. The larger $D$ is, the longer it takes for the $Y_{t}$ to show the effect of $X_{t}$, and the longer memory that the deep learning model needs to remember. For this exercise, I will consider $D=10$, and generate 1,000 sets of time series, independently. Plot examples of the generated time series. The frequency of the waves vary across samples. The first 10 $Y_t$ are set to zero ($Y_1=...=Y_{10} = 0$) In [40]: def random_sample ( len_ts = 3000 , D = 1001 ): c_range = range ( 5 , 100 ) c1 = np . random . choice ( c_range ) u = np . random . random ( 1 ) const = - 1.0 / len_ts ts = np . arange ( 0 , len_ts ) x1 = np . cos ( ts / float ( 1.0 + c1 )) x1 = x1 * ts * u * const y1 = np . zeros ( len_ts ) for t in range ( D , len_ts ): ## the output time series depend on input as follows: y1 [ t ] = x1 [ t - 2 ] * x1 [ t - D ] y = np . array ([ y1 ]) . T X = np . array ([ x1 ]) . T return y , X def generate_data ( D = 1001 , Nsequence = 1000 , T = 4000 , seed = 123 ): X_train = [] y_train = [] set_seed ( sd = seed ) for isequence in range ( Nsequence ): y , X = random_sample ( T , D = D ) X_train . append ( X ) y_train . append ( y ) return np . array ( X_train ), np . array ( y_train ) D = 10 T = 1000 X , y = generate_data ( D = D , T = T , Nsequence = 1000 ) print ( X . shape , y . shape ) ((1000, 1000, 1), (1000, 1000, 1)) Plot examples of the generated time series. The frequency of the waves vary across samples. The first 10 $Y_t$ are set to zero ($Y_1=...=Y_{10} = 0$) In [41]: def plot_examples ( X , y , ypreds = None , nm_ypreds = None ): fig = plt . figure ( figsize = ( 16 , 10 )) fig . subplots_adjust ( hspace = 0.32 , wspace = 0.15 ) count = 1 n_ts = 16 for irow in range ( n_ts ): ax = fig . add_subplot ( n_ts / 4 , 4 , count ) ax . set_ylim ( - 0.5 , 0.5 ) ax . plot ( X [ irow ,:, 0 ], \"--\" , label = \"x1\" ) ax . plot ( y [ irow ,:,:], label = \"y\" , linewidth = 3 , alpha = 0.5 ) ax . set_title ( \"{:}th time series sample\" . format ( irow )) if ypreds is not None : for ypred , nm in zip ( ypreds , nm_ypreds ): ax . plot ( ypred [ irow ,:,:], label = nm ) count += 1 plt . legend () plt . show () plot_examples ( X , y , ypreds = None , nm_ypreds = None ) LSTM model I create a LSTM model to predict $Y_t$ using the time series $X_k, k=1,...,t$ (t=1,...,1,000). Difference with RNN model The RNN layer updates the hidden states $h_t$ in a simple formula with two unknown weights and a bias, $w_{1x}$, $w_{1h}$ and $b_1$ given the previous hidden state $h_{t-1}$ and input $x_t$. The update process can be written in a single line as: $$ h_t = \\textrm{tanh}(x_t&#94;T w_{1x} + h_{t-1}&#94;T w_{1h} + b_1) $$ The hidden state $h_t$ is passed to the next cell as well as the next layer as inputs. The LSTM model also have hidden states that are updated between recurrent cells. In fact, the LSTM layer has two types of states: hidden state and cell states that are passed between the LSTM cells. However, only hidden states are passed to the next layer. LSTM cell formulation Let nfeat denote the number of input time series features. In our example, nfeat = 1. Then the LSTM layer with \"hunits\" hidden units $h_{t} \\in R&#94;{\\textrm{hunits}}$ are defined with 4(hunits x hunits + hunits x nfeat + hunits x 1) parameters: $$ \\boldsymbol{W}_i \\in R&#94;{\\textrm{hunits x nfeat}}, \\boldsymbol{U}_i \\in R&#94;{\\textrm{hunits x hunits}},\\boldsymbol{b}_i \\in R&#94;{\\textrm{hunits x 1}}\\\\ \\boldsymbol{W}_f \\in R&#94;{\\textrm{hunits x nfeat}},\\boldsymbol{U}_f \\in R&#94;{\\textrm{hunits x hunits}},\\boldsymbol{b}_f \\in R&#94;{\\textrm{hunits x 1}}\\\\ \\boldsymbol{W}_c \\in R&#94;{\\textrm{hunits x nfeat}},\\boldsymbol{U}_c \\in R&#94;{\\textrm{hunits x hunits}},\\boldsymbol{b}_c \\in R&#94;{\\textrm{hunits x 1}}\\\\ \\boldsymbol{W}_o \\in R&#94;{\\textrm{hunits x nfeat}},\\boldsymbol{U}_o \\in R&#94;{\\textrm{hunits x hunits}},\\boldsymbol{b}_o \\in R&#94;{\\textrm{hunits x 1}}\\\\ $$ input gate $$ \\boldsymbol{i}_t = \\textrm{sigmoid} \\left( \\boldsymbol{W}_i\\boldsymbol{h}_{t-1} + \\boldsymbol{U}_i\\boldsymbol{x}_{t} + \\boldsymbol{b}_i \\right) $$ sigmoid is applied element wise. forget gate $$ \\boldsymbol{f}_t = \\textrm{sigmoid} \\left( \\boldsymbol{W}_f \\boldsymbol{h}_{t-1}+ \\boldsymbol{U}_f \\boldsymbol{x}_{t} + \\boldsymbol{b}_f \\right) $$ sigmoid is applied element wise. new candidate cell state $$ \\boldsymbol{\\tilde{c}} = \\textrm{tanh} \\left( \\boldsymbol{W}_c \\boldsymbol{h}_{t-1}+ \\boldsymbol{U}_c \\boldsymbol{x}_{t} + \\boldsymbol{b}_c \\right) $$ tanh is applied element wise. output gate $$ \\boldsymbol{o}_t = \\textrm{sigmoid} \\left( \\boldsymbol{W}_o \\boldsymbol{h}_{t-1}+ \\boldsymbol{U}_o \\boldsymbol{x}_{t} + \\boldsymbol{b}_o \\right) $$ sigmoid is applied element wise. cell state $$ \\boldsymbol{c}_t= \\boldsymbol{f}_t * \\boldsymbol{c}_{t-1} + \\boldsymbol{i}_t * \\boldsymbol{\\tilde{c}} $$ $*$ means element wise multiplication. hidden state $$ \\boldsymbol{h}_t= \\boldsymbol{o}_t * \\textrm{tanh}(\\boldsymbol{c}_{t}) $$ $\\boldsymbol{h}_t$ is passed as an input of higher layer. For example in our model, we pass $\\boldsymbol{h}_t$ to the fully connected layer. I will create a single layer LSTM model with 3 nodes, followed by fully connected layer. This model contains 60 parameters (= 4 (3 3+3 1 + 3 1) = 4(hunits x hunits + hunits x nfeat + hunits x 1)) in LSTM layer and 4 parameters in fully connected layer. In [42]: from keras import models from keras import layers def define_model ( len_ts , hidden_neurons = 1 , nfeature = 1 , batch_size = None , stateful = False ): in_out_neurons = 1 inp = layers . Input ( batch_shape = ( batch_size , len_ts , nfeature ), name = \"input\" ) rnn = layers . LSTM ( hidden_neurons , return_sequences = True , stateful = stateful , name = \"RNN\" )( inp ) dens = layers . TimeDistributed ( layers . Dense ( in_out_neurons , name = \"dense\" ))( rnn ) model = models . Model ( inputs = [ inp ], outputs = [ dens ]) model . compile ( loss = \"mean_squared_error\" , sample_weight_mode = \"temporal\" , optimizer = \"rmsprop\" ) return ( model ,( inp , rnn , dens )) Here I define a model. In [43]: X_train , y_train = X , y hunits = 3 model1 , _ = define_model ( hidden_neurons = hunits , len_ts = X_train . shape [ 1 ]) model1 . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (None, 1000, 1) 0 _________________________________________________________________ RNN (LSTM) (None, 1000, 3) 60 _________________________________________________________________ time_distributed_3 (TimeDist (None, 1000, 1) 4 ================================================================= Total params: 64 Trainable params: 64 Non-trainable params: 0 _________________________________________________________________ For model training, we use MSE loss. As the first 10 $Y_t$ is not defined based on $X_{t}$, I will not compute the MSE for the first 10 samples and compute it only for the t = 10,...,1000. This can be easily done in Keras by incorporating \"sample_weight\" in loss definition. The loss for each sample sequence is defined with weights as: $$ Loss = \\frac{1}{\\sum_{t=1}&#94;T w_t}\\sum_{t=1}&#94;T w_t (Y_t - \\hat{Y}_t)&#94;2 $$ where $w_t = 1$ for $t>D$ and $w_t=0$ otherwise. In [44]: w = np . zeros ( y_train . shape [: 2 ]) w [:, D :] = 1 w_train = w Model training I will save model at every epoch using call back function. In [45]: from keras.callbacks import ModelCheckpoint start = time . time () hist1 = model1 . fit ( X_train , y_train , batch_size = 2 ** 9 , epochs = 200 , verbose = False , sample_weight = w_train , validation_split = 0.05 , callbacks = [ ModelCheckpoint ( filepath = \"weights{epoch:03d}.hdf5\" )]) end = time . time () print ( \"Time took {:3.1f} min\" . format (( end - start ) / 60 )) Time took 10.5 min The validation loss plot In [46]: labels = [ \"loss\" , \"val_loss\" ] for lab in labels : plt . plot ( hist1 . history [ lab ], label = lab + \" model1\" ) plt . yscale ( \"log\" ) plt . legend () plt . show () Validate the model performance with new data In [47]: X_test , y_test = generate_data ( D = D , T = T , seed = 2 , Nsequence = 1000 ) y_pred1 = model1 . predict ( X_test ) w_test = np . zeros ( y_test . shape [: 2 ]) w_test [:, D :] = 1 In [48]: plot_examples ( X_test , y_test , ypreds = [ y_pred1 ], nm_ypreds = [ \"ypred model1\" ]) print ( \"The final validation loss is {:5.4f}\" . format ( np . mean (( y_pred1 [ w_test == 1 ] - y_test [ w_test == 1 ]) ** 2 ))) The final validation loss is 0.0003 Reproduce LSTM layer outputs by hands The best way to understand how LSTM layer calculate hidden states and cell states are to reproduce them by hands! We first extract the estimated weights of the LSTM layer from model1. In [49]: for layer in model1 . layers : if \"LSTM\" in str ( layer ): weightLSTM = layer . get_weights () warr , uarr , barr = weightLSTM warr . shape , uarr . shape , barr . shape Out[49]: ((1, 12), (3, 12), (12,)) warr is a numpy array of weights for inputs uarr is a numpy array of weights for hidden units barr is a numpy array of bias The following methods extract weights for input, forget and output gates, and the cell states. Then I calculate cell states and hidden states. In [50]: def sigmoid ( x ): return ( 1.0 / ( 1.0 + np . exp ( - x ))) def LSTMlayer ( weight , x_t , h_tm1 , c_tm1 ): ''' c_tm1 = np.array([0,0]).reshape(1,2) h_tm1 = np.array([0,0]).reshape(1,2) x_t = np.array([1]).reshape(1,1) warr.shape = (nfeature,hunits*4) uarr.shape = (hunits,hunits*4) barr.shape = (hunits*4,) ''' warr , uarr , barr = weight s_t = ( x_t . dot ( warr ) + h_tm1 . dot ( uarr ) + barr ) hunit = uarr . shape [ 0 ] i = sigmoid ( s_t [:,: hunit ]) f = sigmoid ( s_t [:, 1 * hunit : 2 * hunit ]) _c = np . tanh ( s_t [:, 2 * hunit : 3 * hunit ]) o = sigmoid ( s_t [:, 3 * hunit :]) c_t = i * _c + f * c_tm1 h_t = o * np . tanh ( c_t ) return ( h_t , c_t ) The initial values of cell states and hidden states are zero. In [51]: c_tm1 = np . array ([ 0 ] * hunits ) . reshape ( 1 , hunits ) h_tm1 = np . array ([ 0 ] * hunits ) . reshape ( 1 , hunits ) We consider three time points $X_1=0.003,X_2=0.002$ and $X_3=1$ as inputs and evaluate $\\boldsymbol{h_3}$ and $\\boldsymbol{c_3}$: In [52]: xs = np . array ([ 0.003 , 0.002 , 1 ]) for i in range ( len ( xs )): x_t = xs [ i ] . reshape ( 1 , 1 ) h_tm1 , c_tm1 = LSTMlayer ( weightLSTM , x_t , h_tm1 , c_tm1 ) print ( \"h3={}\" . format ( h_tm1 )) print ( \"c3={}\" . format ( c_tm1 )) h3=[[0.15008775 0.01919995 0.1935698 ]] c3=[[0.22576768 0.03767894 0.46717375]] We can calculate hidden states and cell states using Keras's functional API. In [53]: batch_size = 1 len_ts = len ( xs ) nfeature = X_test . shape [ 2 ] inp = layers . Input ( batch_shape = ( batch_size , len_ts , nfeature ), name = \"input\" ) rnn , s , c = layers . LSTM ( hunits , return_sequences = True , stateful = False , return_state = True , name = \"RNN\" )( inp ) states = models . Model ( inputs = [ inp ], outputs = [ s , c , rnn ]) for layer in states . layers : for layer1 in model1 . layers : if layer . name == layer1 . name : layer . set_weights ( layer1 . get_weights ()) h_t_keras , c_t_keras , rnn = states . predict ( xs . reshape ( 1 , len_ts , 1 )) print ( \"h3={}\" . format ( h_t_keras )) print ( \"c3={}\" . format ( c_t_keras )) h3=[[0.14217362 0.01967374 0.19257708]] c3=[[0.22343878 0.03877562 0.45090497]] The hidden states and cell states from Keras and from my hand calculations reasonably agree. (But the two are not identical and I am not sure why...) In [54]: fig = plt . figure ( figsize = ( 9 , 4 )) ax = fig . add_subplot ( 1 , 2 , 1 ) ax . plot ( h_tm1 . flatten (), h_t_keras . flatten (), \"p\" ) ax . set_xlabel ( \"h by hand\" ) ax . set_ylabel ( \"h by Keras\" ) ax = fig . add_subplot ( 1 , 2 , 2 ) ax . plot ( c_tm1 . flatten (), c_t_keras . flatten (), \"p\" ) ax . set_xlabel ( \"h by hand\" ) ax . set_ylabel ( \"h by Keras\" ) plt . show () Obtain weights from LSTM Philippe Rémy commented how to obtain weights for forgate gatesm input gates, cell states and output gates. But this method seems outdated for the latest version of Keras. Here I try to extract LSTM weights by refering to LSTMCell definition at Keras's reccurent.py . In [55]: def get_LSTM_UWb ( weight ): ''' weight must be output of LSTM's layer.get_weights() W: weights for input U: weights for hidden states b: bias ''' warr , uarr , barr = weight gates = [ \"i\" , \"f\" , \"c\" , \"o\" ] hunit = uarr . shape [ 0 ] U , W , b = {},{},{} for i1 , i2 in enumerate ( range ( 0 , len ( barr ), hunit )): W [ gates [ i1 ]] = warr [:, i2 : i2 + hunit ] U [ gates [ i1 ]] = uarr [:, i2 : i2 + hunit ] b [ gates [ i1 ]] = barr [ i2 : i2 + hunit ] . reshape ( hunit , 1 ) return ( W , U , b ) def get_LSTMweights ( model1 ): for layer in model1 . layers : if \"LSTM\" in str ( layer ): w = layer . get_weights () W , U , b = get_LSTM_UWb ( w ) break return W , U , b def vectorize_with_labels ( W , U , b ): bs , bs_label , ws , ws_label , us , us_label = [],[],[],[],[],[] for k in [ \"i\" , \"f\" , \"c\" , \"o\" ]: temp = list ( W [ k ] . flatten ()) ws_label . extend ([ \"W_\" + k ] * len ( temp )) ws . extend ( temp ) temp = list ( U [ k ] . flatten ()) us_label . extend ([ \"U_\" + k ] * len ( temp )) us . extend ( temp ) temp = list ( b [ k ] . flatten ()) bs_label . extend ([ \"b_\" + k ] * len ( temp )) bs . extend ( temp ) weight = ws + us + bs wlabel = ws_label + us_label + bs_label return ( weight , wlabel ) Weights at every 10 epochs are plotted. The weights seem to converge stably. In [56]: from copy import copy df = {} for epoch in np . arange ( 0 , 200 , 10 ): model1 . load_weights ( \"weights{:03d}.hdf5\" . format ( epoch + 1 )) WUb = get_LSTMweights ( model1 ) weight , wlabel = vectorize_with_labels ( * WUb ) df [ \"{:03d}\" . format ( epoch )] = copy ( weight ) df = pd . DataFrame ( df , index = wlabel ) df = df [ np . sort ( df . columns )] plt . figure ( figsize = ( 15 , 15 )) sns . heatmap ( df ) plt . xlabel ( 'epoch' ) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Extract weights from Keras's LSTM and calcualte hidden and cell states"},{"url":"Understand-Keras's-RNN-behind-the-scenes-with-a-sin-wave-example.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Recurrent Neural Network (RNN) has been successful in modeling time series data. People say that RNN is great for modeling sequential data because it is designed to potentially remember the entire history of the time series to predict values. \"In theory\" this may be true. But when it comes to implementation of the RNN model in Keras, practitioners need to specify a \"length of time series\" in batch_shape: batch_shape = (N of time series in a batch, the length of time series, N of features) Well, I was very confused with this parameter at first. Why do I need to specify the length of time series when the model is meant to handle a sequence of potentially infinite length?? Where does this parameter come into play in the definition of the RNN model? Hindsight, these questions show my lack of understanding in back propagation through time (BPTT) algorithms. Nevertheless, there are not many good, concrete and simple explanations about the role of this parameter. The goal of this blog post is to help my-past-self and someone who is stack at the similar problems in understanding Keras's RNN model. I believe that the best way to understand models is to reproduce the model script by hands. Therefore, I will use a simple example (sin wave time series) to train a simple RNN (only 5 weights!!!!) and predict the sin wave values by hands. Here I will touch the concept of \"stateful\" and \"stateless\" prediction. I hope that this blog helps you understood the Keras's sequential model better. Reference Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras Stateful LSTM in Keras 深層学習ライブラリKerasでRNNを使ってsin波予測 In [1]: import sys print ( sys . version ) import tensorflow print ( tensorflow . __version__ ) import keras print ( keras . __version__ ) import pandas as pd import numpy as np import math import random import matplotlib.pyplot as plt % matplotlib inline 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] 1.5.0 2.1.3 Using TensorFlow backend. Generate a sin wave with no noise First, I create a function that generates sin wave with/without noise. Using this function, I will generate a sin wave with no noise. As this sin wave is completely deterministic, I should be able to create a model that can do prefect prediction the next value of sin wave given the previous values of sin waves! Here I generate period-10 sin wave, repeating itself 500 times, and plot the first few cycles. In [2]: def noisy_sin ( steps_per_cycle = 50 , number_of_cycles = 500 , random_factor = 0.4 ): ''' random_factor : amont of noise in sign wave. 0 = no noise number_of_cycles : The number of steps required for one cycle Return : pd.DataFrame() with column sin_t containing the generated sin wave ''' random . seed ( 0 ) df = pd . DataFrame ( np . arange ( steps_per_cycle * number_of_cycles + 1 ), columns = [ \"t\" ]) df [ \"sin_t\" ] = df . t . apply ( lambda x : math . sin ( x * ( 2 * math . pi / steps_per_cycle ) + random . uniform ( - 1.0 , + 1.0 ) * random_factor )) df [ \"sin_t_clean\" ] = df . t . apply ( lambda x : math . sin ( x * ( 2 * math . pi / steps_per_cycle ))) print ( \"create period-{} sin wave with {} cycles\" . format ( steps_per_cycle , number_of_cycles )) print ( \"In total, the sin wave time series length is {}\" . format ( steps_per_cycle * number_of_cycles + 1 )) return ( df ) steps_per_cycle = 10 df = noisy_sin ( steps_per_cycle = steps_per_cycle , random_factor = 0 ) n_plot = 8 df [[ \"sin_t\" ]] . head ( steps_per_cycle * n_plot ) . plot ( title = \"Generated first {} cycles\" . format ( n_plot ), figsize = ( 15 , 3 )) create period-10 sin wave with 500 cycles In total, the sin wave time series length is 5001 Out[2]: Create a training and testing data. Here, the controversial \"length of time series\" parameter comes into play. For now, we set this parameter to 2. In [3]: def _load_data ( data , n_prev = 100 ): \"\"\" data should be pd.DataFrame() \"\"\" docX , docY = [], [] for i in range ( len ( data ) - n_prev ): docX . append ( data . iloc [ i : i + n_prev ] . as_matrix ()) docY . append ( data . iloc [ i + n_prev ] . as_matrix ()) alsX = np . array ( docX ) alsY = np . array ( docY ) return alsX , alsY length_of_sequences = 2 test_size = 0.25 ntr = int ( len ( df ) * ( 1 - test_size )) df_train = df [[ \"sin_t\" ]] . iloc [: ntr ] df_test = df [[ \"sin_t\" ]] . iloc [ ntr :] ( X_train , y_train ) = _load_data ( df_train , n_prev = length_of_sequences ) ( X_test , y_test ) = _load_data ( df_test , n_prev = length_of_sequences ) print ( X_train . shape , y_train . shape , X_test . shape , y_test . shape ) ((3748, 2, 1), (3748, 1), (1249, 2, 1), (1249, 1)) Simple RNN model As a deep learning model, I consider the simplest possible RNN model: RNN with a single hidden unit followed by fully connected layer with a single unit. The RNN layer contains 3 weights: 1 weight for input, 1 weight for hidden unit, 1 weight for bias The fully connected layer contains 2 weights: 1 weight for input (i.e., the output from the previous RNN layer), 1 weight for bias In total, there are only 5 weights in this model. Let $x_t$ be the sin wave at time point $t$, then Formally, This simple model can be formulated in two lines as: $$ h_t = \\textrm{tanh}(x_t&#94;T w_{1x} + h_{t-1}&#94;T w_{1h}+ b_1)\\\\ x_{t+1} = h_t&#94;T w_2 + b_2 \\\\ $$ Conventionally $h_0=0$. Notice that the length of time series is not involved in the definition of the RNN. The model should be able to \"remember\" the past history of $x_t$ through the hidden unit $h_t$. batch_shape needs for BPTT. Every time when the model weights are updated, the BPTT uses only the randomly selected subset of the data. This means that the each batch is treated as independent. This batch_shape determines the size of this subset. Every batch starts will the initial hidden unit $h_0=0$. As we specify the length of the time series to be 2, our model only knows about the past 2 sin wave values to predict the next sin wave value. The practical limitation of the finite length of the time series defeats the theoretical beauty of RNN: the RNN here is not a model remembeing infinite past sequence!!! Now, we define this model using Keras and show the model summary. define_model It contains the stateful flag, and its default value is set to False, because this is the default setting in SimpleRNN method. We will set this flag to True and do the prediction later. It contains batch_size flag, and its default value is set to None, meaning that the batch_size can be decided during the training. None is the default for the Input layer when stateful = False. However, this value needs to be pre-specified when stateful = True. The details are discussed later. For the model definition, I use functional API, despite that you can readily define this model using Sequential API. The reason becomes clear later. In [4]: from keras.layers import Input from keras.models import Model from keras.layers.core import Dense , Activation from keras.layers.recurrent import SimpleRNN def define_model ( length_of_sequences , batch_size = None , stateful = False ): in_out_neurons = 1 hidden_neurons = 1 inp = Input ( batch_shape = ( batch_size , length_of_sequences , in_out_neurons )) rnn = SimpleRNN ( hidden_neurons , return_sequences = False , stateful = stateful , name = \"RNN\" )( inp ) dens = Dense ( in_out_neurons , name = \"dense\" )( rnn ) model = Model ( inputs = [ inp ], outputs = [ dens ]) model . compile ( loss = \"mean_squared_error\" , optimizer = \"rmsprop\" ) return ( model ,( inp , rnn , dens )) ## use the default values for batch_size, stateful model , ( inp , rnn , dens ) = define_model ( length_of_sequences = X_train . shape [ 1 ]) model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 2, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (None, 1) 3 _________________________________________________________________ dense (Dense) (None, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Now we train the model. The script was run without GPU. In [5]: hist = model . fit ( X_train , y_train , batch_size = 600 , epochs = 1000 , verbose = False , validation_split = 0.05 ) Plot of val_loss and loss. The validation loss and loss are exactly the same because our training data is a sin wave with no noise. Both validation and training data contain identical 10-period sin waves (with different number of cycles). The final validation loss is less than 0.001. In [6]: for label in [ \"loss\" , \"val_loss\" ]: plt . plot ( hist . history [ label ], label = label ) plt . ylabel ( \"loss\" ) plt . xlabel ( \"epoch\" ) plt . title ( \"The final validation loss: {}\" . format ( hist . history [ \"val_loss\" ][ - 1 ])) plt . legend () plt . show () The plot of true and predicted sin waves look nearly identical In [7]: y_pred = model . predict ( X_test ) plt . figure ( figsize = ( 19 , 3 )) plt . plot ( y_test , label = \"true\" ) plt . plot ( y_pred , label = \"predicted\" ) plt . legend () plt . show () What are the model weights? The best way to understand the RNN model is to create a model from scratch. Let's extract the weights and try to reproduce the predicted values from the model by hands. The model weights can be readily obtained from the model.layers. In [8]: ws = {} for layer in model . layers : ws [ layer . name ] = layer . get_weights () ws Out[8]: {'RNN': [array([[-0.41514578]], dtype=float32), array([[-0.64529276]], dtype=float32), array([0.00049243], dtype=float32)], 'dense': [array([[-3.950179]], dtype=float32), array([-0.00039617], dtype=float32)], 'input_1': []} What are the predicted values of hidden units? Since we used Keras's functional API to develop a model, we can easily see the output of each layer by compiling another model with outputs specified to be the layer of interest. In order to use the .predict() function, we need to compile the model, which requires specifying loss and optimizer. You can choose any values of loss and optimizer here, as we do not actually optimize this loss function. The newly created model \"rnn_model\" shares the weights obtained by the previous model's optimization. Therefore for the purpose of visualizing the hidden unit values with the current model result, we do not need to do additional optimizations. In [9]: rnn_model = Model ( inputs = [ inp ], outputs = [ rnn ]) rnn_model . compile ( loss = \"mean_squared_error\" , optimizer = \"rmsprop\" ) hidden_units = rnn_model . predict ( X_test ) . flatten () Plot shows that the predicted hidden unit is capturing the wave shape. Scaling and shifting of the predicted hidden unit yield the predicted sin wave. In [10]: upto = 100 predicted_sin_wave = ws [ \"dense\" ][ 0 ][ 0 ][ 0 ] * hidden_units + ws [ \"dense\" ][ 1 ][ 0 ] plt . figure ( figsize = ( 19 , 3 )) plt . plot ( y_test [: upto ], label = \"y_pred\" ) plt . plot ( hidden_units [: upto ], label = \"hidden units\" ) plt . plot ( predicted_sin_wave [: upto ], \"*\" , label = \"w2 * hidden units + b2\" ) plt . legend () plt . show () Obtain predicted sin wave at the next time point given the current sin wave by hand We understand that how the predicted sin wave values can be obtained using the predicted hidden states from Keras. But how does the predicted hidden states generated from the original inputs i.e. the current sin wave? Here, stateful and stateless prediction comes into very important role. Following the definition of the RNN, we can write a script for RNNmodel as: In [11]: def RNNmodel ( ws , x , h = 0 ): ''' ws: predicted weights x : scalar current sign value h : scalar RNN hidden unit ''' h = np . tanh ( x * ws [ \"RNN\" ][ 0 ][ 0 ][ 0 ] + h * ws [ \"RNN\" ][ 1 ][ 0 ][ 0 ] + ws [ \"RNN\" ][ 2 ][ 0 ]) x = h * ws [ \"dense\" ][ 0 ][ 0 ][ 0 ] + ws [ \"dense\" ][ 1 ][ 0 ] return ( x , h ) Naturally, you can obtain the predicted sin waves ($x_1,x_2,...,x_t$) by looping around RNNmodel as: $$ x&#94;*_{t+1}, h_{t+1} = \\textrm{RNNmodel}(x_{t},h_{t}) $$ Here $x&#94;*_t$ indicates the estimated value of x at time point $t$. As our model is not so complicated, we can readily implement this algorithm as: In [12]: upto = 50 ## predict the first sin values xstars , hs_hand = [], [] for i , x in enumerate ( df_test . values ): if i == 0 : h = 0 ## initial hidden layer value is zero xstar = x print ( \"initial value of sin x_0 = {}, h_0 = {}\" . format ( x , h )) hs_hand . append ( h ) xstars . append ( xstar [ 0 ]) xstar , h = RNNmodel ( ws , x , h ) assert len ( df_test . values ) == len ( xstars ) initial value of sin x_0 = [-1.27375647e-13], h_0 = 0 In this formulation, x_stars[t] contains the prediction of sin wave at time point t just as df_test In [13]: plt . figure ( figsize = ( 18 , 3 )) plt . plot ( df_test . values [: upto ], label = \"true\" , alpha = 0.3 , linewidth = 5 ) plt . plot ( xstars [: upto ], label = \"sin prediction (xstar)\" ) plt . plot ( hs_hand [: upto ], label = \"hidden state (xstar)\" ) plt . legend () Out[13]: You can see that the model prediction is not good in the first few time points and then stabilized. OK. My model seems to over estimates the values when sin wave is going down and underestimates when the sin wave is going up. However, there is one question: this model returns almost zero validation loss. The error seems a bit high. In fact the error from the prediction above is quite large. What is going on? In [14]: \"validation loss {:3.2f}\" . format ( np . mean (( np . array ( xstars ) - df_test [ \"sin_t\" ] . values ) ** 2 )) Out[14]: 'validation loss 0.08' Let's predict the sin wave using the existing predict function from Keras. Remind you that we prepare X_test when X_train was defined. X_test contains data as: $ x_1, x_2\\\\ x_2, x_3\\\\ x_3, x_4\\\\ ... $ In [15]: y_test_from_keras = model . predict ( X_test ) . flatten () Notice that this predicted values are exactly the same as the ones calculated before. In [16]: np . all ( predicted_sin_wave == y_test_from_keras ) Out[16]: True As the prediction starts from x_3, add the 2 NaN into a predicted vector as placeholders. This is just to make sure that the length of y_test_from_keras is compatible with xtars. In [17]: y_test_from_keras = [ np . NaN , np . NaN ] + list ( y_test_from_keras . flatten ()) h_test_from_keras = [ np . NaN , np . NaN ] + list ( hidden_units . flatten ()) The plot shows that Keras's predicted values are almost perfect and the validation loss is nearly zero. Clearly xstars are different from the Keras's prediction. It seems that the predicted states from Keras and from by hand are also slightly different. Then question is, how does Keras predict the output? In [18]: plt . figure ( figsize = ( 18 , 3 )) plt . plot ( df_test . values [: upto ], label = \"true\" , alpha = 0.3 , linewidth = 5 ) plt . plot ( xstars [: upto ], label = \"sin prediction (xstar)\" ) plt . plot ( hs_hand [: upto ], label = \"hidden state (xstar)\" ) plt . plot ( y_test_from_keras [: upto ], label = \"sin prediction (keras)\" ) plt . plot ( h_test_from_keras [: upto ], label = \"hidden state (keras)\" ) plt . legend () print ( \"validation loss {:6.5f}\" . format ( np . nanmean (( np . array ( y_test_from_keras ) - df_test [ \"sin_t\" ] . values ) ** 2 ))) validation loss 0.00017 Here, the technical details of the BPTT algorithm comes in, and the time series length parameter (i.e., batch_size[1]) takes very important role. As the BPTT algorithm only passed back 2 steps, the model assumes that: the hidden units are initialized to zero every 2 steps. the prediction of the next sin value ($x_{t+1}$) is based on the hidden unit ($h_t$) which is created by updating the hidden units twice in the past assuming that $h_{t-1}=0$. $$ x_t&#94;{*} ,h_t = \\textrm{RNNmodel}(x_{t-1},0)\\\\ x_{t+1}, - = \\textrm{RNNmodel}(x_{t},h_{t}) $$ Note that the intermediate predicted sin $x_t&#94;{*} $ based on $h_{t-1}=0$ should not be used as the predicted sin value. This is because the $x_t&#94;*$ was not directly used to evaluate the loss function. Finally, obtain the Keras's predicted sin wave at the next time point given the current sin wave by hand. In [19]: def myRNNpredict ( ws , X ): X = X . flatten () h = 0 for i in range ( len ( X )): x , h = RNNmodel ( ws , X [ i ], h ) return ( x , h ) xs , hs = [], [] for i in range ( X_test . shape [ 0 ]): x , h = myRNNpredict ( ws , X_test [ i ,:,:]) xs . append ( x ) hs . append ( h ) In [20]: print ( \"All sin estimates agree with ones from Keras = {}\" . format ( np . all ( np . abs ( np . array ( xs ) - np . array ( y_test_from_keras [ 2 :]) ) < 1E-5 ))) print ( \"All hidden state estmiates agree with ones fome Keras = {}\" . format ( np . all ( np . abs ( np . array ( hs ) - np . array ( h_test_from_keras [ 2 :]) ) < 1E-5 )) ) All sin estimates agree with ones from Keras = True All hidden state estmiates agree with ones fome Keras = True Now we understand how Keras is predicting the sin wave. In fact, Keras has a way to return xstar as predicted values, using \"stateful\" flag. This stateful is a notorious parameter and many people seem to be very confused. But by now you can understand what this stateful flag is doing, at least during the prediction phase. When stateful = True, you can decide when to reset the states to 0 by yourself. In order to predict in \"stateful\" mode, we need to re-define the model with stateful = True. When stateful is True, we need to specify the exact integer for batch_size. As we only have a single sin time series, we will set the batch_size to 1. In [21]: model_stateful , _ = define_model ( length_of_sequences = 1 , batch_size = 1 , stateful = True ) model_stateful . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (1, 1, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (1, 1) 3 _________________________________________________________________ dense (Dense) (1, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Assign the trained weights into the stateful model. In [22]: for layer in model . layers : for layer_predict in model_stateful . layers : if ( layer_predict . name == layer . name ): layer_predict . set_weights ( layer . get_weights ()) break Now we predict in stateful mode. Here it is very important to reset_state() before the prediction so that $h_0 = 0$. In [51]: pred = df_test . values [ 0 ][ 0 ] stateful_sin = [] model_stateful . reset_states () for i in range ( df_test . shape [ 0 ]): stateful_sin . append ( pred ) pred = model_stateful . predict ( df_test . values [ i ] . reshape ( 1 , 1 , 1 ))[ 0 ][ 0 ] stateful_sin = np . array ( stateful_sin ) In [57]: print ( \"All predicted sin values with stateful model agree to xstars = {}\" . format ( np . all ( np . abs ( np . array ( stateful_sin ) - np . array ( xstars )) < 1E-5 ))) All predicted sin values with stateful model agree to xstars = True Now we understand that xstars is the prediction result when stateful = True. We also understand that the prediction results are way better when stateful = False at least for this sin wave example. However, the prediction with stateful = False brings to some awkwardness: what if our batch have a very long time series of length, say $K$? Do we always have to go back all the $K$ time steps, set $h_{t-K}=0$ and then feed forward $K$ steps in order to predict at the time point $t$? This may be computationally intense. Next step Assess stateful VS stateless prediction with more examples and confirm whether the stateful prediction is ALWAYS worse. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Understand Keras's RNN behind the scenes with a sin wave example -  Stateful and Stateless prediction -"},{"url":"create-a-simple-game-using-pygame.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } The above gif is my very first game. In this blog post, I will introduce a very simple game created using pygame. What is this game about? In this game, you are a \"princess\" and the goal is to rescue \"price\". But be careful, there is a snake monster that could hit you! I used pygame. This pygame library is really REALLY amazing for creating a game. I originally thought that creating game needs a \"serious\" computer science degree, and it is out of my reach. Creating graphical user interface seemed very complicated! How can my script accepts keyboard input, do calculation and then update the game screen accodingly? No worry, pygame will take care of such things, so you can focus on creating game logics (e.g., how to score points, what decides \"game over\"). In fact the game above requires only about 400 lines. Where did I start? Did I take any online courses? Well, yes and no. All I did was follow this great youtube series from thenewboston . This series has many tutorial videos. I watched the first 40 (40!). Each tutorial is nicely modularized and takes only about 5 - 10 minutes. I would love to watch the rest later in my life! Game features If you are interested in learning pygame from scratch, you should go to thenewboston . Here, I will show some features of my game, and how I did them. The keyboad input are extracted in less than 20 lines. This is not really features of my game but I want to say where this is defined. Its defined Here The game is 400 x 400 pixcels You can specify the number of pixcels by specifying npix_x and npix_y inputs of the SavePrince object. But the game display is really defined in the line: Here The snake monster hunts you down. I added this functionality to make the game more challenging. It will always move toward you. For example, if you are on the right of the snake monster and above it, then the snake monster will go up or right. The decision of going up or going right is based on randomness. Defined here The snake monster appears around prince. This is another fuctionality to make the game more challenging. While you want to go closer to prince, you need to avoid the snake that is around the prince. Defined ere The speed of the snake monster increases when it moves toward the same direction consequtively. Even if the snake seems to be far away from you, it will get you really quick if the snake is on the same row or on the same column. Defined here for princess Defined here for the snake monster How to play the game? To run the game, download the whole SavePrinceGame repository, pip install pygame, and then within the SavePrinceGame repository and on your terminal, type: python2 SavePrince.py if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Create a simple game using pygame"},{"url":"Create-deep-learning-calculators-based-on-Encoder-Decoder-RNN-using-Keras.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Google search is great. You can ask any question you have and give you links to the potential solutions. It can sometimes give you the solution itself when the question is simple enough. One of such example is that google search can act as a calcualator. You can ask \"1 + 1\" or \"1 + 1\" to get the value 2. It somehow knows that the spaces are nuicense and return correct value. You can even ask the calculation with some strings. See pic below: In [3]: from keras.preprocessing.image import ImageDataGenerator , img_to_array , load_img load_img ( \"./pic/Capture2.PNG\" ) Using TensorFlow backend. Out[3]: Somehow, google understand that what I am asking is \"32+123\" and do the calculations. Can we do the same using deep learning model that takes a string as input? I was fascinated by one of the Keras's examples in Github called addition rnn . This script shows the implementation of sequence to sequence learning for performing addition. The script considers the summation of two 3-digit numbers, for example, 123+420=543. The input of the model is a string \"123+429\" and output is \"543\". In this blog post, we try to understand how the RNNs learn calculations. We will first consider very VERY simple example of summation of two 1-digits, then we will work on more complex senario. In [2]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys import warnings warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" #### 2 GPU1 #### 0 GPU3 #### 4 GPU4 #### 3 GPU2 set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 Create one-hot encoders that takes string and return one-hot encoded matrix of size as many as the number of characters in the string This class is bollowed from addition rnn with no change. In [3]: class CharacterTable ( object ): \"\"\"Given a set of characters: + Encode them to a one hot integer representation + Decode the one hot integer representation to their character output + Decode a vector of probabilities to their character output \"\"\" def __init__ ( self , chars ): \"\"\"Initialize character table. # Arguments chars: Characters that can appear in the input. \"\"\" self . chars = sorted ( set ( chars )) self . char_indices = dict (( c , i ) for i , c in enumerate ( self . chars )) self . indices_char = dict (( i , c ) for i , c in enumerate ( self . chars )) def encode ( self , C , num_rows ): \"\"\"One hot encode given string C. # Arguments num_rows: Number of rows in the returned one hot encoding. This is used to keep the # of rows for each data the same. \"\"\" x = np . zeros (( num_rows , len ( self . chars ))) for i , c in enumerate ( C ): x [ i , self . char_indices [ c ]] = 1 return x def decode ( self , x , calc_argmax = True ): if calc_argmax : x = x . argmax ( axis =- 1 ) return '' . join ( self . indices_char [ x ] for x in x ) class colors : ok = ' \\033 [92m' fail = ' \\033 [91m' close = ' \\033 [0m' RNN model for summation of two single digits. Generate 10,000 samples. As we are only considering the summation of two single digits, there are only 10 x 10 = 100 possible samples. Sampling 10,000 samples means we are sampling identical samples 100 times on average. In [4]: import numpy as np set_seed ( 123 ) # Parameters for the model and dataset. TRAINING_SIZE = 10000 DIGITS = 1 # Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of # int is DIGITS. MAXLEN = DIGITS + 1 + DIGITS # All the numbers, plus sign and space for padding. chars = '0123456789+ ' ctable = CharacterTable ( chars ) f = lambda : int ( '' . join ( np . random . choice ( list ( '0123456789' )) for i in range ( np . random . randint ( 1 , DIGITS + 1 )))) questions = [] expected = [] print ( 'Generating data...' ) while len ( questions ) < TRAINING_SIZE : if len ( questions ) % 1000 == 0 : print ( \"{} samples are generated...\" . format ( len ( questions ))) a , b = f (), f () q = '{}+{}' . format ( a , b ) query = q + ' ' * ( MAXLEN - len ( q )) ans = str ( a + b ) # Answers can be of maximum size DIGITS + 1. ans += ' ' * ( DIGITS + 1 - len ( ans )) questions . append ( query ) expected . append ( ans ) print ( 'Total addition questions:{}' . format ( len ( questions ))) Generating data... 0 samples are generated... 1000 samples are generated... 2000 samples are generated... 3000 samples are generated... 4000 samples are generated... 5000 samples are generated... 6000 samples are generated... 7000 samples are generated... 8000 samples are generated... 9000 samples are generated... Total addition questions:10000 Example data strings look like: In [5]: for q , a in zip ( questions , expected )[: 10 ]: print ( \"{:3} = {:4}\" . format ( q , a )) 2+2 = 4 6+1 = 7 3+9 = 12 6+1 = 7 0+1 = 1 9+0 = 9 0+9 = 9 3+4 = 7 0+0 = 0 4+1 = 5 Using the class previously created, we one-hot encode each character. Input There are 3 characters per sequence and each character could potential take 12 values (\"0123456789+ \"). The input of the model is 3 one-hot encoded vectors each of which represents a single character. Hence one sample sequence is represented as 3 by 12. The example of the one-hot encoded vectors ($\\boldsymbol{x}_{i,t} \\in R&#94;{12}$ The time index take values: $t=1,2,3$.) are: $\\boldsymbol{x}_{i,t_1}=[0,0,0,0,0,0,1,0,0,0,0,0]$ indicates \"6\" as the 6th position (position counting starting from 0) is nonzero. $\\boldsymbol{x}_{i,t_2}=[0,0,0,0,0,0,0,0,0,0,1,0]$ indicates \"+\". $\\boldsymbol{x}_{i,t_3}=[1,0,0,0,0,0,0,0,0,0,0,0]$ indicates \"0\". $[\\boldsymbol{x}_{i,t_1},\\boldsymbol{x}_{i,t_2},\\boldsymbol{x}_{i,t_3}]$ together represents a single sentence \"6+0\" $\\boldsymbol{x}_{i,t_4}=[0,0,0,0,0,0,0,0,0,0,0,1]$ indicates empty space i.e., \" \" Targets The targets are 2 one-hot encoded vectors, and each one-hot encoded vector must have length 12. If the solution is a single digit (for example, the solution to \"0+6\" is \"6\" and \"6\" is a single digit), then solution has to be \"6 \", which can be represetned as $[\\boldsymbol{x}_{i,t_1}, \\boldsymbol{x}_{i,t_4}]$ using the notation above. In practice we use $y$ to represent the target vectors. $ \\boldsymbol{y}_{i,k} \\in [0,1]&#94;{12}, k=1,2 $ In [6]: def one_hot_encoder ( expected , questions , x_dim , y_dim , chars , ctable ): print ( 'Vectorization...' ) x = np . zeros (( len ( questions ), x_dim , len ( chars )), dtype = np . bool ) y = np . zeros (( len ( questions ), y_dim , len ( chars )), dtype = np . bool ) for i , sentence in enumerate ( questions ): x [ i ] = ctable . encode ( sentence , x_dim ) for i , sentence in enumerate ( expected ): y [ i ] = ctable . encode ( sentence , y_dim ) return ( x , y ) x , y = one_hot_encoder ( expected , questions , MAXLEN , DIGITS + 1 , chars , ctable ) Vectorization... Split between training and testing In [7]: def split_train_test ( x , y ): # Explicitly set apart 10% for validation data that we never train over. split_at = len ( x ) - len ( x ) // 10 ( x_train , x_val ) = x [: split_at ], x [ split_at :] ( y_train , y_val ) = y [: split_at ], y [ split_at :] return ( x_train , x_val ),( y_train , y_val ) ( x_train , x_val ),( y_train , y_val ) = split_train_test ( x , y ) print ( 'Training Data:' ) print ( x_train . shape ) print ( y_train . shape ) Training Data: (9000, 3, 12) (9000, 2, 12) Model Definition We consider encoder-decoder RNN models. The RNN is previously discussed here . The encoder-decoder model is often used in the field of machine translation, and it has been explained by many blogs. So I will not give you a nice smooth introduction, but interested readers should read (or at least see the graph) Peeking into the neural network architecture used for Google's Neural Machine Translation or How Does Attention Work in Encoder-Decoder Recurrent Neural Networks . As the model's name suggests, from a high-level, the model is comprised of two sub-models: an encoder and a decoder. Encoder: The encoder is responsible for stepping through the input time steps and encoding the entire sequence into a fixed length vector called a context vector. Decoder: The decoder is responsible for stepping through the output time steps while reading from the context vector. The original addition rnn considers the solutions to the summation of two 3-or-less-digit numbers. The model used was an encoder-decoder RNN with 208,529 parameters. As there are 1 million potential combination sample (1,000 x 1,000), this number of parameters may be necessary considering the complexity of the problem. However, in my example, I only consider the summatoin of two single digit numbers, meaning that there are only 10 x 10 = 100 unique samples. The function with 100 if-else statements can do the perfect jobs in predicting the outcome. So I should be able to create a model with less than 100 parameters! Simpler model is preferable also because it is easier to understand. So for now, we consider an encoder-decoder model with 77 parameters. The model defenition follows: HIDDEN_ENCODER=1 HIDDEN_DECODER=3 In [9]: load_img ( \"./pic/encoder_decoder.png\" ) Out[9]: Encoder layer: In total there are 48 parameters ( = 12x1 + 1x1 + 1 = 12 + 1 + 1 = 14) $ \\boldsymbol{w}_{e} \\in R&#94;{\\textrm{HIDDEN_ENCODER x 12}},\\boldsymbol{w}_{h_{e}} \\in R&#94;{\\textrm{HIDDEN_ENCODER x HIDDEN_ENCODER}}, \\boldsymbol{b}_e \\in R&#94;{\\textrm{HIDDEN_ENCODER}} $ layer definition $ \\boldsymbol{e}_{i,0}=0\\in R&#94;{\\textrm{HIDDEN_ENCODER}}\\\\ \\boldsymbol{e}_{i,t} = \\textrm{tanh}(\\boldsymbol{x}_{i,t}&#94;T \\boldsymbol{w}_{e} + \\boldsymbol{e}_{i,t-1}&#94;T \\boldsymbol{w}_{e} +\\boldsymbol{b}_e) \\\\ $ Decoder layer: In total there are 21 parameters ( = 3x1 + 3x3 + 3 = 3 + 9 + 3 = 15) $ \\boldsymbol{w}_{d} \\in R&#94;{\\textrm{HIDDEN_DECODER x HIDDEN_ENCODER}},\\boldsymbol{w}_{h_{d}} \\in R&#94;{\\textrm{HIDDEN_DECODER x HIDDEN_DECODER}}, \\boldsymbol{b}_d \\in R&#94;{\\textrm{HIDDEN_DECODER}} $ layer definition $k=1,2$ $ \\boldsymbol{d}_{i,0}=0\\in R&#94;{\\textrm{HIDDEN_DECODER}}\\\\ \\boldsymbol{d}_{i,k} = \\textrm{tanh}(\\boldsymbol{e}_{i,3}&#94;T \\boldsymbol{w}_{d} + \\boldsymbol{d}_{i,k-1}&#94;T \\boldsymbol{w}_{d} +\\boldsymbol{b}_d) \\\\ $ Time-distributed Dense layer: In total there are 48 parameters ( = 12x3 + 12 = 48) $ \\boldsymbol{w}_{dense}\\in R&#94;{\\textrm{12 x HIDDEN_DECODER}}\\boldsymbol{b}_{dense}\\in R&#94;{12}\\\\ $ layer defenition $ \\boldsymbol{y}_{i,k} = \\textrm{softmax}(\\boldsymbol{d}_{i,k}&#94;T \\boldsymbol{w}_{dense} + \\boldsymbol{b}_{dense}) $ In [8]: import keras.layers as layers from keras.models import Model from keras.layers.core import Dense print ( 'Build model...' ) def define_model ( MAXLEN_x , MAXLEN_y , chars , HIDDEN_ENCODER , HIDDEN_DECODER , RNN ): def scalarToList ( a ): if not isinstance ( a , list ): a = [ a ] lena = len ( a ) return ( a , lena ) def return_seq ( nlayer , lenEncoder ): ''' return sequence must be False if this layer is the final one ''' return ( False if nlayer == lenENCODER else True ) HIDDEN_ENCODER , lenENCODER = scalarToList ( HIDDEN_ENCODER ) HIDDEN_DECODER , lenDECODER = scalarToList ( HIDDEN_DECODER ) inp = layers . Input ( batch_shape = ( None , MAXLEN_x , len ( chars )), name = \"Input\" ) # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE. encoder = RNN ( HIDDEN_ENCODER [ 0 ], return_sequences = return_seq ( 1 , lenENCODER ), name = \"encoder1\" )( inp ) nlayer = 1 if lenENCODER > 1 : for HE in HIDDEN_ENCODER [ 1 :]: nlayer += 1 encoder = RNN ( HE , return_sequences = return_seq ( nlayer , lenENCODER ), name = \"encoder\" + str ( nlayer ))( encoder ) rep_encoder = layers . RepeatVector ( MAXLEN_y , name = \"repeat_vector\" )( encoder ) # By setting return_sequences to True, return not only the last output but # all the outputs so far in the form of (num_samples, timesteps, # output_dim). This is necessary as TimeDistributed in the below expects # the first dimension to be the timesteps. decoder = RNN ( HIDDEN_DECODER [ 0 ], return_sequences = True , name = \"decoder1\" )( rep_encoder ) if lenDECODER > 1 : for HE in HIDDEN_DECODER [ 1 :]: decoder = RNN ( HE , return_sequences = True , name = name + str ( nlayer ))( decoder ) # Apply a dense layer to the every temporal slice of an input. For each of step # of the output sequence, decide which character should be chosen. #out = layers.Dense(len(chars))(decoder) out = layers . TimeDistributed ( layers . Dense ( len ( chars ), activation = \"softmax\" ), name = \"time_distributed_dense\" )( decoder ) model = Model ( inputs = [ inp ], outputs = [ out ]) encoder = Model ( inputs = [ inp ], outputs = [ rep_encoder ]) model . summary () return ( model , encoder ) HIDDEN_ENCODER = 1 #128 HIDDEN_DECODER = 3 #128 model , encoder = define_model ( MAXLEN , DIGITS + 1 , chars , HIDDEN_ENCODER , HIDDEN_DECODER , RNN = layers . SimpleRNN ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) Build model... _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= Input (InputLayer) (None, 3, 12) 0 _________________________________________________________________ encoder1 (SimpleRNN) (None, 1) 14 _________________________________________________________________ repeat_vector (RepeatVector) (None, 2, 1) 0 _________________________________________________________________ decoder1 (SimpleRNN) (None, 2, 3) 15 _________________________________________________________________ time_distributed_dense (Time (None, 2, 12) 48 ================================================================= Total params: 77 Trainable params: 77 Non-trainable params: 0 _________________________________________________________________ Define printing functions for model training In [9]: def print_predicted ( x , y , model , ctable , colors ): for i in range ( x . shape [ 0 ]): rowx , rowy = x [[ i ]], y [[ i ]] preds = model . predict ( rowx , verbose = 0 ) preds = preds . argmax ( axis = 2 ) q = ctable . decode ( rowx [ 0 ]) correct = ctable . decode ( rowy [ 0 ]) guess = ctable . decode ( preds [ 0 ], calc_argmax = False ) print ( ' Q:{}' . format ( q )), print ( 'T:{}' . format ( correct )), print ( 'Model:{}' . format ( guess )), if correct == guess : print ( colors . ok + '☑' + colors . close ) else : print ( colors . fail + '☒' + colors . close ) Model Training In [10]: def train ( model , ctable , x_train , y_train , x_val , y_val , nb_epochs , print_every , BATCH_SIZE ): history = { 'acc' :[], 'loss' :[], 'val_acc' :[], 'val_loss' :[]} for iteration in range ( 1 , nb_epochs ): hist = model . fit ( x_train , y_train , batch_size = BATCH_SIZE , epochs = 1 , verbose = False , validation_data = ( x_val , y_val )) ## printing if iteration % print_every == 0 : print ( '-' * 50 ) print ( 'Iteration {}' . format ( iteration )) for key in hist . history . keys (): history [ key ] . append ( hist . history [ key ][ 0 ]) if iteration % print_every == 0 : print ( \"{}:{:4.3f}\" . format ( key , hist . history [ key ][ 0 ])), if iteration % print_every == 0 : print ( \"\" ) # Select 10 samples from the validation set at random so we can visualize # errors. if iteration % print_every == 0 : index = np . random . randint ( 0 , len ( x_val ), 10 ) print_predicted ( x_val [ index ], y_val [ index ], model , ctable , colors ) return ( history ) In [11]: print ( x_train . shape , y_train . shape , x_val . shape , y_val . shape ) set_seed () history = train ( model , ctable , x_train , y_train , x_val , y_val , nb_epochs = 2001 , print_every = 400 , BATCH_SIZE = 128 ) ((9000, 3, 12), (9000, 2, 12), (1000, 3, 12), (1000, 2, 12)) -------------------------------------------------- Iteration 400 acc:0.909 loss:0.435 val_acc:0.908 val_loss:0.437 Q:6+0 T:6 Model:6 ☑ Q:6+1 T:7 Model:7 ☑ Q:3+9 T:12 Model:12 ☑ Q:2+8 T:10 Model:10 ☑ Q:2+9 T:11 Model:11 ☑ Q:5+9 T:14 Model:13 ☒ Q:8+6 T:14 Model:13 ☒ Q:6+9 T:15 Model:12 ☒ Q:0+0 T:0 Model:5 ☒ Q:0+6 T:6 Model:6 ☑ -------------------------------------------------- Iteration 800 acc:0.958 loss:0.233 val_acc:0.950 val_loss:0.234 Q:4+2 T:6 Model:6 ☑ Q:9+2 T:11 Model:11 ☑ Q:8+4 T:12 Model:12 ☑ Q:3+9 T:12 Model:12 ☑ Q:2+7 T:9 Model:9 ☑ Q:0+3 T:3 Model:4 ☒ Q:3+3 T:6 Model:6 ☑ Q:6+0 T:6 Model:6 ☑ Q:6+3 T:9 Model:9 ☑ Q:6+9 T:15 Model:15 ☑ -------------------------------------------------- Iteration 1200 acc:0.985 loss:0.142 val_acc:0.985 val_loss:0.142 Q:3+2 T:5 Model:5 ☑ Q:1+7 T:8 Model:8 ☑ Q:7+4 T:11 Model:11 ☑ Q:6+7 T:13 Model:13 ☑ Q:3+1 T:4 Model:4 ☑ Q:5+8 T:13 Model:13 ☑ Q:9+1 T:10 Model:10 ☑ Q:6+5 T:11 Model:11 ☑ Q:5+7 T:12 Model:12 ☑ Q:1+2 T:3 Model:3 ☑ -------------------------------------------------- Iteration 1600 acc:0.993 loss:0.103 val_acc:0.985 val_loss:0.105 Q:3+4 T:7 Model:7 ☑ Q:5+6 T:11 Model:11 ☑ Q:4+7 T:11 Model:11 ☑ Q:0+4 T:4 Model:4 ☑ Q:1+4 T:5 Model:5 ☑ Q:3+9 T:12 Model:12 ☑ Q:5+6 T:11 Model:11 ☑ Q:3+5 T:8 Model:8 ☑ Q:8+8 T:16 Model:16 ☑ Q:2+0 T:2 Model:3 ☒ -------------------------------------------------- Iteration 2000 acc:1.000 loss:0.082 val_acc:1.000 val_loss:0.083 Q:2+7 T:9 Model:9 ☑ Q:3+3 T:6 Model:6 ☑ Q:2+5 T:7 Model:7 ☑ Q:1+2 T:3 Model:3 ☑ Q:1+2 T:3 Model:3 ☑ Q:9+6 T:15 Model:15 ☑ Q:3+7 T:10 Model:10 ☑ Q:5+2 T:7 Model:7 ☑ Q:2+3 T:5 Model:5 ☑ Q:8+4 T:12 Model:12 ☑ The validation loss and the training loss This small model performs surprisingly well. Since both training and testing data contain all possible samples, the model performance in the two data is the same. In [12]: def plot_loss_acc ( history ): fig = plt . figure ( figsize = ( 20 , 8 )) labels = [[ \"acc\" , \"val_acc\" ],[ \"loss\" , \"val_loss\" ]] count = 1 for ilab in range ( len ( labels )): ax = fig . add_subplot ( 2 , 1 , count ) count += 1 for label in labels [ ilab ]: ax . plot ( history [ label ], label = label ) ax . set_xlabel ( \"epochs\" ) plt . legend () plt . show () plot_loss_acc ( history ) Plot encoders My encoder only has 1 dimention. I plot the values of this encoder for every possible sample values. The plot shows that 1 + 3, 2 + 2 and 3 + 1 receive the same encoder value. Similarly, 1 + 5, 2 + 4, 3 + 3, 4 + 2 and 1 + 5 receive the same encoder value. In fact, plot shows that the ecoder seems to record the (scaled) values of summation. Encoders discard the infomation of what original digits were and remember only the summation values. In [13]: import seaborn as sns import pandas as pd hidden = { \"encoder\" : [], \"num1\" :[], \"num2\" :[]} for num1 in range ( 10 ): for num2 in range ( 10 ): string = \"{}+{}\" . format ( num1 , num2 ) myx = ctable . encode ( string , MAXLEN ) e = encoder . predict ( myx . reshape ( 1 , myx . shape [ 0 ], myx . shape [ 1 ]))[ 0 ] ## notice that the predicted encoders are duplicated twice k = 1 , 2 ## because of repeatVector ## we only extract one of the vector. if ~ np . all ( e [ 0 ] == e [ 1 ]): ## should never be TRUE! print ( \"ERROR!\" ) h = e [ 0 ] hidden [ \"encoder\" ] . append ( h [ 0 ]) hidden [ \"num1\" ] . append ( num1 ) hidden [ \"num2\" ] . append ( num2 ) hidden = pd . DataFrame ( hidden ) sns . set () plt . figure ( figsize = ( 10 , 8 )) sns . heatmap ( hidden . pivot ( \"num1\" , \"num2\" , \"encoder\" )) plt . title ( \"encoder \\n num1 + num2\" ) plt . show () Deep learning calculators for more complex calculations Now I consider creating a deep learning calculator for more complex calculations: Mupltiple operations: summation (+), negation (-), multiplication (*) and devision (/). Number of digits up to 3 (i.e., 0 - 999) as an input Allow \" \" to appear in the input calculation e.g., \"1 23 + 3 12\" = \"123+312\". However, output is clean: it does not contain space in between digits. This results in more characters in my character dictionary. In addition to '0123456789+ ', now I have '-*/Na'. Na arises because when a number is devided by 0 the output sould be \"NaN \". I consider integer devision. This means, for example, 1/3 = 0, 5/3=1. I tried to train a model with the trainig data having the same size, i.e., 10,000. But it seemd that 10,000 samples is not enough to create a generalizable model. This makes sense because previously we only have 100 combinatiosn of problems (=10x10). But now, there are 4 million (4,000,000 = 4*1,000&#94;2) combinations of problems (even excluding the spaces that can randomly appear in the left hand side of the equation). The problem is much more complex! I demonstrate the data training with 100,000 samples. In [20]: ## \"N\" and \"a\" are set_seed () TRAINING_SIZE = 100000 chars2 = '0123456789+-*/Na ' print ( \"The number of characters in dictionary: {}\" . format ( len ( chars2 ))) ctable2 = CharacterTable ( chars2 ) DIGITS = 3 MAXLEN_x = DIGITS + 1 + DIGITS ## Maximum length of the output would occur ## when 99999x99999=9,999,800,001 which has length 10 MAXLEN_y = int ( np . ceil ( np . log10 ( (( 10 ** DIGITS ) - 1 ) ** 2 ))) def frand (): st = '' for i in range ( DIGITS ): st += str ( np . random . choice ( list ( '0123456789 ' ))) if st == ' ' * DIGITS : st = frand () return st ## arithmetic operation foperation = lambda : np . random . choice ( list ( '+/*-' )) def get_answer ( a , b , oper ): clean = lambda a : int ( a . replace ( \" \" , \"\" )) a = clean ( a ) b = clean ( b ) if oper == \"+\" : out = a + b elif oper == \"-\" : out = a - b elif oper == \"*\" : out = a * b elif oper == \"/\" : if b == 0 : out = \"NaN\" else : out = a / b return ( out ) questions = [] expected = [] print ( 'Generating data...' ) while len ( questions ) < TRAINING_SIZE : if len ( questions ) % 10000 == 0 : print ( \"{} samples are generated...\" . format ( len ( questions ))) a , b , oper = frand (), frand (), foperation () # Pad the data with spaces such that it is always MAXLEN. q = '{}{}{}' . format ( a , oper , b ) query = q + ' ' * ( MAXLEN_x - len ( q )) ans = str ( get_answer ( a , b , oper )) # Answers can be of maximum size MAXLEN_y. ans += ' ' * ( MAXLEN_y - len ( ans )) questions . append ( query ) expected . append ( ans ) print ( 'Total addition questions:{}' . format ( len ( questions ))) The number of characters in dictionary: 17 Generating data... 0 samples are generated... 10000 samples are generated... 20000 samples are generated... 30000 samples are generated... 40000 samples are generated... 50000 samples are generated... 60000 samples are generated... 70000 samples are generated... 80000 samples are generated... 90000 samples are generated... Total addition questions:100000 Example data strings look like: In [15]: for q , e in zip ( questions , expected )[: 20 ]: print ( \"{} = {}\" . format ( q , e )) 226-13 = 213 961+019 = 980 093+400 = 493 173*247 = 42731 480+793 = 1273 461/562 = 0 83 *502 = 41666 62/446 = 0 3 0-647 = -617 671/ 57 = 11 248/121 = 2 359*081 = 29079 335*979 = 327965 333-869 = -536 639/666 = 0 34/310 = 0 868-910 = -42 134+761 = 895 337/686 = 0 447+009 = 456 One-hot encoding and split between training and testing. In [16]: x , y = one_hot_encoder ( expected , questions , MAXLEN_x , MAXLEN_y , chars2 , ctable2 ) ( x_train , x_val ),( y_train , y_val ) = split_train_test ( x , y ) Vectorization... Define Model This time, I consider LSTM with more nodes and more layers for encoders! Note we also consider the encoder structure with bottole neck encoder dimention to be 1. This model was converging prohibitably slow: e.g., at epoch 500, it was still about validation accuracy was still as low as 0.6. Due to the computational resource, we consider complex model that converges faster. In [17]: set_seed () model2 , encoder2 = define_model ( MAXLEN_x , MAXLEN_y , chars2 , HIDDEN_ENCODER = [ 256 ], HIDDEN_DECODER = [ 256 ], RNN = layers . LSTM ) model2 . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= Input (InputLayer) (None, 7, 17) 0 _________________________________________________________________ encoder1 (LSTM) (None, 256) 280576 _________________________________________________________________ repeat_vector (RepeatVector) (None, 6, 256) 0 _________________________________________________________________ decoder1 (LSTM) (None, 6, 256) 525312 _________________________________________________________________ time_distributed_dense (Time (None, 6, 17) 4369 ================================================================= Total params: 810,257 Trainable params: 810,257 Non-trainable params: 0 _________________________________________________________________ Training In [18]: set_seed () history2 = train ( model2 , ctable2 , x_train , y_train , x_val , y_val , nb_epochs = 100 , #2000 print_every = 10 , BATCH_SIZE = 64 ) -------------------------------------------------- Iteration 10 acc:0.799 loss:0.504 val_acc:0.801 val_loss:0.498 Q:405*934 T:378270 Model:359600 ☒ Q:799+472 T:1271 Model:1271 ☑ Q:33 *952 T:31416 Model:30966 ☒ Q:283+926 T:1209 Model:1211 ☒ Q:232+59 T:291 Model:399 ☒ Q: 88/154 T:0 Model:0 ☑ Q:412*280 T:115360 Model:110880 ☒ Q:787+074 T:861 Model:861 ☑ Q:2 3+ 39 T:62 Model:50 ☒ Q:1 7/397 T:0 Model:0 ☑ -------------------------------------------------- Iteration 20 acc:0.904 loss:0.256 val_acc:0.886 val_loss:0.296 Q:077/558 T:0 Model:0 ☑ Q:765* 60 T:45900 Model:46300 ☒ Q:037+3 8 T:75 Model:75 ☑ Q: 54-721 T:-667 Model:-667 ☑ Q:308-666 T:-358 Model:-358 ☑ Q:976/619 T:1 Model:1 ☑ Q: 19/127 T:0 Model:0 ☑ Q:982+899 T:1881 Model:1881 ☑ Q:426/669 T:0 Model:0 ☑ Q:8 -5 2 T:-44 Model:-44 ☑ -------------------------------------------------- Iteration 30 acc:0.929 loss:0.197 val_acc:0.894 val_loss:0.293 Q:236*520 T:122720 Model:123720 ☒ Q:525/402 T:1 Model:1 ☑ Q:012-869 T:-857 Model:-857 ☑ Q:115*615 T:70725 Model:66925 ☒ Q:419-66 T:353 Model:353 ☑ Q:838+4 3 T:881 Model:881 ☑ Q:932*006 T:5592 Model:6632 ☒ Q: 58*619 T:35902 Model:35742 ☒ Q:144/353 T:0 Model:0 ☑ Q:898* 04 T:3592 Model:3992 ☒ -------------------------------------------------- Iteration 40 acc:0.952 loss:0.139 val_acc:0.889 val_loss:0.359 Q:923*709 T:654407 Model:655407 ☒ Q:709- 43 T:666 Model:666 ☑ Q:64 /269 T:0 Model:0 ☑ Q:172+618 T:790 Model:790 ☑ Q:234- 08 T:226 Model:226 ☑ Q:152-313 T:-161 Model:-161 ☑ Q:62 *513 T:31806 Model:32346 ☒ Q:354-1 7 T:337 Model:337 ☑ Q:535+604 T:1139 Model:1139 ☑ Q:934*7 2 T:67248 Model:68768 ☒ -------------------------------------------------- Iteration 50 acc:0.968 loss:0.099 val_acc:0.889 val_loss:0.424 Q:3 2/525 T:0 Model:0 ☑ Q:592/864 T:0 Model:0 ☑ Q:616*016 T:9856 Model:9034 ☒ Q:53 -08 T:45 Model:55 ☒ Q:92 +145 T:237 Model:237 ☑ Q:488*419 T:204472 Model:205772 ☒ Q:020/312 T:0 Model:0 ☑ Q:151*607 T:91657 Model:96957 ☒ Q:507-537 T:-30 Model:-30 ☑ Q:2 8*649 T:18172 Model:17812 ☒ -------------------------------------------------- Iteration 60 acc:0.983 loss:0.058 val_acc:0.889 val_loss:0.505 Q:192*998 T:191616 Model:188778 ☒ Q:290+24 T:314 Model:314 ☑ Q:540*034 T:18360 Model:18360 ☑ Q:368*910 T:334880 Model:330080 ☒ Q: 07-417 T:-410 Model:-410 ☑ Q:0 1*642 T:642 Model:642 ☑ Q: 99/219 T:0 Model:0 ☑ Q:01 *872 T:872 Model:872 ☑ Q:8 0*394 T:31520 Model:31780 ☒ Q:178/654 T:0 Model:0 ☑ -------------------------------------------------- Iteration 70 acc:0.989 loss:0.039 val_acc:0.883 val_loss:0.623 Q: 22*841 T:18502 Model:17422 ☒ Q:298-465 T:-167 Model:-167 ☑ Q: 94-619 T:-525 Model:-525 ☑ Q:3 4+035 T:69 Model:69 ☑ Q:367*213 T:78171 Model:77231 ☒ Q:6 +0 8 T:14 Model:14 ☑ Q:215+211 T:426 Model:426 ☑ Q:035+695 T:730 Model:730 ☑ Q:3 3*615 T:20295 Model:20765 ☒ Q:845+416 T:1261 Model:1261 ☑ -------------------------------------------------- Iteration 80 acc:0.990 loss:0.037 val_acc:0.884 val_loss:0.670 Q:121+944 T:1065 Model:1065 ☑ Q: 08+933 T:941 Model:941 ☑ Q:368+626 T:994 Model:994 ☑ Q:751-895 T:-144 Model:-144 ☑ Q:913/495 T:1 Model:1 ☑ Q:305/381 T:0 Model:0 ☑ Q:511+ 7 T:518 Model:528 ☒ Q: 19/187 T:0 Model:0 ☑ Q:293/ 11 T:26 Model:26 ☑ Q:040-889 T:-849 Model:-849 ☑ -------------------------------------------------- Iteration 90 acc:0.993 loss:0.027 val_acc:0.884 val_loss:0.710 Q:731/15 T:48 Model:48 ☑ Q:013*567 T:7371 Model:7471 ☒ Q:008*013 T:104 Model:144 ☒ Q:424*260 T:110240 Model:100460 ☒ Q: 95*529 T:50255 Model:58135 ☒ Q: 67*294 T:19698 Model:20058 ☒ Q:33 *952 T:31416 Model:32756 ☒ Q:142+201 T:343 Model:343 ☑ Q:806/559 T:1 Model:1 ☑ Q:648-055 T:593 Model:593 ☑ Plot validation and accuracy over epochs The validation loss starts increasing after 30 epochs indicating that the model overfits. On the other hand, the overfitting behavior cannot be seen from the validation accuracy (it remains the same (and not decreasing) after the 40 epochs.) This is probably because the accuracy is a more \"corse\" measure than the \"softmax-categorical_crossentropy\" which was used as our loss. I would increase training sample size or change the model structure to improve the model performance. In [19]: plot_loss_acc ( history2 ) Model performance The code block below shows the randomly selected 100 validation samples. It is clear that the model performance is reasonable in many problems but the performance becomes poor when the solution requires more than 4 digits. In [30]: set_seed ( 1 ) random_index = np . random . choice ( x_val . shape [ 0 ], 100 , replace = False ) print_predicted ( x_val [ random_index ], y_val [ random_index ], model2 , ctable2 , colors ) Q:478+629 T:1107 Model:1107 ☑ Q:383*249 T:95367 Model:944977 ☒ Q:231-94 T:137 Model:137 ☑ Q:841/232 T:3 Model:3 ☑ Q:151+448 T:599 Model:597 ☒ Q:539-661 T:-122 Model:-122 ☑ Q:9 5*474 T:45030 Model:44070 ☒ Q:801+180 T:981 Model:981 ☑ Q:767/260 T:2 Model:2 ☑ Q:079/983 T:0 Model:0 ☑ Q:180+926 T:1106 Model:1106 ☑ Q:411+515 T:926 Model:926 ☑ Q: 78/983 T:0 Model:0 ☑ Q:067+029 T:96 Model:96 ☑ Q:595+ 76 T:671 Model:671 ☑ Q:182/691 T:0 Model:0 ☑ Q: 5+800 T:805 Model:805 ☑ Q:238-617 T:-379 Model:-379 ☑ Q: 05- 64 T:-59 Model:-59 ☑ Q:734-59 T:675 Model:675 ☑ Q:53 +037 T:90 Model:80 ☒ Q:326-157 T:169 Model:169 ☑ Q:681+695 T:1376 Model:1376 ☑ Q:1 0/6 9 T:0 Model:0 ☑ Q:825/518 T:1 Model:1 ☑ Q:3 4/809 T:0 Model:0 ☑ Q:097+312 T:409 Model:409 ☑ Q:435*892 T:388020 Model:388180 ☒ Q:699+420 T:1119 Model:1109 ☒ Q:2 8/985 T:0 Model:0 ☑ Q:574+725 T:1299 Model:1299 ☑ Q:35 -581 T:-546 Model:-546 ☑ Q: 98/171 T:0 Model:0 ☑ Q:679-237 T:442 Model:442 ☑ Q:088+401 T:489 Model:489 ☑ Q:390/007 T:55 Model:52 ☒ Q:577/091 T:6 Model:6 ☑ Q:887*909 T:806283 Model:811773 ☒ Q:225+324 T:549 Model:549 ☑ Q:165/0 0 T:NaN Model:NaN ☑ Q:551- 1 T:550 Model:550 ☑ Q:15 -747 T:-732 Model:-732 ☑ Q: 63+520 T:583 Model:583 ☑ Q:251+158 T:409 Model:309 ☒ Q:698-518 T:180 Model:180 ☑ Q:3 1+980 T:1011 Model:1011 ☑ Q:970/902 T:1 Model:1 ☑ Q:183*53 T:9699 Model:1669 ☒ Q:294-213 T:81 Model:71 ☒ Q:707+400 T:1107 Model:1107 ☑ Q:402+846 T:1248 Model:1248 ☑ Q:533/971 T:0 Model:0 ☑ Q:2 7+549 T:576 Model:576 ☑ Q:654-805 T:-151 Model:-151 ☑ Q:974/609 T:1 Model:1 ☑ Q:783+540 T:1323 Model:1323 ☑ Q:01 *835 T:835 Model:835 ☑ Q:068*650 T:44200 Model:44400 ☒ Q:071-566 T:-495 Model:-495 ☑ Q:390/821 T:0 Model:0 ☑ Q:794+954 T:1748 Model:1748 ☑ Q: 17-323 T:-306 Model:-306 ☑ Q:386-042 T:344 Model:344 ☑ Q:797*354 T:282138 Model:270618 ☒ Q:1 /44 T:0 Model:0 ☑ Q:531+299 T:830 Model:820 ☒ Q:357*6 8 T:24276 Model:22716 ☒ Q:862*608 T:524096 Model:528016 ☒ Q:7 5*36 T:2700 Model:2600 ☒ Q:326/ 13 T:25 Model:20 ☒ Q:538+214 T:752 Model:752 ☑ Q:713/ 54 T:13 Model:13 ☑ Q:126+013 T:139 Model:139 ☑ Q:906* 2 T:1812 Model:1972 ☒ Q:39 /0 7 T:5 Model:5 ☑ Q:664*5 7 T:37848 Model:37348 ☒ Q:790+914 T:1704 Model:1704 ☑ Q:909*1 6 T:14544 Model:14564 ☒ Q:49 *529 T:25921 Model:25981 ☒ Q:299+308 T:607 Model:607 ☑ Q:040+503 T:543 Model:543 ☑ Q:47 -088 T:-41 Model:-41 ☑ Q:525/3 1 T:16 Model:17 ☒ Q:479+981 T:1460 Model:1460 ☑ Q:731-358 T:373 Model:373 ☑ Q:96 *856 T:82176 Model:81536 ☒ Q: 65-541 T:-476 Model:-476 ☑ Q:806+323 T:1129 Model:1129 ☑ Q:099* 73 T:7227 Model:7287 ☒ Q:481-06 T:475 Model:475 ☑ Q: 89/6 4 T:1 Model:1 ☑ Q:080/919 T:0 Model:0 ☑ Q:917-741 T:176 Model:176 ☑ Q:449-565 T:-116 Model:-116 ☑ Q:851-67 T:784 Model:794 ☒ Q:427/083 T:5 Model:5 ☑ Q:76 *00 T:0 Model:0 ☑ Q:101+30 T:131 Model:131 ☑ Q:961*179 T:172019 Model:168879 ☒ Q:6 2+ 5 T:67 Model:67 ☑ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Create deep learning calculators based on Encoder-Decoder RNN using Keras"},{"url":"Assess-the-robustness-of-CapsNet.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In the Understanding and Experimenting Capsule Networks , I experimented Hinton's Capsule Network . Dynamic Routing Between Capsules discusses the robustness of the Capsule Networks to affine transformations: \"Experiments show that each DigitCaps capsule learns a more robust representation for each class than a traditional convolutional network. Because there is natural variance in skew, rotation, style, etc in hand written digits, the trained CapsNet is moderately robust to small affine transformations of the training data (Section 5.2, page 6).\" The authors used affNIST for the robustness valdiation. In this blog, I evaluate the robustness of this network. I will create affine transformed MNIST test data by myself and use my version of CapsNet and my version of the standard CNN model introduced in prvious blog posts for predicting the correct class of digits in this new data. my version of CapsNet and my version of the standard CNN model respectively yield validation losses of 0.42% and 0.50% on MNIST with no affine transformation. As the models are trained only on 2-pixel shifted MNIST, the performance of models would deteriorate. Nevertheless, I expect that robust model would have lower performance deterioration. Reference Dynamic Routing Between Capsules Understanding and Experimenting Capsule Networks In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"2\" #### 2 GPU1 #### 0 GPU3 #### 4 GPU4 #### 3 GPU2 set_session ( tf . Session ( config = config )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 Define class objects for CapsNet. The codes here are created by modifing Kevin Mader's ipython notebook script in Kaggle competition , which, in turn are written by adapting Xifeng Guo's script in Github . In [2]: import keras.backend as K from keras import initializers , layers from keras import models class Length ( layers . Layer ): \"\"\" Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss inputs: shape=[dim_1, ..., dim_{n-1}, dim_n] output: shape=[dim_1, ..., dim_{n-1}] \"\"\" def call ( self , inputs , ** kwargs ): return K . sqrt ( K . sum ( K . square ( inputs ), - 1 )) def compute_output_shape ( self , input_shape ): return input_shape [: - 1 ] class Mask ( layers . Layer ): \"\"\" Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1. Output shape: [None, d2] This class is used to reduce the dimention of the (Nsample, n_class, dim_vector) --> (Nsample, dim_vector) For training: only keep the activity vector (v in paper) of true class for testing only keep the activity vector with the largest norm (length in vector) \"\"\" def call ( self , inputs , ** kwargs ): # use true label to select target capsule, shape=[batch_size, num_capsule] if type ( inputs ) is list : # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code. assert len ( inputs ) == 2 inputs , mask = inputs else : # if no true label, mask by the max length of vectors of capsules x = inputs # Enlarge the range of values in x to make max(new_x)=1 and others < 0 x = ( x - K . max ( x , 1 , True )) / K . epsilon () + 1 mask = K . clip ( x , 0 , 1 ) # the max value in x clipped to 1 and other to 0 # masked inputs, shape = [batch_size, dim_vector] inputs_masked = K . batch_dot ( inputs , mask , [ 1 , 1 ]) return inputs_masked def compute_output_shape ( self , input_shape ): if type ( input_shape [ 0 ]) is tuple : # true label provided return tuple ([ None , input_shape [ 0 ][ - 1 ]]) else : return tuple ([ None , input_shape [ - 1 ]]) def squash ( vectors , axis =- 1 ): \"\"\" The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0 :param vectors: some vectors to be squashed, N-dim tensor :param axis: the axis to squash :return: a Tensor with same shape as input vectors \"\"\" s_squared_norm = K . sum ( K . square ( vectors ), axis , keepdims = True ) scale = s_squared_norm / ( 1 + s_squared_norm ) / K . sqrt ( s_squared_norm ) return scale * vectors class CapsuleLayer ( layers . Layer ): \"\"\" The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\ [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1. :param num_capsule: number of capsules in this layer :param dim_vector: dimension of the output vectors of the capsules in this layer :param num_routings: number of iterations for the routing algorithm \"\"\" def __init__ ( self , num_capsule , dim_vector , num_routing = 3 , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , ** kwargs ): super ( CapsuleLayer , self ) . __init__ ( ** kwargs ) self . num_capsule = num_capsule self . dim_vector = dim_vector self . num_routing = num_routing self . kernel_initializer = initializers . get ( kernel_initializer ) self . bias_initializer = initializers . get ( bias_initializer ) def build ( self , input_shape ): assert len ( input_shape ) >= 3 , \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\" self . input_num_capsule = input_shape [ 1 ] self . input_dim_vector = input_shape [ 2 ] # Transform matrix self . W = self . add_weight ( shape = [ self . input_num_capsule , self . num_capsule , self . input_dim_vector , self . dim_vector ], initializer = self . kernel_initializer , name = 'W' ) # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation. self . bias = self . add_weight ( shape = [ 1 , self . input_num_capsule , self . num_capsule , 1 , 1 ], initializer = self . bias_initializer , name = 'bias' , trainable = False ) self . built = True def call ( self , inputs , training = None ): # inputs.shape=[None, input_num_capsule, input_dim_vector] # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector] inputs_expand = K . expand_dims ( K . expand_dims ( inputs , 2 ), 2 ) # Replicate num_capsule dimension to prepare being multiplied by W # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector] inputs_tiled = K . tile ( inputs_expand , [ 1 , 1 , self . num_capsule , 1 , 1 ]) \"\"\" # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size. # Now W has shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector] w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1]) # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector] inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3]) \"\"\" # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow. # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector] inputs_hat = tf . scan ( lambda ac , x : K . batch_dot ( x , self . W , [ 3 , 2 ]), elems = inputs_tiled , initializer = K . zeros ([ self . input_num_capsule , self . num_capsule , 1 , self . dim_vector ])) \"\"\" # Routing algorithm V1. Use tf.while_loop in a dynamic way. def body(i, b, outputs): c = tf.nn.softmax(self.bias, dim=2) # dim=2 is the num_capsule dimension outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True)) b = b + K.sum(inputs_hat * outputs, -1, keepdims=True) return [i-1, b, outputs] cond = lambda i, b, inputs_hat: i > 0 loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)] _, _, outputs = tf.while_loop(cond, body, loop_vars) \"\"\" # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance assert self . num_routing > 0 , 'The num_routing should be > 0.' for i in range ( self . num_routing ): c = tf . nn . softmax ( self . bias , dim = 2 ) # dim=2 is the num_capsule dimension # outputs.shape=[None, 1, num_capsule, 1, dim_vector] outputs = squash ( K . sum ( c * inputs_hat , 1 , keepdims = True )) # last iteration needs not compute bias which will not be passed to the graph any more anyway. if i != self . num_routing - 1 : # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True)) self . bias += K . sum ( inputs_hat * outputs , - 1 , keepdims = True ) # tf.summary.histogram('BigBee', self.bias) # for debugging return K . reshape ( outputs , [ - 1 , self . num_capsule , self . dim_vector ]) def compute_output_shape ( self , input_shape ): return tuple ([ None , self . num_capsule , self . dim_vector ]) def PrimaryCap ( inputs , dim_vector , n_channels , kernel_size , strides , padding ): \"\"\" Apply Conv2D `n_channels` times and concatenate all capsules :param inputs: 4D tensor, shape=[None, width, height, channels] :param dim_vector: the dim of the output vector of capsule :param n_channels: the number of types of capsules :return: output tensor, shape=[None, num_capsule, dim_vector] \"\"\" output = layers . Conv2D ( filters = dim_vector * n_channels , ## 8 x 32 kernel_size = kernel_size , ## 9x9 strides = strides , ## 2 padding = padding , name = \"PrimaryCap_conv2d\" )( inputs ) outputs = layers . Reshape ( target_shape = [ - 1 , dim_vector ], name = \"PrimaryCap_reshape\" )( output ) return layers . Lambda ( squash , name = \"PrimaryCap_squash\" )( outputs ) def NetworkInputToDigitCap ( input_shape , n_class , num_routing ): x = layers . Input ( shape = input_shape ) # Layer 1: Just a conventional Conv2D layer conv1 = layers . Conv2D ( filters = 256 , kernel_size = 9 , strides = 1 , padding = 'valid' , activation = 'relu' , name = 'Conv1' )( x ) # Layer 2: Conv2D layer with `squash` activation, # then reshape to [None, num_capsule, dim_vector] primarycaps = PrimaryCap ( conv1 , dim_vector = 8 , n_channels = 32 , kernel_size = 9 , strides = 2 , padding = 'valid' ) # Layer 3: Capsule layer. Routing algorithm works here. digitcaps = CapsuleLayer ( num_capsule = n_class , dim_vector = 16 , num_routing = num_routing , name = 'DigitCaps' )( primarycaps ) return ( x , digitcaps ) CapsNet functions In [3]: from keras import backend as K def CapsNet ( input_shape , n_class , num_routing ): \"\"\" A Capsule Network on MNIST. :param input_shape: data shape, 4d, [None, width, height, channels] :param n_class: number of classes :param num_routing: number of routing iterations :return: A Keras Model with 2 inputs and 2 outputs \"\"\" x , digitcaps = NetworkInputToDigitCap ( input_shape , n_class , num_routing ) # Layer 4: This is an auxiliary layer to replace each capsule with its length. # Just to match the true label's shape. # If using tensorflow, this will not be necessary. :) out_caps = Length ( name = 'out_caps' )( digitcaps ) ##||v|| margin_loss # Decoder network. y = layers . Input ( shape = ( n_class ,), name = \"true_label\" ) masked = Mask ( name = \"activity_vec\" )([ digitcaps , ## digitcaps.shape = (Nsample, n_class, dim_vector ) y ]) # The true label is used to mask the output of capsule layer. x_recon = Decoder ( masked ) # two-input-two-output keras Model m1 = models . Model ([ x , y ], [ out_caps , x_recon ]) m2 = models . Model ([ x ],[ digitcaps ]) ## YUMI added this line return m1 , m2 def Decoder ( masked ): ## Yumi refactored x_recon = layers . Dense ( 512 , activation = 'relu' , name = \"FC1\" )( masked ) x_recon = layers . Dense ( 1024 , activation = 'relu' , name = \"FC2\" )( x_recon ) x_recon = layers . Dense ( 784 , activation = 'sigmoid' , name = \"FC3\" )( x_recon ) ## mse to ensure that the reconstructed images are close to the original image. x_recon = layers . Reshape ( target_shape = [ 28 , 28 , 1 ], name = 'out_recon' )( x_recon ) return ( x_recon ) Build a CapsNet model and load the weights In [4]: import keras.backend as K from keras import initializers , layers from keras import models CapsNet , model_wo_decoder = CapsNet ( input_shape = [ 28 , 28 , 1 ], n_class = 10 , num_routing = 3 ) CapsNet . load_weights ( \"CapsNet_weights.h5\" ) Build a standard CNN model and load the weights In [5]: from keras.models import model_from_json def load_model ( name ): model = model_from_json ( open ( name + '_architecture.json' ) . read ()) model . load_weights ( name + '_weights.h5' ) return ( model ) standard = load_model ( \"standard\" ) Use Keras's ImageDataGenerator to modify the original data, and create transforemd data. In [6]: from keras.preprocessing.image import ImageDataGenerator from numpy.random import seed from tensorflow import set_random_seed def generate_once ( x , y , shift_fraction = 0.1 , shear_range = 0.2 , zoom_range = 0.2 , rotation_range = 40 ): ''' modify each image in x to generate new data return x, y of the same shape seed is set for reproducable results ''' seed ( 1 ) set_random_seed ( 1 ) batch_size = x . shape [ 0 ] train_datagen = ImageDataGenerator ( width_shift_range = shift_fraction , height_shift_range = shift_fraction , shear_range = shear_range , rotation_range = rotation_range , zoom_range = zoom_range ) # shift up to 2 pixel for MNIST generator = train_datagen . flow ( x , y , batch_size = batch_size ) x , y = generator . next () return ([ x , y ]) def plot_example_images ( x_tra , title , n = 10 ): ''' plot first n images in x_tra ''' const = 0.9 fig = plt . figure ( figsize = ( n * const , 1 * const )) fig . subplots_adjust ( hspace = 0.01 , wspace = 0.01 , left = 0 , right = 1 , bottom = 0 , top = 1 ) for i in range ( x_tra . shape [ 0 ]): ax = fig . add_subplot ( 1 , n , i + 1 , xticks = [], yticks = []) ax . imshow ( x_tra [ i ] . reshape ( 28 , 28 )) if n < ( i + 2 ): break plt . suptitle ( \"shift_fraction:{}, shear_range:{}, zoom_range:{}, {}\" . format ( shift_fraction , shear_range , zoom_range , title ), y = 1.5 ) plt . show () def get_error ( y_tra , y_pr ): ''' Return error y_tr : np.array() of shape (Nsample, Ncat), containing 0 or 1 y_pr : np.array() of shape (Nsample, Ncat), containing probabilities ''' pred_class = y_pr . argmax ( axis = 1 ) acc = np . mean ( y_tra [ range ( len ( pred_class )), pred_class ] == 1 ) * 100 return ( 100 - acc ) Load MNIST test data Here, I only load test data because I will use the test for evaluation only. In [7]: from mnist import MNIST import numpy as np from keras.utils import to_categorical mndata = MNIST ( './MNIST_data/' ) images , labels = mndata . load_testing () x_test , y_test = np . array ( images ), np . array ( labels ) x_test = x_test . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. y_test = to_categorical ( y_test . astype ( 'float32' )) print ( x_test . shape , y_test . shape ) ((10000, 28, 28, 1), (10000, 10)) The validation error reported in my previous blog article about CapsNet and my previous blog article about standard CNN model blog are: - CapsNet: 0.42% - Standard: 0.54% We reproduce these results first. In [8]: y_pred_Cap , _ = CapsNet . predict ([ x_test , y_test ]) y_pred_sta = standard . predict ( x_test ) acc_Cap = get_error ( y_test , y_pred_Cap ) acc_sta = get_error ( y_test , y_pred_sta ) print ( \"Error CapsNet: {:5.3}%, Standard: {:5.3}%\" . format ( acc_Cap , acc_sta )) Error CapsNet: 0.42%, Standard: 0.54% Model performance assessments with affine transfomation. I consider four types of affine transfomation: shifting shearing zooming rotation For rotation, I always consider rotating images for at most 40 degrees. For other types of affine transfomation, I consider 3 different degree, and created 27 different generated data. For each generated data, I will classify the digits using the two models. Let's see which model is more robust to the changes. In [9]: shift_fractions = [ 0.1 , 0.15 , 0.2 ] shear_ranges = [ 0.1 , 0.15 , 0.2 ] zoom_ranges = [ 0.05 , 0.1 , 0.15 ] N = len ( shift_fractions ) result_Cap = np . zeros (( N , N , N )) result_sta = np . zeros (( N , N , N )) for ishift , shift_fraction in enumerate ( shift_fractions ): for ishear , shear_range in enumerate ( shear_ranges ): for izoom , zoom_range in enumerate ( zoom_ranges ): x_tra , y_tra = generate_once ( x_test , y_test , shift_fraction , shear_range , zoom_range ) ### Prediction by CapsNet and the standard CNN y_pred_Cap , _ = CapsNet . predict ([ x_tra , y_tra ]) y_pred_sta = standard . predict ( x_tra ) ### Prediction error acc_Cap = get_error ( y_tra , y_pred_Cap ) acc_sta = get_error ( y_tra , y_pred_sta ) result_Cap [ ishift , ishear , izoom ] = acc_Cap result_sta [ ishift , ishear , izoom ] = acc_sta title = \"Error CapsNet: {:5.3}%, Standard: {:5.3}%\" . format ( acc_Cap , acc_sta ) plot_example_images ( x_tra , title ) Evaluation results for the two model's prediction performance on various augmented data. Both CapsNet and standard CNN deteriorates the performance when more severe transfomation is performed. CapsNet does not always yield the smaller error than the standard CNN for considered set of transformed data. When zooming proportion is low (<=0.1), CapsNet always performs better than the standard CNN no matter what other affine transfomation is applyed to the data. In [10]: const = 3 N = result_Cap . shape [ 0 ] fig = plt . figure ( figsize = ( N * const , N * const )) fig . subplots_adjust ( hspace = 0.1 , wspace = 0.1 , left = 0 , right = 1 , bottom = 0 , top = 1 ) count = 1 for ishift in range ( N ): for ishear in range ( N ): if ( ishear == 0 ) and ( ishift == N - 1 ): ax = fig . add_subplot ( N , N , count ) elif ishear == 0 : ax = fig . add_subplot ( N , N , count , xticks = [] ) elif ishift == N - 1 : ax = fig . add_subplot ( N , N , count , yticks = []) else : ax = fig . add_subplot ( N , N , count , xticks = [], yticks = []) count += 1 cvec = result_Cap [ ishift , ishear ,:] svec = result_sta [ ishift , ishear ,:] ax . plot ( zoom_ranges , cvec , label = \"CapsNet\" ) ax . plot ( zoom_ranges , svec , label = \"standard\" ) vec = list ( result_Cap [ ishift ,:,:]) + list ( result_sta [ ishift ,:,:]) ax . set_ylim ( np . min ( vec ), np . max ( vec )) ax . set_title ( \"shift%={}, shear={}\" . format ( shift_fractions [ ishift ], shear_ranges [ ishear ])) ax . legend () ax = fig . add_subplot ( 1 , 1 , 1 , frameon = False , xticks = [], yticks = []) ax . set_xlabel ( \"zoom ranges\" , labelpad = 30 ) ax . set_ylabel ( \"error (%)\" , labelpad = 50 ) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Assess the robustness of CapsNet"},{"url":"Understanding-and-Experimenting-Capsule-Networks.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } This blog is inspired by Dynamic Routing Between Capsules and aims to understand Capsule Networks with hands-on coding. I use Keras with tensorflow backend. The codes here are created by modifing Kevin Mader's ipython notebook script in Kaggle competition , which, in turn are written by adapting Xifeng Guo's script in Github . CapsNet is cool in many ways but I was particularly impressed with the following claims that the authors made: CapsNet requires less parameters to achieve lower validation error (0.25%) the state of art (0.39%). This CapsNet model \"only\" has 8M parameters while the baseline (current state of art) requires 35M parameters. CapsNet is interpretable. CapsNet is more robust than state of art. CapsNet needs less images for training. In this blog, I examine the points 1 and 2, and would like to discuss the points 3 and 4 in Assess the robustness of CapsNet . Findings: I was able to use Capsule Network to return validation error of 0.42%, which is less than my baseline (0.54%). Intepretation of the capsules is possible but it requires some efforts. Reference: Kaggle competition Siraj Raval's YouTube video Jonathan Hui's blog <- This blog helped me a lot to understand the techinical details. So thank you very much Jonathan! Max Pechyonkin's blog My previous blog Importing necessary modules In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" #### 2 GPU1 #### 0 GPU3 #### 4 GPU4 #### 3 GPU2 set_session ( tf . Session ( config = config )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 Core functions for DigitCaps and PrimaryCaps In [2]: import keras.backend as K from keras import initializers , layers from keras import models class Length ( layers . Layer ): \"\"\" Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss inputs: shape=[dim_1, ..., dim_{n-1}, dim_n] output: shape=[dim_1, ..., dim_{n-1}] \"\"\" def call ( self , inputs , ** kwargs ): return K . sqrt ( K . sum ( K . square ( inputs ), - 1 )) def compute_output_shape ( self , input_shape ): return input_shape [: - 1 ] class Mask ( layers . Layer ): \"\"\" Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1. Output shape: [None, d2] This class is used to reduce the dimention of the (Nsample, n_class, dim_vector) --> (Nsample, dim_vector) For training: only keep the activity vector (v in paper) of true class for testing only keep the activity vector with the largest norm (length in vector) \"\"\" def call ( self , inputs , ** kwargs ): # use true label to select target capsule, shape=[batch_size, num_capsule] if type ( inputs ) is list : # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code. assert len ( inputs ) == 2 inputs , mask = inputs else : # if no true label, mask by the max length of vectors of capsules x = inputs # Enlarge the range of values in x to make max(new_x)=1 and others < 0 x = ( x - K . max ( x , 1 , True )) / K . epsilon () + 1 mask = K . clip ( x , 0 , 1 ) # the max value in x clipped to 1 and other to 0 # masked inputs, shape = [batch_size, dim_vector] inputs_masked = K . batch_dot ( inputs , mask , [ 1 , 1 ]) return inputs_masked def compute_output_shape ( self , input_shape ): if type ( input_shape [ 0 ]) is tuple : # true label provided return tuple ([ None , input_shape [ 0 ][ - 1 ]]) else : return tuple ([ None , input_shape [ - 1 ]]) def squash ( vectors , axis =- 1 ): \"\"\" The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0 :param vectors: some vectors to be squashed, N-dim tensor :param axis: the axis to squash :return: a Tensor with same shape as input vectors \"\"\" s_squared_norm = K . sum ( K . square ( vectors ), axis , keepdims = True ) scale = s_squared_norm / ( 1 + s_squared_norm ) / K . sqrt ( s_squared_norm ) return scale * vectors class CapsuleLayer ( layers . Layer ): \"\"\" The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\ [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1. :param num_capsule: number of capsules in this layer :param dim_vector: dimension of the output vectors of the capsules in this layer :param num_routings: number of iterations for the routing algorithm \"\"\" def __init__ ( self , num_capsule , dim_vector , num_routing = 3 , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , ** kwargs ): super ( CapsuleLayer , self ) . __init__ ( ** kwargs ) self . num_capsule = num_capsule self . dim_vector = dim_vector self . num_routing = num_routing self . kernel_initializer = initializers . get ( kernel_initializer ) self . bias_initializer = initializers . get ( bias_initializer ) def build ( self , input_shape ): assert len ( input_shape ) >= 3 , \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\" self . input_num_capsule = input_shape [ 1 ] self . input_dim_vector = input_shape [ 2 ] # Transform matrix self . W = self . add_weight ( shape = [ self . input_num_capsule , self . num_capsule , self . input_dim_vector , self . dim_vector ], initializer = self . kernel_initializer , name = 'W' ) # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation. self . bias = self . add_weight ( shape = [ 1 , self . input_num_capsule , self . num_capsule , 1 , 1 ], initializer = self . bias_initializer , name = 'bias' , trainable = False ) self . built = True def call ( self , inputs , training = None ): # inputs.shape=[None, input_num_capsule, input_dim_vector] # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector] inputs_expand = K . expand_dims ( K . expand_dims ( inputs , 2 ), 2 ) # Replicate num_capsule dimension to prepare being multiplied by W # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector] inputs_tiled = K . tile ( inputs_expand , [ 1 , 1 , self . num_capsule , 1 , 1 ]) \"\"\" # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size. # Now W has shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector] w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1]) # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector] inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3]) \"\"\" # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow. # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector] inputs_hat = tf . scan ( lambda ac , x : K . batch_dot ( x , self . W , [ 3 , 2 ]), elems = inputs_tiled , initializer = K . zeros ([ self . input_num_capsule , self . num_capsule , 1 , self . dim_vector ])) \"\"\" # Routing algorithm V1. Use tf.while_loop in a dynamic way. def body(i, b, outputs): c = tf.nn.softmax(self.bias, dim=2) # dim=2 is the num_capsule dimension outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True)) b = b + K.sum(inputs_hat * outputs, -1, keepdims=True) return [i-1, b, outputs] cond = lambda i, b, inputs_hat: i > 0 loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)] _, _, outputs = tf.while_loop(cond, body, loop_vars) \"\"\" # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance assert self . num_routing > 0 , 'The num_routing should be > 0.' for i in range ( self . num_routing ): c = tf . nn . softmax ( self . bias , dim = 2 ) # dim=2 is the num_capsule dimension # outputs.shape=[None, 1, num_capsule, 1, dim_vector] outputs = squash ( K . sum ( c * inputs_hat , 1 , keepdims = True )) # last iteration needs not compute bias which will not be passed to the graph any more anyway. if i != self . num_routing - 1 : # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True)) self . bias += K . sum ( inputs_hat * outputs , - 1 , keepdims = True ) # tf.summary.histogram('BigBee', self.bias) # for debugging return K . reshape ( outputs , [ - 1 , self . num_capsule , self . dim_vector ]) def compute_output_shape ( self , input_shape ): return tuple ([ None , self . num_capsule , self . dim_vector ]) def PrimaryCap ( inputs , dim_vector , n_channels , kernel_size , strides , padding ): \"\"\" Apply Conv2D `n_channels` times and concatenate all capsules :param inputs: 4D tensor, shape=[None, width, height, channels] :param dim_vector: the dim of the output vector of capsule :param n_channels: the number of types of capsules :return: output tensor, shape=[None, num_capsule, dim_vector] \"\"\" output = layers . Conv2D ( filters = dim_vector * n_channels , ## 8 x 32 kernel_size = kernel_size , ## 9x9 strides = strides , ## 2 padding = padding , name = \"PrimaryCap_conv2d\" )( inputs ) outputs = layers . Reshape ( target_shape = [ - 1 , dim_vector ], name = \"PrimaryCap_reshape\" )( output ) return layers . Lambda ( squash , name = \"PrimaryCap_squash\" )( outputs ) def NetworkInputToDigitCap ( input_shape , n_class , num_routing ): x = layers . Input ( shape = input_shape ) # Layer 1: Just a conventional Conv2D layer conv1 = layers . Conv2D ( filters = 256 , kernel_size = 9 , strides = 1 , padding = 'valid' , activation = 'relu' , name = 'Conv1' )( x ) # Layer 2: Conv2D layer with `squash` activation, # then reshape to [None, num_capsule, dim_vector] primarycaps = PrimaryCap ( conv1 , dim_vector = 8 , n_channels = 32 , kernel_size = 9 , strides = 2 , padding = 'valid' ) # Layer 3: Capsule layer. Routing algorithm works here. digitcaps = CapsuleLayer ( num_capsule = n_class , dim_vector = 16 , num_routing = num_routing , name = 'DigitCaps' )( primarycaps ) return ( x , digitcaps ) CapsNet function In [3]: from keras import backend as K def CapsNet ( input_shape , n_class , num_routing ): \"\"\" A Capsule Network on MNIST. :param input_shape: data shape, 4d, [None, width, height, channels] :param n_class: number of classes :param num_routing: number of routing iterations :return: A Keras Model with 2 inputs and 2 outputs \"\"\" x , digitcaps = NetworkInputToDigitCap ( input_shape , n_class , num_routing ) # Layer 4: This is an auxiliary layer to replace each capsule with its length. # Just to match the true label's shape. # If using tensorflow, this will not be necessary. :) out_caps = Length ( name = 'out_caps' )( digitcaps ) ##||v|| margin_loss # Decoder network. y = layers . Input ( shape = ( n_class ,), name = \"true_label\" ) masked = Mask ( name = \"activity_vec\" )([ digitcaps , ## digitcaps.shape = (Nsample, n_class, dim_vector ) y ]) # The true label is used to mask the output of capsule layer. x_recon = Decoder ( masked ) # two-input-two-output keras Model m1 = models . Model ([ x , y ], [ out_caps , x_recon ]) m2 = models . Model ([ x ],[ digitcaps ]) ## YUMI added this line return m1 , m2 def Decoder ( masked ): ## Yumi refactored x_recon = layers . Dense ( 512 , activation = 'relu' , name = \"FC1\" )( masked ) x_recon = layers . Dense ( 1024 , activation = 'relu' , name = \"FC2\" )( x_recon ) x_recon = layers . Dense ( 784 , activation = 'sigmoid' , name = \"FC3\" )( x_recon ) ## mse to ensure that the reconstructed images are close to the original image. x_recon = layers . Reshape ( target_shape = [ 28 , 28 , 1 ], name = 'out_recon' )( x_recon ) return ( x_recon ) def margin_loss ( y_true , y_pred ): \"\"\" Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it. :param y_true: [None, n_classes] :param y_pred: [None, num_capsule] :return: a scalar loss value. \"\"\" L = y_true * K . square ( K . maximum ( 0. , 0.9 - y_pred )) + \\ 0.5 * ( 1 - y_true ) * K . square ( K . maximum ( 0. , y_pred - 0.1 )) return K . mean ( K . sum ( L , 1 )) CapsNet outputs: model: Keras's model object for entire CapsNet model_wo_decoder: Keras's model object without encoder parts CapsNet has decoder subnetwork which reconstruct the image from the 16-dimensional activity vector. The model without the decoder subnetwork only contains the single ouputs which are the output vectors of capsule. I create a model without the decoder subnetwork so that I can easily access the output vectors of capsules. These two model objects share weights. I will train using \"model\" this will automatically update the digitcaps's weights. CapsNet architecture: The summary shows CapsNet architecture. It is always good to check the understanding of model by checking the number of parameters produced by Keras model and your own calculation! input_1 : 28 x 28 frame image Conv1 : 256, 9 x 9 convolution kernels with a stride of 1 and Relu activation Param N : 20,992 = 256 x (1 x (9 x 9) + 1), +1 for bias The 1st and 2nd dim of output is 20 = (28 - 9 + 1)/1 PrimaryCamspules layer : convolusional capsule layer with 32 channels of convolutional 8D capsules (each primary capsule contains 8 convolutional units with a 9 x 9 kernel and a stride of 2). Param N : 5,308,672 = (32 x 8) x (256 x (9 x 9 ) + 1) The input of Primary Camspules is 256 20 x 20 feature maps. For each feature map, 9 x 9 kernel with stride 2 and no padding is applied 32 x 8 times. Each input feature map is reduced to 6 x 6 feature maps (6 = floor[ (20 - 9)/2 ] + 1) and there are 32 x 8 of such feature maps. Therefore, the output dimention is 6 x 6 x (32 x 8). Each group of 8 feature maps is considered as a capsule. This layer returns: $\\boldsymbol{u}_i \\in R&#94;8, i=1,..., 6 * 6 * 32$ DigiCaps layer : Param N : 1,486,080 = 16 x 8 x (6 x 6 x 32) x 10 + 6 x 6 x 32 x 10 This is the core layer for Capsule Network. The index $j$ represents the index for output class $j=1,...,10$ and let $c_{i,j} \\in R&#94;1,\\boldsymbol{W}_{i,j} \\in R&#94;{16 x 8} $ and $\\boldsymbol{v}_j \\in R&#94;{16}$. $$ \\widehat{\\boldsymbol{u}}_{j|i} = \\boldsymbol{W}_{i,j} \\boldsymbol{u}_i\\\\ \\boldsymbol{s}_j = \\sum_{i=1}&#94;{6 x 6 x32} c_{i,j} \\widehat{\\boldsymbol{u}}_{j|i}\\\\ \\boldsymbol{v}_j = \\frac{ ||\\boldsymbol{s}_j||&#94;2 }{ 1 + ||\\boldsymbol{s}_j||&#94;2 } \\frac{ \\boldsymbol{s}_j }{ ||\\boldsymbol{s}_j|| } $$ DigiCaps layer contains 2 types of unknown weights: $c_{i,j}$ and $\\boldsymbol{W}_{i,j}$. As $c_{i,j} \\in R&#94;1$, we have 1 x (6 x 6 x 32) x 10 parameters. As $\\boldsymbol{W}_{i,j} \\in R&#94;{16 x 8} $, we have 1,474,560 = (16 x 8 x (6 x 6 x 32) x 10) parameters for the affine transfomation matrix The output is 10 x 16. In [4]: def myprint (): print ( \"~~~\" * 10 ) ## define model n_class = 10 model , model_wo_decoder = CapsNet ( input_shape = [ 28 , 28 , 1 ], n_class = n_class , num_routing = 3 ) myprint () print ( \"Capsule Net (all)\" ) myprint () model . summary () myprint () print ( \"Capsule Net (without the decoder)\" ) myprint () model_wo_decoder . summary () ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Capsule Net (all) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ____________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 28, 28, 1) 0 ____________________________________________________________________________________________________ Conv1 (Conv2D) (None, 20, 20, 256) 20992 input_1[0][0] ____________________________________________________________________________________________________ PrimaryCap_conv2d (Conv2D) (None, 6, 6, 256) 5308672 Conv1[0][0] ____________________________________________________________________________________________________ PrimaryCap_reshape (Reshape) (None, 1152, 8) 0 PrimaryCap_conv2d[0][0] ____________________________________________________________________________________________________ PrimaryCap_squash (Lambda) (None, 1152, 8) 0 PrimaryCap_reshape[0][0] ____________________________________________________________________________________________________ DigitCaps (CapsuleLayer) (None, 10, 16) 1486080 PrimaryCap_squash[0][0] ____________________________________________________________________________________________________ true_label (InputLayer) (None, 10) 0 ____________________________________________________________________________________________________ activity_vec (Mask) (None, 16) 0 DigitCaps[0][0] true_label[0][0] ____________________________________________________________________________________________________ FC1 (Dense) (None, 512) 8704 activity_vec[0][0] ____________________________________________________________________________________________________ FC2 (Dense) (None, 1024) 525312 FC1[0][0] ____________________________________________________________________________________________________ FC3 (Dense) (None, 784) 803600 FC2[0][0] ____________________________________________________________________________________________________ out_caps (Length) (None, 10) 0 DigitCaps[0][0] ____________________________________________________________________________________________________ out_recon (Reshape) (None, 28, 28, 1) 0 FC3[0][0] ==================================================================================================== Total params: 8,153,360 Trainable params: 8,141,840 Non-trainable params: 11,520 ____________________________________________________________________________________________________ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Capsule Net (without the decoder) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ Conv1 (Conv2D) (None, 20, 20, 256) 20992 _________________________________________________________________ PrimaryCap_conv2d (Conv2D) (None, 6, 6, 256) 5308672 _________________________________________________________________ PrimaryCap_reshape (Reshape) (None, 1152, 8) 0 _________________________________________________________________ PrimaryCap_squash (Lambda) (None, 1152, 8) 0 _________________________________________________________________ DigitCaps (CapsuleLayer) (None, 10, 16) 1486080 ================================================================= Total params: 6,815,744 Trainable params: 6,804,224 Non-trainable params: 11,520 _________________________________________________________________ Load data Keras has nice API to extract MNIST data. Using keras.datasets, we can readily load data. from keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() However, I have a connection issue in accessing MNIST data via this API. So instead, I use python-mnist for loading data. Split between training and testing. Following paper, 60K and 10K images are used for training and testing respectively. I reshape the data. In [5]: from mnist import MNIST import numpy as np from keras.utils import to_categorical mndata = MNIST ( './MNIST_data/' ) images , labels = mndata . load_training () x_train , y_train = np . array ( images ), np . array ( labels ) images , labels = mndata . load_testing () x_test , y_test = np . array ( images ), np . array ( labels ) x_train = x_train . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. x_test = x_test . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. y_train = to_categorical ( y_train . astype ( 'float32' )) y_test = to_categorical ( y_test . astype ( 'float32' )) print ( x_train . shape , x_test . shape , y_train . shape , y_test . shape ) ((60000, 28, 28, 1), (10000, 28, 28, 1), (60000, 10), (10000, 10)) Train CapsNet As in paper, training is performed on 28 x 28 MNIST images that have been shifted by up to 2 pixels in each direction with zero padding. This is done using Keras's ImageDataGenerator API. See my prebious blog that discusses the useage of this API. In [6]: import pandas as pd from keras.preprocessing.image import ImageDataGenerator from keras import callbacks from keras.utils.vis_utils import plot_model from datetime import datetime from numpy.random import seed from tensorflow import set_random_seed def train ( model , data , epochs , epoch_size_frac = 1.0 ): \"\"\" Training a CapsuleNet :param model: the CapsuleNet model :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))` :param args: arguments :return: The trained model \"\"\" start = datetime . today () model . compile ( optimizer = 'adam' , loss = [ margin_loss , 'mse' ], loss_weights = [ 1. , 0.0005 ], metrics = { 'out_caps' : 'accuracy' }) ( x_train , y_train ), ( x_test , y_test ) = data # callbacks log = callbacks . CSVLogger ( 'log.csv' ) checkpoint = callbacks . ModelCheckpoint ( 'weights-{epoch:02d}.h5' , save_best_only = True , save_weights_only = True , verbose = False ) lr_decay = callbacks . LearningRateScheduler ( schedule = lambda epoch : 0.001 * np . exp ( - epoch / 10. )) # compile the model model . compile ( optimizer = 'adam' , loss = [ margin_loss , 'mse' ], loss_weights = [ 1. , 0.0005 ], metrics = { 'out_caps' : 'accuracy' }) \"\"\" # Training without data augmentation: model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs, validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint]) \"\"\" # -----------------------------------Begin: Training with data augmentation -----------------------------------# def train_generator ( x , y , batch_size , shift_fraction = 0. ): train_datagen = ImageDataGenerator ( width_shift_range = shift_fraction , height_shift_range = shift_fraction ) # shift up to 2 pixel for MNIST generator = train_datagen . flow ( x , y , batch_size = batch_size ) while 1 : x_batch , y_batch = generator . next () yield ([ x_batch , y_batch ], [ y_batch , x_batch ]) # Training with data augmentation. If shift_fraction=0., also no augmentation. hist = model . fit_generator ( generator = train_generator ( x_train , y_train , 64 , 0.1 ), steps_per_epoch = int ( epoch_size_frac * y_train . shape [ 0 ] / 64 ), epochs = epochs , verbose = 2 , validation_data = [[ x_test , y_test ], [ y_test , x_test ]], callbacks = [ log , checkpoint , lr_decay ]) # -----------------------------------End: Training with data augmentation -----------------------------------# end = datetime . today () print ( \"_\" * 10 ) print ( \"Time Took: {}\" . format ( end - start )) return hist The paper did not say exact number of epochs. Here, I try 100 epochs. In [7]: seed ( 1 ) set_random_seed ( 1 ) hist = train ( model = model , epochs = 100 , #50, data = (( x_train , y_train ), ( x_test , y_test )), epoch_size_frac = 1 ) Epoch 1/100 382s - loss: 0.0840 - out_caps_loss: 0.0839 - out_recon_loss: 0.0818 - out_caps_acc: 0.9204 - val_loss: 0.0248 - val_out_caps_loss: 0.0248 - val_out_recon_loss: 0.0585 - val_out_caps_acc: 0.9862 Epoch 2/100 388s - loss: 0.0255 - out_caps_loss: 0.0255 - out_recon_loss: 0.0658 - out_caps_acc: 0.9814 - val_loss: 0.0146 - val_out_caps_loss: 0.0146 - val_out_recon_loss: 0.0562 - val_out_caps_acc: 0.9898 Epoch 3/100 390s - loss: 0.0193 - out_caps_loss: 0.0193 - out_recon_loss: 0.0632 - out_caps_acc: 0.9838 - val_loss: 0.0122 - val_out_caps_loss: 0.0122 - val_out_recon_loss: 0.0553 - val_out_caps_acc: 0.9905 Epoch 4/100 391s - loss: 0.0151 - out_caps_loss: 0.0151 - out_recon_loss: 0.0620 - out_caps_acc: 0.9880 - val_loss: 0.0113 - val_out_caps_loss: 0.0112 - val_out_recon_loss: 0.0553 - val_out_caps_acc: 0.9910 Epoch 5/100 391s - loss: 0.0136 - out_caps_loss: 0.0136 - out_recon_loss: 0.0612 - out_caps_acc: 0.9884 - val_loss: 0.0096 - val_out_caps_loss: 0.0096 - val_out_recon_loss: 0.0545 - val_out_caps_acc: 0.9916 Epoch 6/100 391s - loss: 0.0118 - out_caps_loss: 0.0118 - out_recon_loss: 0.0607 - out_caps_acc: 0.9902 - val_loss: 0.0093 - val_out_caps_loss: 0.0093 - val_out_recon_loss: 0.0542 - val_out_caps_acc: 0.9919 Epoch 7/100 390s - loss: 0.0109 - out_caps_loss: 0.0109 - out_recon_loss: 0.0602 - out_caps_acc: 0.9908 - val_loss: 0.0085 - val_out_caps_loss: 0.0085 - val_out_recon_loss: 0.0539 - val_out_caps_acc: 0.9926 Epoch 8/100 386s - loss: 0.0092 - out_caps_loss: 0.0092 - out_recon_loss: 0.0601 - out_caps_acc: 0.9928 - val_loss: 0.0106 - val_out_caps_loss: 0.0106 - val_out_recon_loss: 0.0540 - val_out_caps_acc: 0.9911 Epoch 9/100 384s - loss: 0.0088 - out_caps_loss: 0.0088 - out_recon_loss: 0.0598 - out_caps_acc: 0.9926 - val_loss: 0.0075 - val_out_caps_loss: 0.0075 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9934 Epoch 10/100 384s - loss: 0.0078 - out_caps_loss: 0.0077 - out_recon_loss: 0.0596 - out_caps_acc: 0.9937 - val_loss: 0.0069 - val_out_caps_loss: 0.0068 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9931 Epoch 11/100 383s - loss: 0.0071 - out_caps_loss: 0.0071 - out_recon_loss: 0.0595 - out_caps_acc: 0.9940 - val_loss: 0.0072 - val_out_caps_loss: 0.0071 - val_out_recon_loss: 0.0536 - val_out_caps_acc: 0.9936 Epoch 12/100 382s - loss: 0.0063 - out_caps_loss: 0.0063 - out_recon_loss: 0.0594 - out_caps_acc: 0.9951 - val_loss: 0.0062 - val_out_caps_loss: 0.0062 - val_out_recon_loss: 0.0534 - val_out_caps_acc: 0.9948 Epoch 13/100 382s - loss: 0.0059 - out_caps_loss: 0.0058 - out_recon_loss: 0.0594 - out_caps_acc: 0.9952 - val_loss: 0.0066 - val_out_caps_loss: 0.0065 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9939 Epoch 14/100 381s - loss: 0.0055 - out_caps_loss: 0.0055 - out_recon_loss: 0.0593 - out_caps_acc: 0.9957 - val_loss: 0.0056 - val_out_caps_loss: 0.0055 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9949 Epoch 15/100 381s - loss: 0.0049 - out_caps_loss: 0.0049 - out_recon_loss: 0.0592 - out_caps_acc: 0.9961 - val_loss: 0.0057 - val_out_caps_loss: 0.0057 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9950 Epoch 16/100 381s - loss: 0.0046 - out_caps_loss: 0.0045 - out_recon_loss: 0.0592 - out_caps_acc: 0.9965 - val_loss: 0.0052 - val_out_caps_loss: 0.0052 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9950 Epoch 17/100 383s - loss: 0.0042 - out_caps_loss: 0.0042 - out_recon_loss: 0.0591 - out_caps_acc: 0.9969 - val_loss: 0.0055 - val_out_caps_loss: 0.0054 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9941 Epoch 18/100 384s - loss: 0.0039 - out_caps_loss: 0.0039 - out_recon_loss: 0.0591 - out_caps_acc: 0.9969 - val_loss: 0.0054 - val_out_caps_loss: 0.0054 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9952 Epoch 19/100 384s - loss: 0.0035 - out_caps_loss: 0.0034 - out_recon_loss: 0.0590 - out_caps_acc: 0.9973 - val_loss: 0.0055 - val_out_caps_loss: 0.0054 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9957 Epoch 20/100 384s - loss: 0.0033 - out_caps_loss: 0.0032 - out_recon_loss: 0.0590 - out_caps_acc: 0.9977 - val_loss: 0.0053 - val_out_caps_loss: 0.0053 - val_out_recon_loss: 0.0530 - val_out_caps_acc: 0.9948 Epoch 21/100 383s - loss: 0.0030 - out_caps_loss: 0.0030 - out_recon_loss: 0.0589 - out_caps_acc: 0.9978 - val_loss: 0.0053 - val_out_caps_loss: 0.0053 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9954 Epoch 22/100 383s - loss: 0.0030 - out_caps_loss: 0.0030 - out_recon_loss: 0.0589 - out_caps_acc: 0.9978 - val_loss: 0.0048 - val_out_caps_loss: 0.0048 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9948 Epoch 23/100 383s - loss: 0.0025 - out_caps_loss: 0.0025 - out_recon_loss: 0.0590 - out_caps_acc: 0.9983 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9960 Epoch 24/100 383s - loss: 0.0026 - out_caps_loss: 0.0026 - out_recon_loss: 0.0590 - out_caps_acc: 0.9981 - val_loss: 0.0048 - val_out_caps_loss: 0.0048 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9957 Epoch 25/100 383s - loss: 0.0023 - out_caps_loss: 0.0022 - out_recon_loss: 0.0588 - out_caps_acc: 0.9984 - val_loss: 0.0046 - val_out_caps_loss: 0.0045 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9957 Epoch 26/100 383s - loss: 0.0023 - out_caps_loss: 0.0023 - out_recon_loss: 0.0588 - out_caps_acc: 0.9984 - val_loss: 0.0047 - val_out_caps_loss: 0.0046 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9958 Epoch 27/100 383s - loss: 0.0020 - out_caps_loss: 0.0020 - out_recon_loss: 0.0588 - out_caps_acc: 0.9987 - val_loss: 0.0048 - val_out_caps_loss: 0.0047 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 28/100 383s - loss: 0.0020 - out_caps_loss: 0.0019 - out_recon_loss: 0.0588 - out_caps_acc: 0.9986 - val_loss: 0.0048 - val_out_caps_loss: 0.0048 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9953 Epoch 29/100 383s - loss: 0.0020 - out_caps_loss: 0.0019 - out_recon_loss: 0.0587 - out_caps_acc: 0.9987 - val_loss: 0.0043 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 30/100 380s - loss: 0.0016 - out_caps_loss: 0.0016 - out_recon_loss: 0.0587 - out_caps_acc: 0.9991 - val_loss: 0.0044 - val_out_caps_loss: 0.0044 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9961 Epoch 31/100 380s - loss: 0.0017 - out_caps_loss: 0.0017 - out_recon_loss: 0.0587 - out_caps_acc: 0.9989 - val_loss: 0.0045 - val_out_caps_loss: 0.0044 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9956 Epoch 32/100 380s - loss: 0.0017 - out_caps_loss: 0.0016 - out_recon_loss: 0.0588 - out_caps_acc: 0.9990 - val_loss: 0.0044 - val_out_caps_loss: 0.0044 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 33/100 380s - loss: 0.0015 - out_caps_loss: 0.0015 - out_recon_loss: 0.0587 - out_caps_acc: 0.9990 - val_loss: 0.0044 - val_out_caps_loss: 0.0044 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 34/100 383s - loss: 0.0015 - out_caps_loss: 0.0014 - out_recon_loss: 0.0588 - out_caps_acc: 0.9992 - val_loss: 0.0045 - val_out_caps_loss: 0.0045 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9953 Epoch 35/100 383s - loss: 0.0014 - out_caps_loss: 0.0014 - out_recon_loss: 0.0588 - out_caps_acc: 0.9992 - val_loss: 0.0045 - val_out_caps_loss: 0.0045 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 36/100 381s - loss: 0.0014 - out_caps_loss: 0.0014 - out_recon_loss: 0.0587 - out_caps_acc: 0.9992 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9953 Epoch 37/100 380s - loss: 0.0013 - out_caps_loss: 0.0013 - out_recon_loss: 0.0588 - out_caps_acc: 0.9993 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 38/100 380s - loss: 0.0013 - out_caps_loss: 0.0013 - out_recon_loss: 0.0588 - out_caps_acc: 0.9991 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 39/100 379s - loss: 0.0013 - out_caps_loss: 0.0013 - out_recon_loss: 0.0588 - out_caps_acc: 0.9993 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 40/100 380s - loss: 0.0012 - out_caps_loss: 0.0012 - out_recon_loss: 0.0588 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 41/100 380s - loss: 0.0012 - out_caps_loss: 0.0012 - out_recon_loss: 0.0587 - out_caps_acc: 0.9992 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9959 Epoch 42/100 381s - loss: 0.0011 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9960 Epoch 43/100 382s - loss: 0.0011 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 44/100 382s - loss: 0.0012 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 45/100 382s - loss: 0.0010 - out_caps_loss: 9.8727e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0043 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 46/100 382s - loss: 0.0011 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 47/100 382s - loss: 0.0010 - out_caps_loss: 9.8633e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 48/100 381s - loss: 0.0012 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 49/100 380s - loss: 0.0010 - out_caps_loss: 0.0010 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0043 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 50/100 379s - loss: 0.0010 - out_caps_loss: 9.9851e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9955 Epoch 51/100 380s - loss: 0.0011 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 52/100 379s - loss: 0.0010 - out_caps_loss: 9.7172e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 53/100 380s - loss: 9.6384e-04 - out_caps_loss: 9.3450e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 54/100 379s - loss: 9.5086e-04 - out_caps_loss: 9.2148e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0041 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 55/100 380s - loss: 9.2795e-04 - out_caps_loss: 8.9861e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0041 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 56/100 379s - loss: 0.0010 - out_caps_loss: 9.9036e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 57/100 380s - loss: 9.6289e-04 - out_caps_loss: 9.3352e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0041 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 58/100 380s - loss: 9.2247e-04 - out_caps_loss: 8.9310e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9996 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 59/100 379s - loss: 9.4866e-04 - out_caps_loss: 9.1929e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 60/100 380s - loss: 9.7747e-04 - out_caps_loss: 9.4812e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 61/100 381s - loss: 0.0010 - out_caps_loss: 9.8595e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 62/100 379s - loss: 9.6557e-04 - out_caps_loss: 9.3622e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0041 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 63/100 379s - loss: 9.8001e-04 - out_caps_loss: 9.5062e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 64/100 379s - loss: 9.3814e-04 - out_caps_loss: 9.0879e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9996 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 65/100 381s - loss: 9.6921e-04 - out_caps_loss: 9.3984e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 66/100 381s - loss: 9.7658e-04 - out_caps_loss: 9.4722e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 67/100 381s - loss: 9.5560e-04 - out_caps_loss: 9.2624e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 68/100 381s - loss: 9.9863e-04 - out_caps_loss: 9.6929e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 69/100 381s - loss: 9.9393e-04 - out_caps_loss: 9.6456e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 70/100 381s - loss: 9.1131e-04 - out_caps_loss: 8.8195e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9996 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 71/100 381s - loss: 9.9856e-04 - out_caps_loss: 9.6921e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 72/100 380s - loss: 9.1848e-04 - out_caps_loss: 8.8910e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 73/100 380s - loss: 9.3726e-04 - out_caps_loss: 9.0791e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 74/100 379s - loss: 0.0010 - out_caps_loss: 9.9644e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 75/100 379s - loss: 0.0010 - out_caps_loss: 9.8316e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 76/100 379s - loss: 9.9465e-04 - out_caps_loss: 9.6529e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 77/100 379s - loss: 0.0010 - out_caps_loss: 9.8456e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 78/100 379s - loss: 9.6042e-04 - out_caps_loss: 9.3105e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 79/100 379s - loss: 9.4492e-04 - out_caps_loss: 9.1553e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 80/100 379s - loss: 9.0155e-04 - out_caps_loss: 8.7220e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 81/100 379s - loss: 9.3271e-04 - out_caps_loss: 9.0336e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 82/100 379s - loss: 9.8777e-04 - out_caps_loss: 9.5841e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 83/100 379s - loss: 9.4561e-04 - out_caps_loss: 9.1624e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 84/100 379s - loss: 0.0010 - out_caps_loss: 9.7501e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 85/100 379s - loss: 9.1841e-04 - out_caps_loss: 8.8903e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 86/100 379s - loss: 9.4855e-04 - out_caps_loss: 9.1920e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 87/100 379s - loss: 9.4658e-04 - out_caps_loss: 9.1723e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 88/100 379s - loss: 8.9132e-04 - out_caps_loss: 8.6196e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 89/100 379s - loss: 9.4895e-04 - out_caps_loss: 9.1962e-04 - out_recon_loss: 0.0586 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 90/100 379s - loss: 9.2920e-04 - out_caps_loss: 8.9982e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 91/100 380s - loss: 9.9252e-04 - out_caps_loss: 9.6317e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 92/100 379s - loss: 9.4020e-04 - out_caps_loss: 9.1085e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 93/100 379s - loss: 9.8118e-04 - out_caps_loss: 9.5181e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 94/100 380s - loss: 9.4192e-04 - out_caps_loss: 9.1257e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 95/100 379s - loss: 9.0891e-04 - out_caps_loss: 8.7955e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 96/100 380s - loss: 9.5846e-04 - out_caps_loss: 9.2910e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 97/100 380s - loss: 9.3610e-04 - out_caps_loss: 9.0674e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 98/100 380s - loss: 8.9387e-04 - out_caps_loss: 8.6452e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 99/100 380s - loss: 9.2877e-04 - out_caps_loss: 8.9941e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 100/100 380s - loss: 9.8709e-04 - out_caps_loss: 9.5768e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 __________ Time Took: 10:36:17.718351 Save model architecutres and weights into current directory In [8]: def save_model ( model , name ): ''' save model architecture and model weights ''' json_string = model . to_json () open ( name + '_architecture.json' , 'w' ) . write ( json_string ) model . save_weights ( name + '_weights.h5' ) save_model ( model , \"CapsNet\" ) The size of the final model There are 8.2M parameters in this model. What is the size? In [9]: import os file_paths = [ \"CapsNet_weights.h5\" , \"CapsNet_architecture.json\" ] for file_path in file_paths : w = os . stat ( file_path ) . st_size print ( \"{:30} {:4.2f} MB\" . format ( file_path , w / ( 1024.0 ** 2 ))) CapsNet_weights.h5 31.13 MB CapsNet_architecture.json 0.01 MB Plotting accuracy and validation accuracy over epochs The validation accuracy (and loss, plot not shown) seems to be stabilized after 40 epochs. 100 epochs are more than enough for the convergence of the algorithm. In [10]: for label in [ \"out_caps_acc\" , \"val_out_caps_acc\" ]: plt . plot ( hist . history [ label ], label = label ) plt . title ( \"Final validation accuracy: {}\" . format ( hist . history [ \"val_out_caps_acc\" ][ - 1 ])) plt . legend () plt . ylim ( 0.98 , 1 ) plt . show () In [11]: def combine_images ( generated_images ): num = generated_images . shape [ 0 ] width = int ( np . sqrt ( num )) height = int ( np . ceil ( float ( num ) / width )) shape = generated_images . shape [ 1 : 3 ] image = np . zeros (( height * shape [ 0 ], width * shape [ 1 ]), dtype = generated_images . dtype ) for index , img in enumerate ( generated_images ): i = int ( index / width ) j = index % width image [ i * shape [ 0 ]:( i + 1 ) * shape [ 0 ], j * shape [ 1 ]:( j + 1 ) * shape [ 1 ]] = img [:, :, 0 ] return image def test ( model , data ): x_test , y_test = data y_pred , x_recon = model . predict ([ x_test , y_test ], batch_size = 100 ) print ( '-' * 50 ) class_pred = np . argmax ( y_pred , 1 ) class_test = np . argmax ( y_test , 1 ) acc = np . sum ( class_pred == class_test ) / float ( y_test . shape [ 0 ]) * 100 print ( 'Test accuracy: {:5.3f}%' . format ( acc )) print ( 'Test error : {:5.3f}%' . format (( 100 - acc ))) from PIL import Image img = combine_images ( np . concatenate ([ x_test [: 50 ], x_recon [: 50 ]])) image = img * 255 print ( '-' * 50 ) plt . figure ( figsize = ( 10 , 10 )) plt . imshow ( image ) plt . show () Test set results Testing error of paper was 0.25% with standard deviation of 0.005 after 3 trials. My error was 0.420%, not quite as low; I did not reproduce the result entirely. This is probablly because I only consider one testing set and not three, and some of the parameter values are not the same as paper , e.g., number of epochs or parameters of Adam's optimizer. (The paper use Tensorflow for implementation.) Unfortunatelly, the error of my CapsNet was even higher than the error produced by the baseline (0.39%). However, when I try fitting this baseline model to the MNIST data in my prebious blog , the validation loss of my baseline model was 0.5%. At least, the validation loss of my CapsNet model is less than the validation loss of the my baseline. This means that my CapsNet model classified additional 12 (= 10000x(0.540 - 0.420)/100) images correctly in comparisons to my baseline model. Correct classification of additional 8 images sounds low, and the statistical significance of improvment from the CapsNet is questionable for MNIST-like data. paper validates CapsNet's performance on other data such as CIFAR10 or smallNORB. I should also consider applying CapsNet for more complex image data. In [12]: ntest = x_test . shape [ 0 ] test ( model = model , data = ( x_test [: ntest ], y_test [: ntest ])) -------------------------------------------------- Test accuracy: 99.580% Test error : 0.420% -------------------------------------------------- Interprete Capsule Networks Since the Capsule Network passing the acitivity vector to encoding of only one digit and zeroing out other digits (Mask method is doing this in the code), the activity vector of the correct class $\\boldsymbol{v}_{\\textrm{correct}}$ should be able to reconstruct the image well while the activity vector of the incorrect $\\boldsymbol{v}_{\\textrm{not correct}}$ cannot reconstruct sensible image. Let's see if this is the case. In order to visualize the reconstructed images from every activity vector, we create a decoder model that takes activity vectors as input and output the reconstructed images. In [13]: dim_vector = 16 activity_vec = layers . Input ( shape = ( dim_vector ,), name = \"activity_vec\" ) decoder_out = Decoder ( activity_vec ) decoder_model = models . Model ( activity_vec , decoder_out ) decoder_model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= activity_vec (InputLayer) (None, 16) 0 _________________________________________________________________ FC1 (Dense) (None, 512) 8704 _________________________________________________________________ FC2 (Dense) (None, 1024) 525312 _________________________________________________________________ FC3 (Dense) (None, 784) 803600 _________________________________________________________________ out_recon (Reshape) (None, 28, 28, 1) 0 ================================================================= Total params: 1,337,616 Trainable params: 1,337,616 Non-trainable params: 0 _________________________________________________________________ Transfer the weights trained Capsule Network to the newly created decoder model. In [14]: for layer_CapsNet in model . layers : for layer_decoder in decoder_model . layers : if layer_decoder . name == layer_CapsNet . name : weight = layer_CapsNet . get_weights () if len ( weight ) != 0 : layer_decoder . set_weights ( weight ) Example reconstructed images from activity vector of each capsule in DigitCaps layer Observations Expectedly, the reconstructed image from the activity vector of the correct class reproduces the better image. We also see that when the model is not confident about the prediction ($|| \\boldsymbol{v}_j ||$ is high for multiple $j$), all reconstructed images of clases with high predicted probabilities are also well represented. See the example image for digit 2. The model thinks that the digit could be either digit 2 or 7. I am slighly surprized to see that many lengths of output vector of the capsule are almost zero, i.e., the probability that the image belongs to wrong digit class is almost zero, correctly. As the margin loss function was defined as: $$ L_k = T_k \\max(0, m&#94;+ - || \\boldsymbol{v}_k ||)&#94;2 + (1-T_k) \\max(0, || \\boldsymbol{v}_k || - m&#94;-)&#94;2 $$ where $T_k=1$ iff a digit of class $k$ is present, and $m&#94;+=0.9$ and $m&#94;-=0.1$. I interpret this loss as \"avoid model being over confident. The model is good enough if the probability estimate is < 0.1 when the class is incorrect and > 0.9 if the probability estimate is > 0.9 when the class is correct.\" So the probability estimate of 0.1 is just as confident as 0 that the class is incorrect, and 0.9 is just as confident as 0.95 that the class is correct. However, the model seems to return probability estimates of 0 pretty often when the class is incorrect. In [37]: #Npic = 5 #random_pick_pics = np.random.choice(x_test.shape[1],Npic,replace=False) random_pick_pics = [ 10 , ## 0 1097 , ## 1 924 , ## 2 looks like 7 912 , ## 3 5923 , ## 4 1948 , ## 5 3923 , ## 6 124 , ## 7 800 , ## 8 7 ## 9 ] Npic = len ( random_pick_pics ) count = 1 fig = plt . figure ( figsize = ( 20 , 2 * Npic )) for image_index in random_pick_pics : orig_image = x_test [[ image_index ]] V = model_wo_decoder . predict ( orig_image ) orig_image = orig_image . reshape ( 28 , 28 ) ax = fig . add_subplot ( Npic , 11 , count , xticks = [], yticks = []) ax . imshow ( orig_image ) ax . set_title ( \"original\" ) count += 1 for myclass , pred_class in enumerate ( range ( V . shape [ 1 ])): Vpred_class = V [ 0 , pred_class ,:] reconst_image = decoder_model . predict ( Vpred_class . reshape ( 1 , 16 )) ax = fig . add_subplot ( Npic , 11 , count , xticks = [], yticks = []) ax . imshow ( reconst_image . reshape ( 28 , 28 )) ax . set_title ( \"{}: ||v||={:3.2f}\" . format ( myclass , np . sum ( Vpred_class ** 2 ))) count += 1 plt . show () Evaluate the values of activity vectors In [16]: activity_vecs = model_wo_decoder . predict ( x_test ) print ( activity_vecs . shape ) (10000, 10, 16) Dimension perturbations One thing that really fascinates me about the CapsNet was the interpretability of the model. When working with deep learning models for customer projects, I often have difficulty in explaining why the model work (or not work) to non-exparts. Many researchers put efforts to visualize and interpret the model and I also discuss this in the previous post . It would be great if CapsNet can help me interpret the model! Hinton discusses the interpretability and says: \" ... dimensions of a digit capsule should learn to span the space of variations in the way digits of that class are instantiated. These variations include stroke thickness, skew and width. They also include digit-specific variations such as the length of the tail of a 2. We can see what the individual dimensions represent by making use of the decoder network. \" Each dimention of the acivity vector could represent stroke, thickness etc?! That is great! The intepretability of the model was explained with Figure 4 of page 6 in paper , which shows the reconstruction when one of the 16 dimensions in the DigitCaps representation is tweaked by intervals of 0.05 in the range [-0.25,0.25]. Figure 4 explains what the individual dimensions of a capsule represent. I try to reproduce images similar to Figure 4. Although the range of the dimensions are set to [-0.25,0.25], I rather want to make the range to span the entire possible values of the activity vector. Therefore, first, evaluate the range of each element of the activity vector of a capsule. Remind you that the activity vector $\\boldsymbol{v}_j$ is defined as: $$ \\boldsymbol{v}_j = \\frac{ ||\\boldsymbol{s}_j||&#94;2 }{ 1 + ||\\boldsymbol{s}_j||&#94;2 } \\frac{ \\boldsymbol{s}_j }{ ||\\boldsymbol{s}_j|| } $$ As it is defined to be scaled normalized vector with a scale ranging between 0 and 1, each element of the activity vector ranges between -1 and 1. The following code shows that the range is rather narrower. In [17]: min_avec = activity_vecs . min () max_avec = activity_vecs . max () print ( \"The range of each element of the output vector:\" ) print ( \"Min: {:4.2f}\" . format ( min_avec )) print ( \"Max: {:4.2f}\" . format ( max_avec )) avec_range = np . arange ( min_avec , max_avec ,( max_avec - min_avec ) / 5 ) plt . hist ( activity_vecs . flatten ()) plt . title ( \"The distribution of elements of the activity vector\" ) plt . show () The range of each element of the output vector: Min: -0.44 Max: 0.40 Observations Paper says \"We found that one dimension (out of 16) of the capsule almost always represents the width of the digit\". In my particular experiment, dimension 1 (and maybe 3 and 12) seems to measure the width of the digit. The thickness of the stroke is represented by dimension 8. For digit 6, the size of the loop relative to the size of the tail is measured by dimension 10. This seems to be the case for digit 4, and digit 9. In general, the intepretation is not super straightfoward, and you have to sit down and stare at the image a while. But it seems to be not impossible to interpret each dimension of the activity vector. In [18]: from copy import copy def plot_dimension_perturbation ( x_test1 , y_test1 , avec_range ): ''' x_test1 : np.array() of 1 x 28 x 28, containing image y_test1 : np.array() of length 10, binary array indicating class avec_range : the value of activity vectors to replace the original ''' y_test1 = y_test1 . flatten () true_class = np . where ( y_test1 == 1 )[ 0 ] V = model_wo_decoder . predict ( x_test1 ) const = 0.5 fig = plt . figure ( figsize = ( 16 * const , len ( avec_range ) * const )) fig . subplots_adjust ( hspace = 0.05 , wspace = 0.0001 , left = 0 , right = 1 , bottom = 0 , top = 1 ) count = 1 for value in avec_range : for di in range ( V . shape [ 2 ]): activity_vec = ( copy ( V [ 0 , true_class ,:])) . flatten () activity_vec [ di ] = value reconst_image = decoder_model . predict ( activity_vec . reshape ( 1 , 16 )) ax = fig . add_subplot ( len ( avec_range ), 16 , count , xticks = [], yticks = []) ax . imshow ( reconst_image . reshape ( 28 , 28 ), cmap = \"gray\" ) if value == avec_range [ 0 ]: ax . set_title ( \"V[{}]\" . format ( di )) if di == 0 : ax . set_ylabel ( \"V[i]={:3.2f}\" . format ( value ), rotation = 0 , fontsize = 10 , labelpad = 30 ) count += 1 plt . show () for image_index in [ 296 , ## 0 920 , ## 1 421 , ## 2 369 , ## 3 399 , ## 4 502 , ## 5 123 , ## 6 2123 , ## 7 998 , ## 8 1013 ## 9 ]: plot_dimension_perturbation ( x_test [[ image_index ]], y_test [[ image_index ]], avec_range ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Understanding and Experimenting Capsule Networks"},{"url":"CNN-modeling-with-image-translations-using-MNIST-data.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog, I train a standard CNN model on the MNIST data and assess its performance. In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"3\" #### 2 GPU1 #### 3 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 In [2]: from mnist import MNIST import numpy as np from keras.utils import to_categorical mndata = MNIST ( './MNIST_data/' ) images , labels = mndata . load_training () x_train , y_train = np . array ( images ), np . array ( labels ) images , labels = mndata . load_testing () x_test , y_test = np . array ( images ), np . array ( labels ) x_train = x_train . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. x_test = x_test . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. y_train = to_categorical ( y_train . astype ( 'float32' )) y_test = to_categorical ( y_test . astype ( 'float32' )) print ( x_train . shape , x_test . shape , y_train . shape , y_test . shape ) ((60000, 28, 28, 1), (10000, 28, 28, 1), (60000, 10), (10000, 10)) CNN model I define a standard CNN with three convolutional layers of 256, 256, 128 channels. Each has 5x5 kernels and stride of 1. The last convolutional layers are followed by two fully connected layers of size 328, 192. The last fully connected layer is connected with dropout to a 10 class softmax layer with cross entropy loss. According to Recent Hinton's paper , \"[This model] is designed to achieve the best performance on MNIST\" when model complexity is at this level. This model contains 35M parameters. In [3]: from keras.layers import Conv2D , MaxPooling2D , Flatten , Dropout , Activation , Dense from keras.models import Sequential from keras import optimizers from numpy.random import seed from tensorflow import set_random_seed def StandardCNN ( withDropout = False , n_class = 10 , input_shape = ( 96 , 96 , 1 )): ''' WithDropout: If True, then dropout regularlization is added. This feature is experimented later. ''' model = Sequential () model . add ( Conv2D ( 256 ,( 5 , 5 ), strides = 1 , activation = \"relu\" , padding = \"same\" , input_shape = input_shape )) ## 96 - 3 + 2 #model.add(MaxPooling2D(pool_size = (2,2))) ## 96 - (3-1)*2 model . add ( Conv2D ( 256 ,( 5 , 5 ), strides = 1 , activation = \"relu\" , padding = \"same\" )) #model.add(MaxPooling2D(pool_size = (2,2))) model . add ( Conv2D ( 128 ,( 5 , 5 ), strides = 1 , activation = \"relu\" , padding = \"same\" )) #model.add(MaxPooling2D(pool_size=(2,2))) model . add ( Flatten ()) model . add ( Dense ( 328 , activation = 'relu' )) model . add ( Dense ( 192 , activation = 'relu' )) ## 1024, 2048, 4096,8192 model . add ( Dropout ( 0.5 )) model . add ( Dense ( n_class )) model . add ( Activation ( 'softmax' , name = \"output\" )) model . compile ( loss = \"categorical_crossentropy\" , optimizer = optimizers . Adam ( lr = 0.0005 ), metrics = { 'output' : 'accuracy' }) return ( model ) model = StandardCNN ( input_shape = ( 28 , 28 , 1 )) model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 28, 28, 256) 6656 _________________________________________________________________ conv2d_2 (Conv2D) (None, 28, 28, 256) 1638656 _________________________________________________________________ conv2d_3 (Conv2D) (None, 28, 28, 128) 819328 _________________________________________________________________ flatten_1 (Flatten) (None, 100352) 0 _________________________________________________________________ dense_1 (Dense) (None, 328) 32915784 _________________________________________________________________ dense_2 (Dense) (None, 192) 63168 _________________________________________________________________ dropout_1 (Dropout) (None, 192) 0 _________________________________________________________________ dense_3 (Dense) (None, 10) 1930 _________________________________________________________________ output (Activation) (None, 10) 0 ================================================================= Total params: 35,445,522 Trainable params: 35,445,522 Non-trainable params: 0 _________________________________________________________________ Training We set the number of epochs to be 20. In [4]: %% time seed(1) set_random_seed(1) hist1 = model.fit(x_train, y_train, batch_size=64, nb_epoch=20, validation_data=[x_test, y_test],verbose=2) /home/fairy/anaconda2/lib/python2.7/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`. warnings.warn('The `nb_epoch` argument in `fit` ' Train on 60000 samples, validate on 10000 samples Epoch 1/20 52s - loss: 0.1395 - acc: 0.9581 - val_loss: 0.0349 - val_acc: 0.9884 Epoch 2/20 51s - loss: 0.0480 - acc: 0.9862 - val_loss: 0.0340 - val_acc: 0.9889 Epoch 3/20 51s - loss: 0.0308 - acc: 0.9907 - val_loss: 0.0261 - val_acc: 0.9924 Epoch 4/20 51s - loss: 0.0235 - acc: 0.9933 - val_loss: 0.0336 - val_acc: 0.9912 Epoch 5/20 51s - loss: 0.0183 - acc: 0.9948 - val_loss: 0.0345 - val_acc: 0.9911 Epoch 6/20 50s - loss: 0.0138 - acc: 0.9958 - val_loss: 0.0326 - val_acc: 0.9916 Epoch 7/20 50s - loss: 0.0109 - acc: 0.9969 - val_loss: 0.0280 - val_acc: 0.9918 Epoch 8/20 50s - loss: 0.0097 - acc: 0.9974 - val_loss: 0.0373 - val_acc: 0.9915 Epoch 9/20 50s - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0449 - val_acc: 0.9911 Epoch 10/20 50s - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0413 - val_acc: 0.9908 Epoch 11/20 50s - loss: 0.0065 - acc: 0.9984 - val_loss: 0.0324 - val_acc: 0.9914 Epoch 12/20 50s - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0345 - val_acc: 0.9922 Epoch 13/20 50s - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0456 - val_acc: 0.9906 Epoch 14/20 51s - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0525 - val_acc: 0.9892 Epoch 15/20 51s - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0429 - val_acc: 0.9922 Epoch 16/20 50s - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0453 - val_acc: 0.9925 Epoch 17/20 50s - loss: 0.0061 - acc: 0.9985 - val_loss: 0.0521 - val_acc: 0.9908 Epoch 18/20 50s - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0413 - val_acc: 0.9931 Epoch 19/20 50s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0392 - val_acc: 0.9929 Epoch 20/20 50s - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0516 - val_acc: 0.9917 CPU times: user 13min 8s, sys: 1min 56s, total: 15min 5s Wall time: 16min 57s The back propagation algorithm seems to converge. In [5]: for label in [ \"val_acc\" , \"acc\" ]: plt . plot ( hist1 . history [ label ], label = label ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"accuracy\" ) plt . title ( \"validation accuracy, model1:{:6.5f}\" . format ( hist1 . history [ \"val_acc\" ][ - 1 ])) plt . legend () plt . ylim ( 0.95 , 1.005 ) plt . show () Train the model with data augmentation I now train the same model with image translation. \"translation of image\" means shifting image by costant value to right, left, up or down, or some combinations of these. Keras provides the ImageDataGenerator class that defines the configuration for image data preparation and augmentation. Rather than performing the operations on your entire image dataset in memory, the ImageDataGenerator API is designed to be iterated by the deep learning model fitting process, creating augmented image data for you just-in-time. This reduces your memory overhead, but adds some additional time cost during model training. I discussed the useage of ImageDataGenerator in previous blog post . I also found Jason Brownlee's blog is very useful for learning ImageDataGenerator. train_generator First, define train_generator method which infenitely return training and testing batch. batch_size determines the batch size. generator.next() randomly select batch_size many samples from x_train, then the random transformation is applied to the selected images. If the x_train.shape[0] < batch_size, the returned batch_size is x_train.shape[0]. In [6]: from keras.preprocessing.image import ImageDataGenerator def train_generator ( x , y , batch_size , shift_fraction = 0. ): train_datagen = ImageDataGenerator ( width_shift_range = shift_fraction , height_shift_range = shift_fraction ) # shift up to 2 pixel for MNIST generator = train_datagen . flow ( x , y , batch_size = batch_size ) while 1 : x_batch , y_batch = generator . next () ## x_batch.shape = (bsize,28,28,1) ## where bsize = np.min(batch_size,x.shape[0]) yield ([ x_batch , y_batch ]) The following code shows the images generated by train_generator when only 5 images are fed into the train_generator. Although the batch_size is set to 1,000, every batch only yields 5 transformed images, and original image is transoformed only once. In [7]: def plot_sample ( x_train , y_train , batch_size , shift_fraction = 0.4 ): count = 0 for generator in train_generator ( x_train , y_train , batch_size , shift_fraction ): x_tra , y_tra = generator count += 1 if count > 0 : break print ( x_tra . shape , y_tra . shape ) n = 5 fig = plt . figure ( figsize = ( 10 , 10 )) for i in range ( x_tra . shape [ 0 ]): ax = fig . add_subplot ( n , n , i + 1 ) ax . imshow ( x_tra [ i ] . reshape ( 28 , 28 )) if ( n * n ) < ( i + 2 ): break plt . show () plot_sample ( x_train [: 5 ], y_train [: 5 ], batch_size = 1000 , shift_fraction = 0.4 ) plot_sample ( x_train [: 5 ], y_train [: 5 ], batch_size = 1000 , shift_fraction = 0.4 ) ((5, 28, 28, 1), (5, 10)) ((5, 28, 28, 1), (5, 10)) Model training I will set the shift_fraction = 0.1. This means that each image is shifted at most 3 pixels (28x0.1 = 2.8). I noticed that fit_generator causes error messages when steps_per_epoch is NOT specified. This is \"Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch.\" In [8]: %% time seed(1) set_random_seed(1) batch_size = 64 model2 = StandardCNN(input_shape = (28, 28, 1)) hist2 = model2.fit_generator( generator=train_generator(x_train, y_train, batch_size, 0.1), epochs=20, verbose=2, steps_per_epoch=int(y_train.shape[0] / batch_size), validation_data=[x_test, y_test])#,callbacks=[log, checkpoint, lr_decay]) Epoch 1/20 51s - loss: 0.2261 - acc: 0.9295 - val_loss: 0.0298 - val_acc: 0.9899 Epoch 2/20 50s - loss: 0.0744 - acc: 0.9790 - val_loss: 0.0329 - val_acc: 0.9884 Epoch 3/20 50s - loss: 0.0512 - acc: 0.9858 - val_loss: 0.0330 - val_acc: 0.9900 Epoch 4/20 50s - loss: 0.0418 - acc: 0.9879 - val_loss: 0.0280 - val_acc: 0.9911 Epoch 5/20 50s - loss: 0.0380 - acc: 0.9890 - val_loss: 0.0191 - val_acc: 0.9939 Epoch 6/20 50s - loss: 0.0332 - acc: 0.9908 - val_loss: 0.0206 - val_acc: 0.9934 Epoch 7/20 50s - loss: 0.0292 - acc: 0.9913 - val_loss: 0.0276 - val_acc: 0.9924 Epoch 8/20 50s - loss: 0.0267 - acc: 0.9927 - val_loss: 0.0195 - val_acc: 0.9936 Epoch 9/20 50s - loss: 0.0247 - acc: 0.9928 - val_loss: 0.0166 - val_acc: 0.9949 Epoch 10/20 50s - loss: 0.0212 - acc: 0.9938 - val_loss: 0.0176 - val_acc: 0.9949 Epoch 11/20 50s - loss: 0.0208 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9945 Epoch 12/20 50s - loss: 0.0178 - acc: 0.9945 - val_loss: 0.0172 - val_acc: 0.9949 Epoch 13/20 50s - loss: 0.0185 - acc: 0.9950 - val_loss: 0.0233 - val_acc: 0.9946 Epoch 14/20 50s - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0182 - val_acc: 0.9942 Epoch 15/20 50s - loss: 0.0145 - acc: 0.9959 - val_loss: 0.0305 - val_acc: 0.9928 Epoch 16/20 50s - loss: 0.0168 - acc: 0.9953 - val_loss: 0.0185 - val_acc: 0.9945 Epoch 17/20 50s - loss: 0.0139 - acc: 0.9960 - val_loss: 0.0227 - val_acc: 0.9942 Epoch 18/20 50s - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0176 - val_acc: 0.9951 Epoch 19/20 50s - loss: 0.0124 - acc: 0.9963 - val_loss: 0.0284 - val_acc: 0.9936 Epoch 20/20 50s - loss: 0.0115 - acc: 0.9969 - val_loss: 0.0210 - val_acc: 0.9946 CPU times: user 17min 4s, sys: 1min 56s, total: 19min Wall time: 16min 53s Result The final validation accuracy is 0.995 (the validation error is 0.540%). The data augmentation approach improved the validation accuracy by 0.0029. As the validation data contains 10K images, this means that additional 29 (=10000x0.0029) images are correctly classified. According to Recent Hinton's paper , this model could return the validation error of 0.39%. My final model is not as good. I probablly need to tweek the backpropagation parameters and dropout rate. Some comments on algorithm's termination criteria In this excercise, I did not formaly split the data into train, validation and test sets. Instead, I treated testing data also as a validation data. This is OK if testing data is not used for deciding the model parameter updates. However, it is NOT OK if the testing data is used for this purpose, e.g., deciding when to terminate the algorithm. I selected the model updated at the final epoch as the final model; my algorithm termination criteria is simply \"stop at 50th epoch\". This simple approach may cause issues, because the model may be overfitted or underfitted at the pre-specified epoch size. It is wiser to decide the final model by introducing validation set and considering the early stopping when the validation loss is minimized. In [23]: m1_err = hist1 . history [ \"val_acc\" ][ - 1 ] m2_err = hist2 . history [ \"val_acc\" ][ - 1 ] for label in [ \"val_acc\" , \"acc\" ]: plt . plot ( hist1 . history [ label ], \"--\" , label = \"model1 \" + label ) plt . plot ( hist2 . history [ label ], label = \"model2 \" + label ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"accuracy\" ) plt . title ( \"validation accuracy, model1: {:6.5f}, model2: {:6.5f}\" . format ( m1_err , m2_err )) plt . legend () plt . ylim ( 0.95 , 1.005 ) plt . show () print ( \"validation error: \\n model1: {:5.3f}%, model2: {:5.3f}%\" . format ( ( 1 - m1_err ) * 100 ,( 1 - m2_err ) * 100 )) print ( \"validation accuracy improvement: {}\" . format ( m2_err - m1_err )) validation error: model1: 0.830%, model2: 0.540% validation accuracy improvement: 0.0029 Save model In [10]: from keras.models import model_from_json def save_model ( model , name ): ''' save model architecture and model weights ''' json_string = model . to_json () open ( name + '_architecture.json' , 'w' ) . write ( json_string ) model . save_weights ( name + '_weights.h5' ) save_model ( model2 , name = \"standard\" ) Due to the large number of parameters (35M), the model weight object is hudge! In [11]: import os file_paths = [ \"standard_weights.h5\" , \"standard_architecture.json\" ] for file_path in file_paths : w = os . stat ( file_path ) . st_size print ( \"{:30} {:4.2f} MB\" . format ( file_path , w / ( 1024.0 ** 2 ))) standard_weights.h5 135.24 MB standard_architecture.json 0.00 MB if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"CNN modeling with image translations using MNIST data"},{"url":"Learn-the-breed-of-a-dog-using-deep-learning.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } My friend asked me if I can figure out the breed of his dog, Loki. As I am not a dog expart, I will ask opinions from deep learning. Here, I use VGG16 trained on ImageNet dataset. What is VGG16 and ImageNet? According to Wikipedia , \"The ImageNet project is a large visual database designed for use in visual object recognition software research...Since 2010, the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a competition where research teams evaluate their algorithms on the given data set, and compete to achieve higher accuracy on several visual recognition tasks.\" The good think about this dataset is that it contains 120 categories of dog breeds. So the fine tuned model in this data should be able to classify Loki to the right breed! VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. It was used to win the ILSVR (ImageNet) competition in 2014. In [1]: import os import matplotlib.pyplot as plt import numpy as np In [2]: import tensorflow as tf from keras.backend.tensorflow_backend import set_session print ( tf . __version__ ) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"2\" #### 1 GPU1 #### 2 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) Using TensorFlow backend. 1.2.1 My data folder contains 6 images of Loki. Load them into python and save them as a numpy array. The image is converted into 3 dimentional numpy array by the method \"img_to_array\" In [4]: ls data / IMG-20180108-WA0004.jpg * WP_20150225_15_18_47_Pro.jpg * IMG-20180122-WA0003.jpg * WP_20171222_09_09_22_Pro.jpg * WP_20150225_14_59_57_Pro.jpg * WP_20180120_12_00_15_Pro.jpg * In [5]: from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array pic_nms = os . listdir ( \"data/\" ) fig = plt . figure ( figsize = ( 10 , 10 )) count = 1 myimages = [] for pic_nm in pic_nms : axs = fig . add_subplot ( 3 , len ( pic_nms ) / 2 , count ) image = load_img ( 'data/' + pic_nm , target_size = ( 224 , 224 , 3 )) axs . imshow ( image ) axs . set_title ( count ) image = img_to_array ( image ) myimages . append ( image ) count += 1 plt . show () The picture 1 needs to be rotated by 90 degree. For rotating picture, I use opencv. Nicely, pip was available for installation: pip install opencv - python In [6]: import cv2 print ( cv2 . __version__ ) 3.4.0 In [7]: def rotateImage ( image , angle ): image_center = tuple ( np . array ( image . shape [ 1 :: - 1 ]) / 2 ) rot_mat = cv2 . getRotationMatrix2D ( image_center , angle , 1.0 ) result = cv2 . warpAffine ( image , rot_mat , image . shape [ 1 :: - 1 ], flags = cv2 . INTER_LINEAR ) return result image = rotateImage ( myimages [ 0 ], 270 ) Let's check if the image is correctly rotated. Here we have to be careful. The image array converted from the img_to_array has a data type float which is not accepted by plt.imshow. I need to convert its type to unit8. For this casting, I use: np . unit8 ( image ) Please see the discussion here In [8]: print ( image . dtype ) pimage = np . uint8 ( image ) print ( pimage . dtype ) plt . imshow ( pimage ) plt . show () myimages [ 0 ] = image float32 uint8 In [9]: os . chdir ( \"../FacialKeypoint/\" ) Load VGG16 Images are loaded into python as correct formats. Now, we import the VGG16 model. I assume that the followings are available in the current directory. vgg16_weights_tf_dim_ordering_tf_kernels.h5 imagenet_class_index.json See my past blog post which discusses where they are downloaded. In [10]: from keras.applications import VGG16 model = VGG16 ( include_top = True , weights = None ) ## load the locally saved weights model . load_weights ( \"vgg16_weights_tf_dim_ordering_tf_kernels.h5\" ) Create an array containing all the 1000 class labels of ImageNet In [11]: import json CLASS_INDEX = json . load ( open ( \"imagenet_class_index.json\" )) classlabel = [] for i in range ( 1000 ): classlabel . append ( CLASS_INDEX [ str ( i )][ 1 ]) classlabel = np . array ( classlabel ) print ( len ( classlabel )) 1000 Classification The following script predict the class label for each image. One note: RGB to BGR According to this , there are differences in pixel ordering in various modules in python e.g., OpenCV and Matplotlib. Here is a note: OpenCV: BGR order matplotlib: RGB order Keras's img_to_array: RGB order VGG16: BGR order (because it was created using Caffee which uses OpenCV) So I need to re order my image array before prediction. In [12]: ## top 5 selected classes from copy import copy top = 5 prob_string = [] y_preds = [] for image in myimages : image = copy ( image [:,:,:: - 1 ]) ## change order to BGR y_pred = model . predict ( image . reshape ( 1 , image . shape [ 0 ], image . shape [ 1 ], image . shape [ 2 ])) . flatten () chosen_classes = classlabel [ np . argsort ( y_pred )[:: - 1 ][: top ] ] y_preds . append ( y_pred ) mystrs = \"\" for myclass in chosen_classes : myprob = y_pred [ classlabel == myclass ][ 0 ] mystr = \"{:10} p={:5.3}% \\n \" . format ( myclass , myprob * 100 ) mystrs += \" \" + mystr #print(mystr) prob_string . append ( mystrs ) Plot the images together with the top 5 likely class labels It seems like VGG16 is doing good jobs for classifing Loki except the second image: In all but the second image, Loki is classified into some kind of dog class. Loki is not classified into a reasonable dog class in the second picture, probablly because the ImageNet data does not have many data with a dog surrounded by this much snow. In [13]: fig = plt . figure ( figsize = ( 15 , 15 )) count = 1 for image , pstr in zip ( myimages , prob_string ): axs = fig . add_subplot ( 3 , len ( myimages ) / 2 , count ) axs . imshow ( np . uint8 ( image )) axs . set_title ( count ) axs . text ( 80 , 220 , pstr , bbox = { 'facecolor' : 'white' , 'alpha' : 0.95 , 'pad' : 10 }) axs . set_xticks ([]) axs . set_yticks ([]) count += 1 plt . show () So what is the Loki's breed? The predicted classes are somewhat different across images. To give an overall answer, I will aggregate the probability estimates obtained from all pictures. In [14]: y_preds = np . array ( y_preds ) y_pred_all = np . mean ( y_preds , axis = 0 ) print ( \"Make sure that the probability sums up to 1: {}\" . format ( np . sum ( y_pred_all ))) Make sure that the probability sums up to 1: 1.0 Plot below shows the top 30 overall probabilities for Loki's class. It seems that Loki is most likely black and tan coonhound. Loki may be a weevil with 10% chance. In [15]: top = 30 yorder = np . argsort ( y_pred_all )[:: - 1 ] classes_reorder = classlabel [ yorder ][: top ] y_pred_reorder = y_pred_all [ yorder ][: top ] mystrs = \"\" xaxis = range ( len ( y_pred_reorder )) plt . figure ( figsize = ( 10 , 3 )) plt . bar ( xaxis , y_pred_reorder ) plt . xticks ( xaxis , classes_reorder , rotation = 90 ) plt . title ( \"Top {} probailities for Loki's class label\" . format ( top )) plt . show () print ( \"Loki is:\" ) for p , cl in zip ( y_pred_reorder , classes_reorder ): print ( \"{:5.3}% {}\" . format ( p * 100 , cl )) Loki is: 24.8% black-and-tan_coonhound 18.6% Rottweiler 16.0% Doberman 9.74% weevil 6.63% Great_Dane 3.99% kelpie 2.18% Chesapeake_Bay_retriever 1.63% tick 1.4% Weimaraner 1.05% slug 0.917% miniature_pinscher 0.884% lacewing 0.858% bluetick 0.736% Labrador_retriever 0.73% bloodhound 0.714% redbone 0.618% fountain 0.53% curly-coated_retriever 0.514% leafhopper 0.482% schipperke 0.443% cockroach 0.327% German_short-haired_pointer 0.313% Gordon_setter 0.303% flat-coated_retriever 0.282% Airedale 0.278% giant_schnauzer 0.179% German_shepherd 0.172% Greater_Swiss_Mountain_dog 0.165% ant 0.156% ladybug if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Learn the breed of a dog using deep learning"},{"url":"The-first-deep-learning-model-for-NLP-Let-AI-tweet-like-President-Trump.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } The goal of this blog is to learn the functionalities of Keras for language processing applications. In the first section, I create a very simple single-word-in single-word-out model based on a single sentence. With this application, I make sure that the model works in this simple possible scenario and it can correctly predict the next word given the current word correctly for this trainning sentence. In the second section, we will use a similar model with more nodes to more than one sentence. Here I try to create AI that tweets like President Trump. I will use the President Trump's latest ~3,000 tweets to train the model. The data extraction procedure using tweepy is previously discussed . How to Develop Word-Based Neural Language Models in Python with Keras Using pre-trained word embeddings in a Keras model Text Generation With LSTM Recurrent Neural Networks in Python with Keras In [1]: import sys print ( sys . version ) import keras print ( \"keras {}\" . format ( keras . __version__ )) import tensorflow as tf print ( \"tensorflow {}\" . format ( tf . __version__ )) import numpy as np print ( \"numpy {}\" . format ( np . __version__ )) import matplotlib.pyplot as plt 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] Using TensorFlow backend. keras 2.0.6 tensorflow 1.2.1 numpy 1.11.3 In [2]: from keras.backend.tensorflow_backend import set_session print ( tf . __version__ ) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" #### 1 GPU1 #### 2 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) 1.2.1 Very very simple example: One-word-in one-word-out model The first training data is a single sentence with 11 words followed by three exclamation marks. I will create a simple LSTM model using this single sentence. The model should be able to overfit and reproduce this sentence! In [3]: # source text data = \"\"\"I want my deep learning model to guess this sentence perfectly ... YES!\"\"\" First create Tokenizer object. Tokenizer creates a mapping between word and idnex. The mapping is recorded in dictionary: key = words, value = index. The dictionary can be accessed via \"tokenizer.word_index\". Observation The word_index removes special character e.g., ... or ! All the words are converted to lower case The indexing starts from '1' NOT zero! In [4]: from keras.preprocessing.text import Tokenizer tokenizer = Tokenizer () tokenizer . fit_on_texts ([ data ]) for key , value in tokenizer . word_index . items (): print ( \"key:{:10} value:{:4}\" . format ( key , value )) key:this value: 9 key:guess value: 8 key:want value: 2 key:sentence value: 10 key:i value: 1 key:deep value: 4 key:to value: 7 key:learning value: 5 key:model value: 6 key:perfectly value: 11 key:my value: 3 key:yes value: 12 tokenizer.texts_to_sequences converts string to a list of index. The index is from tokenizer.word_index. You see that the index is in the order of the words appearing in the sentence. In [5]: encoded = tokenizer . texts_to_sequences ([ data ])[ 0 ] ## [0] to extract the first sentence print ( encoded ) print ( \"The sentence has {} words\" . format ( len ( encoded ))) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] The sentence has 12 words Define the vocabulary size to the number of unique words + 1. This vocab_size is used for defining the number of classes to predict. Plus 1 is neccesary to include class \"0\". tokenizer.word_index.values() has no word defined with 0. So we can potentially use this class 0 for a place holder class (i.e., padding class). In [6]: ## + 1 for potential padding vocab_size = len ( tokenizer . word_index ) + 1 print ( \"The sentence has {} unique words\" . format ( len ( np . unique ( tokenizer . word_index . keys ())))) print ( \"> vocab_size={}\" . format ( vocab_size )) The sentence has 12 unique words > vocab_size=13 Prepare training data X, y. As we are creating one-word-in one-word out model, X.shape = (N words in sentence - 1, 1) y.shape = (N words in sentence - 1, 1) In [7]: sequences = [] n_lookback = 1 for i in range ( n_lookback , len ( encoded )): sequences . append ( encoded [( i - n_lookback ): i + 1 ]) # split into X and y elements sequences = np . array ( sequences ) X , y = sequences [:, 0 ], sequences [:, 1 ] print ( X . shape , y . shape ) ((11,), (11,)) One hot encoding for outputs. Notice that num_class is set to vocab_size which is N of unique words + 1. The printing of y shows that there is no row that has index in the 0th and 1st columns. This is because: there is no word belonging to class 0 in our original sentence \"data\". the word indexed with 1 appears at the start of the sentence hence it does not appear in the target y. In [8]: from keras.utils import to_categorical y = to_categorical ( y , num_classes = vocab_size ) print ( y . shape ) print ( y ) (11, 13) [[ 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] Define model and train data The model is simple; one embedding layer followed by one LSTM layer and then feed foward layer. \"Word embeddings\" are a family of natural language processing techniques aiming at mapping semantic meaning of each word into a geometric space. Parameter Embedding layer: for each word, create a continuous vector of length 10 to represent itself 130 parameters = \"vocab_size\" x 10 LSTM layers: 10 hidden units each has 4 gates 840 parameters = 10 hidden LSTM untis 4 (3 gates and 1 state) ((10 input + 1 bias) + 10 hidden LSTM untis) Feed forward layer: 143 parameters = (10 hidden LSTM units + 1 bias) x 13 class In [9]: from keras.models import Model from keras.layers import Input , Dense , Activation , Embedding , LSTM def define_model ( vocab_size , input_length = 1 , dim_dense_embedding = 10 , hidden_unit_LSTM = 5 ): main_input = Input ( shape = ( input_length ,), dtype = 'int32' , name = 'main_input' ) embedding = Embedding ( vocab_size , dim_dense_embedding , input_length = input_length )( main_input ) x = LSTM ( hidden_unit_LSTM )( embedding ) main_output = Dense ( vocab_size , activation = 'softmax' )( x ) model = Model ( inputs = [ main_input ], output = [ main_output ]) print ( model . summary ()) return ( model ) model = define_model ( vocab_size , input_length = 1 , dim_dense_embedding = 10 , hidden_unit_LSTM = 10 ) # compile network model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) # fit network hist = model . fit ( X , y , epochs = 500 , verbose = False ) /home/fairy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)` _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 1) 0 _________________________________________________________________ embedding_1 (Embedding) (None, 1, 10) 130 _________________________________________________________________ lstm_1 (LSTM) (None, 10) 840 _________________________________________________________________ dense_1 (Dense) (None, 13) 143 ================================================================= Total params: 1,113 Trainable params: 1,113 Non-trainable params: 0 _________________________________________________________________ None The traiing accuracy is perfect, indicating that the model can predict perfectly for training sentence. In [10]: plt . plot ( hist . history [ \"acc\" ]) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"accuracy\" ) plt . show () Now check if my model can correctly generate the trained sentence. Generate a 13 words sentence starting with 'I'. Successfully it generates the original sentence. The original sentence has 12 words so the 13th word predicted after \"yes\" could be anything. In this case the word after yes was predicted to be \"to\". But this value changes if you train with different initial values. In [11]: def predict_sentence ( model , tokenizer , in_text , n_words ): index_word = { v : k for k , v in tokenizer . word_index . items ()} encoded = tokenizer . texts_to_sequences ([ in_text ])[ 0 ] encoded = np . array ( encoded ) words = [ in_text ] for _ in range ( n_words - 1 ): ## encoded is index probs = model . predict ( encoded , verbose = 0 ) encoded = np . argmax ( probs , axis = 1 )[ 0 ] word = index_word [ encoded ] encoded = np . array ([ encoded ]) words . append ( word ) return ( words ) n_words = 13 in_text = 'I' pred_sentence = predict_sentence ( model , tokenizer , in_text , n_words ) print ( \"Predicted sentence with {} words:\" . format ( n_words )) for k in pred_sentence : print ( k ), Predicted sentence with 13 words: I want my deep learning model to guess this sentence perfectly yes to Look at the probability distribution of the word given the previous word. In [12]: # words is a list of vocabularly words such that words[word_index.values()[i]] = word_index.keys()[i] words = tokenizer . word_index . keys () words = np . array ( words ) words = list ( words [ np . argsort ( tokenizer . word_index . values ())]) words = [ \"padding\" ] + words print ( words ) ['padding', 'i', 'want', 'my', 'deep', 'learning', 'model', 'to', 'guess', 'this', 'sentence', 'perfectly', 'yes'] The extimated probability distribution of all words except 'yes' have large peak and flat elsewhere. The peak is located at the next word. For example, the peak of the probability distribution of the word after 'deep' is at 'learning'. However, the probability distribution of the word after 'yes' is rather flat. In [13]: choice_words = [ \"yes\" , 'deep' , 'my' ] for choice_word in choice_words : encoded = tokenizer . texts_to_sequences ([ choice_word ])[ 0 ] encoded = np . array ([ encoded ]) probs = model . predict ( encoded ) . flatten () y_pos = range ( len ( probs )) plt . figure ( figsize = ( 14 , 1 )) plt . bar ( y_pos , probs ) plt . xticks ( y_pos , words ) plt . title ( \"The probability distribution: word after '{}'\" . format ( choice_word )) plt . show () Train a NLP model with President Trump's Tweets Now let's move on to more complex application. In the previous example, we only have a single sentence to train the model. I will now use ~3,000 tweets from President Trump to train a deep learning model. In the previous post , I presented how to extract ~3,000 President trump's latest tweets using tweepy. I use this data to create a simple deep learning model that tweets like he does! In [14]: import pandas as pd data = pd . read_csv ( \"data/realDonaldTrump_tweets.csv\" ) data . head ( 5 ) Out[14]: id created_at favorite_count retweet_count text 0 952540700683497472 2018-01-14 13:59:35 63773 14402 ...big unnecessary regulation cuts made it all... 1 952538350333939713 2018-01-14 13:50:14 81577 18816 \"President Trump is not getting the credit he ... 2 952530515894169601 2018-01-14 13:19:06 112532 28970 I, as President, want people coming into our C... 3 952528011869478912 2018-01-14 13:09:09 91606 21864 DACA is probably dead because the Democrats do... 4 952526145064505345 2018-01-14 13:01:44 66552 13551 ...and they knew exactly what I said and meant... Let's look at randomly selected 10 tweets in my dataframe. It shows that the tweets contains lots of terms that only appears single times or the terms that are not interesting for predictions. So I will first clean the texts. In [15]: random_index = np . random . choice ( data . shape [ 0 ], 10 ) for index , k in enumerate ( data [ \"text\" ] . iloc [ random_index ]): print ( \"\" ) print ( \"irow={}\" . format ( random_index [ index ])) print ( k ) irow=1846 Getting ready to meet President al-Sisi of Egypt. On behalf of the United States, I look forward to a long and wonderful relationship. irow=2153 Christians in the Middle-East have been executed in large numbers. We cannot allow this horror to continue! irow=962 Thank you, our great honor! https://t.co/StrciEwuWs irow=394 Will be leaving the Philippines tomorrow after many days of constant mtgs & work in order to #MAGA! My promises are rapidly being fulfilled. irow=488 It is finally happening for our great clean coal miners! https://t.co/suAnjs6Ccz irow=611 Great news on the 2018 budget @SenateMajLdr McConnell - first step toward delivering MASSIVE tax cuts for the American people! #TaxReform https://t.co/aBzQR7KR0c irow=1276 My son Donald openly gave his e-mails to the media & authorities whereas Crooked Hillary Clinton deleted (& acid washed) her 33,000 e-mails! irow=246 MAKE AMERICA GREAT AGAIN! irow=457 Getting ready to land in Hawaii. Looking so much forward to meeting with our great Military/Veterans at Pearl Harbor! irow=2238 .@FoxNews \"Outgoing CIA Chief, John Brennan, blasts Pres-Elect Trump on Russia threat. Does not fully understand.\" Oh really, couldn't do... Tweet cleaning stranteges: remove quotes Ideally I want to treat each of '`' and '\"' as a single word. However, I found that Tokenizer is not always treating these as single words. For example, if there is a sentence \"I say so \". \"I is treated as a single word while the ending quote \" is treated as a single word. For simplicity, I will ignore quotes in this blog. remove URL. # and @. Most of these appear only once. Therefore including URL decreases the model performance on valdiation set substantially. In [16]: import re texts = [] for text in data [ \"text\" ] . values : text = text . replace ( '\"' , \"\" ) text = text . replace ( '\"' , \"\" ) text = text . replace ( '\"' , \"\" ) ## remove link text = re . sub ( r 'https?://.*' , '' , text , flags = re . MULTILINE ) ## remove hashtag text = re . sub ( r '#.*' , '' , text , flags = re . MULTILINE ) ## remove text = re . sub ( r '@.*' , '' , text , flags = re . MULTILINE ) texts . append ( text ) data [ \"text\" ] = texts I found that these cleaning is very important to create a meaningful model. Without the cleaning, the model's training accuracy did not increase more than 0.05. I tried to solve this problem by substantially increasing the complexity of the model but I was not very successful. It seems that removing infrequently appearing words is very useful approach. This makes sense because the removals of these infrequently appearing words reduce the size of Tokenizer.word_index by more than 20% times (1 - 5689/7300). Now, we create a mapping between a word and an index. Tokenizer nicely filters special characters. In [17]: tokenizer = Tokenizer () tokenizer . fit_on_texts ( data [ \"text\" ] . values ) vocab_size = len ( tokenizer . word_index ) + 1 index_word = { v : k for k , v in tokenizer . word_index . items ()} print ( \"The sentence has {} unique words\" . format ( len ( np . unique ( tokenizer . word_index . keys ())))) print ( \"> vocab_size={}\" . format ( vocab_size )) The sentence has 5688 unique words > vocab_size=5689 Using the Tokenizer's word index dictionary, represents each sentence just by the word indecies. Let's see how the sentence is represented with word indecies. In [18]: def print_text ( ks ): for k in ks : print ( \"{}({})\" . format ( index_word [ k ], k )), print ( \"\" ) for irow , line in enumerate ( data [ \"text\" ] . iloc [ random_index ]): encoded = tokenizer . texts_to_sequences ([ line ])[ 0 ] print ( \"irow={}\" . format ( random_index [ irow ])) print_text ( encoded ) print ( \"\" ) irow=1846 getting(182) ready(321) to(2) meet(561) president(54) al(1042) sisi(2117) of(4) egypt(1158) on(12) behalf(489) of(4) the(1) united(95) states(91) i(10) look(180) forward(236) to(2) a(6) long(159) and(3) wonderful(171) relationship(674) irow=2153 christians(5059) in(5) the(1) middle(348) east(595) have(22) been(87) executed(5060) in(5) large(741) numbers(282) we(15) cannot(469) allow(568) this(28) horror(1582) to(2) continue(456) irow=962 thank(30) you(20) our(14) great(11) honor(106) irow=394 will(9) be(13) leaving(359) the(1) philippines(1590) tomorrow(150) after(97) many(62) days(305) of(4) constant(3653) mtgs(2491) amp(18) work(177) in(5) order(184) to(2) irow=488 it(21) is(7) finally(355) happening(501) for(8) our(14) great(11) clean(1941) coal(1609) miners(2532) irow=611 great(11) news(36) on(12) the(1) 2018(676) budget(589) irow=1276 my(26) son(666) donald(319) openly(2811) gave(364) his(109) e(565) mails(935) to(2) the(1) media(69) amp(18) authorities(1828) whereas(4405) crooked(141) hillary(77) clinton(81) deleted(826) amp(18) acid(2393) washed(2394) her(272) 33(762) 000(157) e(565) mails(935) irow=246 make(66) america(42) great(11) again(63) irow=457 getting(182) ready(321) to(2) land(1447) in(5) hawaii(2517) looking(181) so(34) much(72) forward(236) to(2) meeting(151) with(16) our(14) great(11) military(117) veterans(432) at(23) pearl(1139) harbor(1330) irow=2238 Restructure the sentence data Currently each row is a sentence We will change it so that every row corresponds to a single word for prediction For example, if there are two sentences \"Make America Great Again\" and \"Thanks United States\" this will create 5 rows such that: \"- - Make America\", \"- Make America Great\", \"Make America Great Again\", \"- - Thanks United\", \"- Thanks United States\". Devide the sentences into taining and testing dataseat. I make sure that any sub sentences from the same single original sentence go to the same dataset. In [19]: N = data . shape [ 0 ] prop_train = 0.8 Ntrain = int ( N * prop_train ) Ntest = N - Ntrain sequences , index_train , index_test = [], [], [] count = 0 for irow , line in enumerate ( data [ \"text\" ]): encoded = tokenizer . texts_to_sequences ([ line ])[ 0 ] for i in range ( 1 , len ( encoded )): sequence = encoded [: i + 1 ] sequences . append ( sequence ) if irow < Ntrain : index_train . append ( count ) else : index_test . append ( count ) count += 1 print ( 'Total Sequences: %d ' % ( len ( sequences ))) Total Sequences: 50854 The sequence lengths differ in the data. We add \"0\" to fill make each sentence the same. See pad_sequence . Transform the target variables into one-hot encoded vectors. In [20]: from keras.preprocessing.sequence import pad_sequences max_length = max ([ len ( seq ) for seq in sequences ]) sequences = pad_sequences ( sequences , maxlen = max_length , padding = 'pre' ) print ( 'Max Sequence Length: %d ' % max_length ) # split into input and output elements sequences = np . array ( sequences ) X , y = sequences [:,: - 1 ], sequences [:, - 1 ] y = to_categorical ( y , num_classes = vocab_size ) X_train , y_train , X_test , y_test = X [ index_train ], y [ index_train ], X [ index_test ], y [ index_test ] Max Sequence Length: 55 Model training starts here! I made the model more complex than the previous example by increasing the dimention of the dense embedding vector, and increasing the number of hidden units in LSTM. The training accuracy keeps increasing but the validation accuracies do not increase as much. This is reasonable considering the small training data size; the model is overfitting. In [21]: model = define_model ( vocab_size , input_length = X . shape [ 1 ], dim_dense_embedding = 30 , hidden_unit_LSTM = 64 ) # compile network model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) # fit network hist = model . fit ( X_train , y_train , validation_data = ( X_test , y_test ), epochs = 50 , verbose = 2 , batch_size = 128 ) /home/fairy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)` _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 54) 0 _________________________________________________________________ embedding_2 (Embedding) (None, 54, 30) 170670 _________________________________________________________________ lstm_2 (LSTM) (None, 64) 24320 _________________________________________________________________ dense_2 (Dense) (None, 5689) 369785 ================================================================= Total params: 564,775 Trainable params: 564,775 Non-trainable params: 0 _________________________________________________________________ None Train on 42933 samples, validate on 7921 samples Epoch 1/50 42s - loss: 7.0381 - acc: 0.0457 - val_loss: 7.1778 - val_acc: 0.0376 Epoch 2/50 43s - loss: 6.7187 - acc: 0.0468 - val_loss: 7.1333 - val_acc: 0.0489 Epoch 3/50 42s - loss: 6.5754 - acc: 0.0561 - val_loss: 7.1526 - val_acc: 0.0518 Epoch 4/50 42s - loss: 6.4653 - acc: 0.0620 - val_loss: 7.1248 - val_acc: 0.0542 Epoch 5/50 42s - loss: 6.3519 - acc: 0.0712 - val_loss: 7.0637 - val_acc: 0.0636 Epoch 6/50 41s - loss: 6.2376 - acc: 0.0831 - val_loss: 7.0578 - val_acc: 0.0659 Epoch 7/50 42s - loss: 6.1370 - acc: 0.0893 - val_loss: 7.0022 - val_acc: 0.0819 Epoch 8/50 42s - loss: 6.0357 - acc: 0.0985 - val_loss: 6.9901 - val_acc: 0.0889 Epoch 9/50 42s - loss: 5.9305 - acc: 0.1113 - val_loss: 6.9461 - val_acc: 0.0971 Epoch 10/50 42s - loss: 5.8292 - acc: 0.1210 - val_loss: 6.9221 - val_acc: 0.1023 Epoch 11/50 41s - loss: 5.7368 - acc: 0.1259 - val_loss: 6.9087 - val_acc: 0.1050 Epoch 12/50 41s - loss: 5.6531 - acc: 0.1313 - val_loss: 6.8999 - val_acc: 0.1057 Epoch 13/50 41s - loss: 5.5744 - acc: 0.1361 - val_loss: 6.8956 - val_acc: 0.1066 Epoch 14/50 41s - loss: 5.4992 - acc: 0.1419 - val_loss: 6.8832 - val_acc: 0.1117 Epoch 15/50 40s - loss: 5.4262 - acc: 0.1450 - val_loss: 6.8832 - val_acc: 0.1146 Epoch 16/50 41s - loss: 5.3554 - acc: 0.1498 - val_loss: 6.8813 - val_acc: 0.1197 Epoch 17/50 42s - loss: 5.2873 - acc: 0.1535 - val_loss: 6.8838 - val_acc: 0.1179 Epoch 18/50 41s - loss: 5.2193 - acc: 0.1569 - val_loss: 6.8880 - val_acc: 0.1199 Epoch 19/50 41s - loss: 5.1511 - acc: 0.1615 - val_loss: 6.8848 - val_acc: 0.1225 Epoch 20/50 41s - loss: 5.0858 - acc: 0.1661 - val_loss: 6.8957 - val_acc: 0.1240 Epoch 21/50 41s - loss: 5.0204 - acc: 0.1709 - val_loss: 6.9047 - val_acc: 0.1278 Epoch 22/50 41s - loss: 4.9554 - acc: 0.1759 - val_loss: 6.9067 - val_acc: 0.1260 Epoch 23/50 40s - loss: 4.8918 - acc: 0.1811 - val_loss: 6.9166 - val_acc: 0.1283 Epoch 24/50 40s - loss: 4.8282 - acc: 0.1861 - val_loss: 6.9308 - val_acc: 0.1299 Epoch 25/50 40s - loss: 4.7659 - acc: 0.1903 - val_loss: 6.9423 - val_acc: 0.1321 Epoch 26/50 39s - loss: 4.7034 - acc: 0.1951 - val_loss: 6.9518 - val_acc: 0.1327 Epoch 27/50 38s - loss: 4.6431 - acc: 0.1998 - val_loss: 6.9729 - val_acc: 0.1312 Epoch 28/50 38s - loss: 4.5823 - acc: 0.2061 - val_loss: 6.9828 - val_acc: 0.1324 Epoch 29/50 38s - loss: 4.5236 - acc: 0.2094 - val_loss: 7.0053 - val_acc: 0.1329 Epoch 30/50 37s - loss: 4.4663 - acc: 0.2153 - val_loss: 7.0225 - val_acc: 0.1328 Epoch 31/50 38s - loss: 4.4097 - acc: 0.2198 - val_loss: 7.0357 - val_acc: 0.1357 Epoch 32/50 40s - loss: 4.3553 - acc: 0.2236 - val_loss: 7.0553 - val_acc: 0.1348 Epoch 33/50 39s - loss: 4.3003 - acc: 0.2296 - val_loss: 7.0785 - val_acc: 0.1360 Epoch 34/50 40s - loss: 4.2490 - acc: 0.2343 - val_loss: 7.0932 - val_acc: 0.1396 Epoch 35/50 40s - loss: 4.1977 - acc: 0.2382 - val_loss: 7.1158 - val_acc: 0.1384 Epoch 36/50 39s - loss: 4.1474 - acc: 0.2444 - val_loss: 7.1318 - val_acc: 0.1385 Epoch 37/50 40s - loss: 4.0999 - acc: 0.2499 - val_loss: 7.1532 - val_acc: 0.1381 Epoch 38/50 40s - loss: 4.0533 - acc: 0.2543 - val_loss: 7.1659 - val_acc: 0.1409 Epoch 39/50 40s - loss: 4.0083 - acc: 0.2604 - val_loss: 7.1852 - val_acc: 0.1389 Epoch 40/50 37s - loss: 3.9632 - acc: 0.2663 - val_loss: 7.2059 - val_acc: 0.1393 Epoch 41/50 41s - loss: 3.9196 - acc: 0.2719 - val_loss: 7.2269 - val_acc: 0.1386 Epoch 42/50 40s - loss: 3.8774 - acc: 0.2764 - val_loss: 7.2385 - val_acc: 0.1386 Epoch 43/50 41s - loss: 3.8377 - acc: 0.2825 - val_loss: 7.2597 - val_acc: 0.1408 Epoch 44/50 41s - loss: 3.7978 - acc: 0.2874 - val_loss: 7.2728 - val_acc: 0.1391 Epoch 45/50 41s - loss: 3.7604 - acc: 0.2923 - val_loss: 7.2909 - val_acc: 0.1372 Epoch 46/50 41s - loss: 3.7229 - acc: 0.2977 - val_loss: 7.3075 - val_acc: 0.1387 Epoch 47/50 41s - loss: 3.6861 - acc: 0.3024 - val_loss: 7.3216 - val_acc: 0.1403 Epoch 48/50 39s - loss: 3.6515 - acc: 0.3062 - val_loss: 7.3379 - val_acc: 0.1384 Epoch 49/50 40s - loss: 3.6179 - acc: 0.3112 - val_loss: 7.3497 - val_acc: 0.1401 Epoch 50/50 42s - loss: 3.5841 - acc: 0.3150 - val_loss: 7.3721 - val_acc: 0.1372 Plot validation accuracy and training accuracy In [34]: plt . figure ( figsize = ( 8 , 8 )) plt . plot ( hist . history [ \"val_acc\" ], label = \"val_acc\" ) plt . plot ( hist . history [ \"acc\" ], label = \"acc\" ) plt . legend () plt . show () Check the dimentions of available weights In [22]: count_layer = 0 for layer in model . layers : ws = layer . get_weights () c = 0 for w in ws : print ( \"layer{}_{} {:20} {}\" . format ( count_layer , c , layer . name , w . shape )) c += 1 count_layer += 1 layer1_0 embedding_2 (5689, 30) layer2_0 lstm_2 (30, 256) layer2_1 lstm_2 (64, 256) layer2_2 lstm_2 (256,) layer3_0 dense_2 (64, 5689) layer3_1 dense_2 (5689,) Reduce the dimention of the word vectors using PCA and visualize their distirubtions in 2d. In [23]: weight = model . layers [ 1 ] . get_weights ()[ 0 ] print ( weight . shape ) from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) y_embed_pca = pca . fit_transform ( weight ) print ( y_embed_pca . shape ) (5689, 30) (5689, 2) Observations Seemingly similar words are clustered in the same area: kim, saudi, radical, differently north, south estimates, statistically knives, dialogs independent, united thank, honor In [27]: fig , ax = plt . subplots ( figsize = ( 25 , 25 )) ax . scatter ( y_embed_pca [:, 0 ], y_embed_pca [:, 1 ], c = \"white\" ) for txt , irow in tokenizer . word_index . items (): try : ax . annotate ( txt , ( y_embed_pca [ irow , 0 ], y_embed_pca [ irow , 1 ])) except : pass ax . set_xlabel ( \"pca embedding 1\" ) ax . set_ylabel ( \"pca embedding 2\" ) plt . show () Let President Trump's AI talk about some topics I feed the first few words, and let's see what opnion of President Trump's AI is!! I randomly sample the next word according to the estimated probability distribution. This way, I can ensure that the outputs are different every time I feed the same initial words. Hummm the predicted tweets makes sense, kind of? Observations When \"Make America\" is provided as the first 2 words, the AI almost always predicts \"great again\" as the next words. When \"North\" is provided, the next word is almost always \"Korea\" followed often by some negative sentence. The sentence starting with \"Omaga is\" tends to have negative meaning. In [37]: def sample ( probs ): return ( np . random . choice ( range ( len ( probs )), p = probs )) def predict_sentence ( in_text , n_words , tokenizer , model , max_length ): words = [] for _ in range ( n_words ): # encode the text as integer enc = tokenizer . texts_to_sequences ([ in_text ])[ 0 ] # pre-pad sequences to a fixed length enc_padding = pad_sequences ([ enc ], maxlen = max_length - 1 , padding = 'pre' ) probs = model . predict ( enc_padding , verbose = 0 ) . flatten () index = sample ( probs ) word = index_word [ index ] in_text += ' ' + word return ( in_text ) print ( predict_sentence ( \"North\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"America\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"I'm\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"I won't\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"MAKE AMERICA\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"american jobs\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"Obama is\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"Universal healthcare\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"H1B visa\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"Women\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"fat\" , n_words , tokenizer , model , max_length )) North korea is so out with my the story on crooked hillary investigation as America is time blames came played many agencies before he voted out this is I'm so is dead they got china failing big crowd amp yates of historic I won't been would be a great nation the u s made what a grateful MAKE AMERICA great again to have forced to be a last legs in alabama uttered american jobs growth and now elections great healthcare with mine that is many fake news Obama is also way china media about representatives if it is a great days or Universal healthcare and now we will the u s demand manchester should do real james H1B visa person jobs in louvre powell to stand for what on the roosevelt room Women individuals our great healthcare amp tax cuts is approved hopefully terminate bonuses in fat other in china like killed presidential support that the rulers of she which Next Steps Try transfer learning Increase more tweets Prediction of the number of retweets/likes given tweets if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"The first deep learning model for NLP - Let AI tweet like President Trump -"},{"url":"extract-someones-tweet-using-tweepy.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } This blog post is to remind myself the simple useage of the tweepy. I will extract someone's past tweets using tweepy and create .csv file that can be used to train machine learning models. I created the scripts by referencing the following seminal blog posts: api.user_timeline not grabbing full tweet yanofsky/tweet_dumper.py tweepy Importing necessary python scripts. In [28]: ## credentials contain: # customer_key = \"XXX\" # customer_secret = \"XXX\" # access_token = \"XXX\" # access_token_secret = \"XXX\" from credentials import * import tweepy print ( tweepy . __version__ ) 3.5.0 Select the userID. In [2]: userID = \"realDonaldTrump\" Step 1: extract the latest 200 tweets using api.user_timeline In [31]: # Authorize our Twitter credentials auth = tweepy . OAuthHandler ( consumer_key , consumer_secret ) auth . set_access_token ( access_token , access_token_secret ) api = tweepy . API ( auth ) tweets = api . user_timeline ( screen_name = userID , # 200 is the maximum allowed count count = 200 , include_rts = False , # Necessary to keep full_text # otherwise only the first 140 words are extracted tweet_mode = 'extended' ) Show the extracted 3 latest tweets info.id is larger for the later tweets In [32]: for info in tweets [: 3 ]: print ( \"ID: {}\" . format ( info . id )) print ( info . created_at ) print ( info . full_text ) print ( \" \\n \" ) ID: 952540700683497472 2018-01-14 13:59:35 ...big unnecessary regulation cuts made it all possible\" (among many other things). \"President Trump reversed the policies of President Obama, and reversed our economic decline.\" Thank you Stuart Varney. @foxandfriends ID: 952538350333939713 2018-01-14 13:50:14 \"President Trump is not getting the credit he deserves for the economy. Tax Cut bonuses to more than 2,000,000 workers. Most explosive Stock Market rally that we've seen in modern times. 18,000 to 26,000 from Election, and grounded in profitability and growth. All Trump, not 0... ID: 952530515894169601 2018-01-14 13:19:06 I, as President, want people coming into our Country who are going to help us become strong and great again, people coming in through a system based on MERIT. No more Lotteries! #AMERICA FIRST Step 2: Extract as many past tweets as possible. I was able to extract In [33]: all_tweets = [] all_tweets . extend ( tweets ) oldest_id = tweets [ - 1 ] . id while True : tweets = api . user_timeline ( screen_name = userID , # 200 is the maximum allowed count count = 200 , include_rts = False , max_id = oldest_id - 1 , # Necessary to keep full_text # otherwise only the first 140 words are extracted tweet_mode = 'extended' ) if len ( tweets ) == 0 : break oldest_id = tweets [ - 1 ] . id all_tweets . extend ( tweets ) print ( 'N of tweets downloaded till now {}' . format ( len ( all_tweets ))) N of tweets downloaded till now 357 N of tweets downloaded till now 548 N of tweets downloaded till now 728 N of tweets downloaded till now 889 N of tweets downloaded till now 1043 N of tweets downloaded till now 1194 N of tweets downloaded till now 1381 N of tweets downloaded till now 1555 N of tweets downloaded till now 1732 N of tweets downloaded till now 1918 N of tweets downloaded till now 2107 N of tweets downloaded till now 2302 N of tweets downloaded till now 2489 N of tweets downloaded till now 2677 N of tweets downloaded till now 2863 N of tweets downloaded till now 2893 The total N of tweets: 2893 Step 3: Save the tweets into csv In [18]: tweet . full_text . encode ( \"utf-8\" ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) in () ----> 1 tweet . full_text . encode ( \"utf-8\" ) NameError : name 'tweet' is not defined In [34]: #transform the tweepy tweets into a 2D array that will populate the csv from pandas import DataFrame outtweets = [[ tweet . id_str , tweet . created_at , tweet . favorite_count , tweet . retweet_count , tweet . full_text . encode ( \"utf-8\" ) . decode ( \"utf-8\" )] for idx , tweet in enumerate ( all_tweets )] df = DataFrame ( outtweets , columns = [ \"id\" , \"created_at\" , \"favorite_count\" , \"retweet_count\" , \"text\" ]) df . to_csv ( ' %s _tweets.csv' % userID , index = False ) df . head ( 3 ) Out[34]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } id created_at favorite_count retweet_count text 0 952540700683497472 2018-01-14 13:59:35 64325 14528 ...big unnecessary regulation cuts made it all... 1 952538350333939713 2018-01-14 13:50:14 82267 18998 \"President Trump is not getting the credit he ... 2 952530515894169601 2018-01-14 13:19:06 113506 29228 I, as President, want people coming into our C... The data is saved at current working directory as: In [23]: ls *. csv realDonaldTrump_tweets.csv In [24]: cat *. csv | head - 4 ,id,created_at,favorite_count,retweet_count,text 0,952540700683497472,2018-01-14 13:59:35,63773,14402,\"...big unnecessary regulation cuts made it all possible\" (among many other things). \"President Trump reversed the policies of President Obama, and reversed our economic decline.\" Thank you Stuart Varney. @foxandfriends\" 1,952538350333939713,2018-01-14 13:50:14,81577,18816,\"\"President Trump is not getting the credit he deserves for the economy. Tax Cut bonuses to more than 2,000,000 workers. Most explosive Stock Market rally that we've seen in modern times. 18,000 to 26,000 from Election, and grounded in profitability and growth. All Trump, not 0...\" 2,952530515894169601,2018-01-14 13:19:06,112532,28970,\"I, as President, want people coming into our Country who are going to help us become strong and great again, people coming in through a system based on MERIT. No more Lotteries! #AMERICA FIRST\" cat: stdout: Broken pipe Preliminary analysis of President Trump's tweets Let's look at how the favorite counts and retweet counts change over time. There are some extraordinary popular tweets. It shows that we extracted the tweets since 2016-10. In [25]: import matplotlib.pyplot as plt ylabels = [ \"favorite_count\" , \"retweet_count\" ] fig = plt . figure ( figsize = ( 13 , 3 )) fig . subplots_adjust ( hspace = 0.01 , wspace = 0.01 ) n_row = len ( ylabels ) n_col = 1 for count , ylabel in enumerate ( ylabels ): ax = fig . add_subplot ( n_row , n_col , count + 1 ) ax . plot ( df [ \"created_at\" ], df [ ylabel ]) ax . set_ylabel ( ylabel ) plt . show () Let's look at the actual most popular tweets. Here, most popular tweets are defined as favorite_count > 400,000 and retweet_count > 200,000. The 1st peak: The tweets that President Trump made when he was selected to President. The 2nd peak: President Trump's response to CNN. This tweet includes a youtube video where President Trump body slams a man whose face is covered with the text \"CNN\". The 3rd peak: President Trump's response to Kim Jong-un. In [26]: df_sub = df . loc [( df [ \"favorite_count\" ] > 400000 ) & ( df [ \"retweet_count\" ] > 200000 ),:] for irow in range ( df_sub . shape [ 0 ]): df_row = df_sub . iloc [ irow ,:] print ( df_row [ \"created_at\" ]) print ( \"favorite_count={:6} retweet_count={:6}\" . format ( df_row [ \"favorite_count\" ], df_row [ \"retweet_count\" ])) print ( df_row [ \"text\" ]) print ( \" \\n \" ) 2017-11-12 00:48:01 favorite_count=617512 retweet_count=271595 Why would Kim Jong-un insult me by calling me \"old,\" when I would NEVER call him \"short and fat?\" Oh well, I try so hard to be his friend - and maybe someday that will happen! 2017-07-02 13:21:42 favorite_count=586558 retweet_count=361672 #FraudNewsCNN #FNN https://t.co/WYUnHjjUjg 2016-11-09 11:36:58 favorite_count=613262 retweet_count=211250 Such a beautiful and important evening! The forgotten man and woman will never be forgotten again. We will all come together as never before 2016-11-08 11:43:14 favorite_count=557330 retweet_count=331050 TODAY WE MAKE AMERICA GREAT AGAIN! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Extract someone's tweet using tweepy"},{"url":"Visualization of Filters with Keras.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } The goal of this blog post is to understand \"what my CNN model is looking at\". People call this visualization of the filters. But more precisely, what I will do here is to visualize the input images that maximizes (sum of the) activation map (or feature map) of the filters. I will visualize the filters of deep learning models for two different applications: Facial landmark detection Classification For the facial landmark detection, I will visualize the filters of the model that was trained and described in my previous post Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow . For the classification, I will use the VGG16. Once again, I will follow the two great blog posts: Shinya's Kerasで学ぶ転移学習 and Keras's official blog . In [3]: import os import matplotlib.pyplot as plt import numpy as np from pandas.io.parsers import read_csv from sklearn.utils import shuffle ## These files must be downloaded from Keras website and saved under data folder Use a single GPU In [4]: import tensorflow as tf from keras.backend.tensorflow_backend import set_session print ( tf . __version__ ) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"2\" #### 1 GPU1 #### 2 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) 1.2.1 Load the previously trained model Model 4 was the best among all considered single models in previous analysis. I will load Model 4. Create alias \"input_img\". This is the 96 pixcel x 96 pixcel image input for the deep learning model. \"layer_names\" is a list of the names of layers to visualize. \"layer_dict\" contains model layers model.summary() shows the deep learning architecture. In [5]: from keras.models import model_from_json def load_model ( name ): model = model_from_json ( open ( name + '_architecture.json' ) . read ()) model . load_weights ( name + '_weights.h5' ) return ( model ) model = load_model ( \"model4\" ) model . summary () input_img = model . layers [ 0 ] . input layer_names = [ \"conv2d_22\" , \"conv2d_23\" , \"conv2d_24\" ] layer_dict = dict ([( layer . name , layer ) for layer in model . layers ]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_22 (Conv2D) (None, 94, 94, 32) 320 _________________________________________________________________ activation_37 (Activation) (None, 94, 94, 32) 0 _________________________________________________________________ max_pooling2d_22 (MaxPooling (None, 47, 47, 32) 0 _________________________________________________________________ conv2d_23 (Conv2D) (None, 46, 46, 64) 8256 _________________________________________________________________ activation_38 (Activation) (None, 46, 46, 64) 0 _________________________________________________________________ max_pooling2d_23 (MaxPooling (None, 23, 23, 64) 0 _________________________________________________________________ conv2d_24 (Conv2D) (None, 22, 22, 128) 32896 _________________________________________________________________ activation_39 (Activation) (None, 22, 22, 128) 0 _________________________________________________________________ max_pooling2d_24 (MaxPooling (None, 11, 11, 128) 0 _________________________________________________________________ flatten_8 (Flatten) (None, 15488) 0 _________________________________________________________________ dense_31 (Dense) (None, 500) 7744500 _________________________________________________________________ activation_40 (Activation) (None, 500) 0 _________________________________________________________________ dense_32 (Dense) (None, 500) 250500 _________________________________________________________________ activation_41 (Activation) (None, 500) 0 _________________________________________________________________ dense_33 (Dense) (None, 30) 15030 ================================================================= Total params: 8,051,502 Trainable params: 8,051,502 Non-trainable params: 0 _________________________________________________________________ Functions to maximize the activation layer In [121]: import numpy as np from keras import backend as K class VisualizeImageMaximizeFmap ( object ): def __init__ ( self , pic_shape ): ''' pic_shape : a dimention of a single picture e.g., (96,96,1) ''' self . pic_shape = pic_shape def find_n_feature_map ( self , layer_name , max_nfmap ): ''' shows the number of feature maps for this layer only works if the layer is CNN ''' n_fmap = None for layer in model . layers : if layer . name == layer_name : weights = layer . get_weights () n_fmap = weights [ 1 ] . shape [ 0 ] if n_fmap is None : print ( layer_name + \" is not one of the layer names..\" ) n_fmap = 1 n_fmap = np . min ([ max_nfmap , n_fmap ]) return ( int ( n_fmap )) def find_image_maximizing_activation ( self , iterate , input_img_data , picorig = False , n_iter = 30 ): ''' The input image is scaled to range between 0 and 1 picorig : True if the picture image for input is original scale ranging between 0 and 225 False if the picture image for input is ranging [0,1] ''' input_img_data = np . random . random (( 1 , self . pic_shape [ 0 ], self . pic_shape [ 1 ], self . pic_shape [ 2 ])) if picorig : ## if the original picture is unscaled and ranging between (0,225), ## then the image values are centered around 123 with STD=25 input_img_data = input_img_data * 25 + 123 ## I played with this step value but the final image looks to be robust step = 500 # gradient ascent loss_values = [] for i in range ( n_iter ): loss_value , grads_value = iterate ([ input_img_data , 0 ]) input_img_data += grads_value * step loss_values . append ( loss_value ) return ( input_img_data , loss_values ) def create_iterate ( self , input_img , layer_output , filter_index ): ''' layer_output[:,:,:,0] is (Nsample, 94, 94) tensor contains: W0&#94;T [f(image)]_{i,j}], i = 1,..., 94, j = 1,..., 94 layer_output[:,:,:,1] contains: W1&#94;T [f(image)]_{i,j}], i = 1,..., 94, j = 1,..., 94 W0 and W1 are different kernel! ''' ## loss is a scalar if len ( layer_output . shape ) == 4 : ## conv layer loss = K . mean ( layer_output [:, :, :, filter_index ]) elif len ( layer_output . shape ) == 2 : ## fully connected layer loss = K . mean ( layer_output [:, filter_index ]) # calculate the gradient of the loss evaluated at the provided image grads = K . gradients ( loss , input_img )[ 0 ] # normalize the gradients grads /= ( K . sqrt ( K . mean ( K . square ( grads ))) + 1e-5 ) # iterate is a function taking (input_img, scalar) and output [loss_value, gradient_value] iterate = K . function ([ input_img , K . learning_phase ()], [ loss , grads ]) return ( iterate ) def deprocess_image ( self , x ): # standardize to have a mean 0 and std 0.1 x -= x . mean () x /= ( x . std () + 1e-5 ) x *= 0.1 # Shift x to have a mean 0.5 and std 0.1 # This means 95% of the x should be in between 0 and 1 # if x is normal x += 0.5 x = np . clip ( x , 0 , 1 ) # resclar the values to range between 0 and 255 x *= 255 x = np . clip ( x , 0 , 255 ) . astype ( 'uint8' ) return x def find_images ( self , input_img , layer_names , layer_dict , max_nfmap , picorig = False , n_iter = 30 ): ''' Input : input_img : the alias of the input layer from the deep learning model layer_names : list containing the name of the layers whose feature maps to be used layer_dict : symbolic outputs of each \"key\" layer (we gave them unique names). max_nfmap : the maximum number of feature map to be used for each layer. pic_shape : For example pic_shape = (96,96,1) Output : dictionary key = layer name value = a list containing the tuple of (images, list of loss_values) that maximize each feature map ''' argimage = {} ## Look for the image for each feature map of each layer one by one for layer_name in layer_names : ## the layer to visualize n_fmap = self . find_n_feature_map ( layer_name , max_nfmap ) layer_output = layer_dict [ layer_name ] . output result = self . find_images_for_layer ( input_img , layer_output , range ( n_fma ), picorig = picorig , n_iter = n_iter ) argimage [ layer_name ] = result return ( argimage ) def find_images_for_layer ( self , input_img , layer_output , indecies , picorig = False , n_iter = 30 ): ''' indecies : list containing index of --> filtermaps of CNN or --> nodes of fully-connected layer Output a list containing the tuple of (images, list of loss_values) that maximize each feature map ''' result_temp = [] for filter_index in indecies : # filtermap to visualize iterate = self . create_iterate ( input_img , layer_output , filter_index ) input_img_data , loss_values = self . find_image_maximizing_activation ( iterate , input_img , picorig = picorig , n_iter = n_iter ) result_temp . append (( input_img_data , loss_values )) return ( result_temp ) def plot_images_wrapper ( self , argimage , n_row = 8 , scale = 1 ): ''' scale : scale up or down the plot size ''' pic_shape = self . pic_shape if pic_shape [ 2 ] == 1 : pic_shape = self . pic_shape [: 2 ] layer_names = np . sort ( argimage . keys ()) for layer_name in layer_names : n_fmap = len ( argimage [ layer_name ]) n_col = np . ceil ( n_fmap / float ( n_row )) fig = plt . figure ( figsize = ( n_col * scale , n_row * scale )) fig . subplots_adjust ( hspace = 0.001 , wspace = 0.001 ) plt . title ( layer_name + \" n_featuremap=\" + str ( n_fmap )) count = 1 for value in argimage [ layer_name ]: input_img_data = value [ 0 ][ 0 ] img = self . deprocess_image ( input_img_data ) ax = fig . add_subplot ( n_row , n_col , count , xticks = [], yticks = []) ax . imshow ( img . reshape ( * pic_shape ), cmap = \"gray\" ) count += 1 plt . show () For each feature map from each CNN layer, we look for the image that maximizes the sum of the feature maps. Look for the 96 pixcel x 96 pixcel image that maximize: $ \\textrm{argmax}_{image} \\sum_{\\textrm{kernel}} \\boldsymbol{W}&#94;T \\left[ f(image) \\right]_{\\textrm{kernel}} $ $\\boldsymbol{W}$ is a \"filter\", vector of length kernel_size[0] * kernel_size[1]. For example, for conv1, kernel_size=(4,4). $\\sum_{\\textrm{kernel}}$ The sum goes over 94 windows, running over the picture for conv1. max_nfmap determines the number of feature maps from each layer to use for analysis. Observations First layer distinguish colours. Some of the first images seem to have duplicated infomation (same colour). So maybe we can reduce the number of featuremap while keeping the same model performance. The 2nd and the 3rd layer get excited with more complex images. In [8]: max_nfmap = np . Inf ## print ALL the images visualizer = VisualizeImageMaximizeFmap ( pic_shape = ( 96 , 96 , 1 )) print ( \"find images that maximize feature maps\" ) argimage = visualizer . find_images ( input_img , layer_names , layer_dict , max_nfmap ) print ( \"plot them...\" ) visualizer . plot_images_wrapper ( argimage , n_row = 8 , scale = 1 ) find images that maximize feature maps plot them... Did the gradient ascent converge? In Kaggle's official blog , the number of iterations for the gradient ascent is set to as low as 20. Do I really find the best image that maximizes the sum of the feature map? To answer to this question, I plotted the sum of the feature map over iterations. Not suprisingly, the gradient ascent did not converge at all!! I run the codes several times and look at different best images found by the gradient ascent (i.e., run the codes in the previous cell several times). It seems that the best image obtained and plotted above are virtually the same even when the algorithm did not coverge. I guess there are various numerical solutions to this optimization but they are vertually the same image after rescaling. In [9]: const = 1 n_row = 8 for layer_name in layer_names : n_fmap = len ( argimage [ layer_name ]) n_col = np . ceil ( n_fmap / float ( n_row )) fig = plt . figure ( figsize = ( n_col * const , n_row * const )) fig . subplots_adjust ( hspace = 0.001 , wspace = 0.001 ) plt . title ( layer_name + \" n_featuremap=\" + str ( n_fmap )) for count , value in enumerate ( argimage [ layer_name ]): objective = value [ 1 ] ax = fig . add_subplot ( n_row , n_col , count + 1 , xticks = [], yticks = []) ax . plot ( objective ) plt . show () Visualizing VGG16 Let's learn to visualize layers of deep learning model for classification problem. Here I use VGG model following the discussion of Keras's official blog . VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. It was used to win the ILSVR (ImageNet) competition in 2014. Due to the proxy problem I cannot download and bulid model using keras.applications.VGG16 so here I take manual approach. Step 1: Downloading data from Github . This is a massive .h5 file (57MB). Step 2: The source code of keras.applications.VGG16 is available. It seems that we can manually set the weights to be the one locally available. In [10]: ls \"vgg16\" * vgg16_weights_tf_dim_ordering_tf_kernels.h5 * vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 * Some observations: Input shape is (None, None, None, 3). This (probablly) means that the sample size and the frame sizes can be specified later. But the channel (RGB) is specified at the 3rd dimention and it must be 3? Keras's official blog sets the 1st dimention for the RGB specification i.e., (None,3,None,None). According to the Keras's documentation about VGG16 , you are allowed to chose the channel to come to the last dimention or the first dimention. But where can I set the dimention? I found my answer in K.image_data_format(). It shows that the default setting is 'channels_last'. Therefore, it makes sense that the VGG16 has 3 as the last dimention. In [11]: import keras.backend as K K . image_data_format () Out[11]: 'channels_last' In [12]: from keras.applications import VGG16 model = VGG16 ( include_top = False , weights = None ) ## load the locally saved weights model . load_weights ( \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\" ) ## show the deep learning model model . summary () input_img = model . layers [ 0 ] . input # get the symbolic outputs of each \"key\" layer (we gave them unique names). layer_dict = dict ([( layer . name , layer ) for layer in model . layers ]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, None, None, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, None, None, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, None, None, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, None, None, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, None, None, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, None, None, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, None, None, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, None, None, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, None, None, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, None, None, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, None, None, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, None, None, 512) 0 ================================================================= Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________ Here the max_nfmap is set to a low number because the number of feature maps is as large as 512 for block5, and this is too many to plot. Notice that although the frame size is set to (96, 96, 3), the frame size can be different: as long as the width and height are larger than 48. Great flexibility of deep learning model :D. This flexilibity is also discussed in the Kaggle's blog . \"Note that we only go up to the last convolutional layer --we don't include fully-connected layers. The reason is that adding the fully connected layers forces you to use a fixed input size for the model (224x224, the original ImageNet format). By only keeping the convolutional modules, our model can be adapted to arbitrary input sizes.\" I observe similar results as Keras's official blog : The first layer encode direction (looks diagonal lines) and color. Later layers mix the direction and colors together. Later layers look more complex and very staticy. In [13]: layer_names = [ \"block1_conv1\" , \"block1_conv2\" , \"block2_conv2\" , \"block3_conv3\" , \"block4_conv3\" , \"block5_conv3\" ] ## (196, 196, 3) , (96,96,3) visualizer = VisualizeImageMaximizeFmap ( pic_shape = ( 48 , 48 , 3 )) max_nfmap = 3 argimage = visualizer . find_images ( input_img , layer_names , layer_dict , max_nfmap ) visualizer . plot_images_wrapper ( argimage , n_row = 1 , scale = 3 ) Use the full VGG16 models Finding an input that maximizes a specific class In the facial landmark detection application, I did not visualize the final output layer. This is because knowing which image can maximize the x (or y) coordinate of the eye center does not give much insite about the model. On the other hand, visualizing the final output layer gives interesting insights in the classification application, because knowing the which image can maximize the probability of being in one of the specific class is interesting. Following Keras's official blog , we will find the images that maximize specific classes. Once again, I download weights from here . This weight is 10 times larger! (528 MB). This makes sense because the weights with top fully connected layer contains 138,357,544 parameters while the weights without the top layers contains 10 times less parameters (14,714,688 parameters). In [20]: ls \"vgg16_weights_tf_dim\" * vgg16_weights_tf_dim_ordering_tf_kernels.h5 * vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 * Notice that the input dimention is set to (None, 224, 224, 3), indicating that our input image needs to have this size In [63]: model = VGG16 ( include_top = True , weights = None ) model . summary () model . load_weights ( \"vgg16_weights_tf_dim_ordering_tf_kernels.h5\" ) input_img = model . layers [ 0 ] . input layer_dict = dict ([( layer . name , layer ) for layer in model . layers ]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) (None, 224, 224, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 _________________________________________________________________ predictions (Dense) (None, 1000) 4097000 ================================================================= Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 _________________________________________________________________ It would be interesting to learn what kind of image can maximize the probability of picture having cats or dogs. The visualization becomes more interesting if we know the actual label/meaning of each of the 1000 classes. So before going into the details of the visualization, I extract the labels of each 1000 classes. These classes are saved as a json object at here . This json object is used internally from the method keras.applications.vgg16.decode_predictions() . In [16]: ls imagenet_class_index . json imagenet_class_index.json * In [66]: import json CLASS_INDEX = json . load ( open ( \"imagenet_class_index.json\" )) classlabel = [] for i in range ( 1000 ): classlabel . append ( CLASS_INDEX [ str ( i )][ 1 ]) classlabel = np . array ( classlabel ) print ( len ( classlabel )) 1000 List all 1000 classes in order It is interesting to know that many of the class labels are related to dogs. As Keras's public blog says the 65th class is sea snake class and the 18th magpie. In [67]: for i , lab in enumerate ( classlabel ): print ( \"{:4.0f}: {:30}\" . format ( i , str ( lab ))), if i % 3 == 2 : print ( \"\" ) 0: tench 1: goldfish 2: great_white_shark 3: tiger_shark 4: hammerhead 5: electric_ray 6: stingray 7: cock 8: hen 9: ostrich 10: brambling 11: goldfinch 12: house_finch 13: junco 14: indigo_bunting 15: robin 16: bulbul 17: jay 18: magpie 19: chickadee 20: water_ouzel 21: kite 22: bald_eagle 23: vulture 24: great_grey_owl 25: European_fire_salamander 26: common_newt 27: eft 28: spotted_salamander 29: axolotl 30: bullfrog 31: tree_frog 32: tailed_frog 33: loggerhead 34: leatherback_turtle 35: mud_turtle 36: terrapin 37: box_turtle 38: banded_gecko 39: common_iguana 40: American_chameleon 41: whiptail 42: agama 43: frilled_lizard 44: alligator_lizard 45: Gila_monster 46: green_lizard 47: African_chameleon 48: Komodo_dragon 49: African_crocodile 50: American_alligator 51: triceratops 52: thunder_snake 53: ringneck_snake 54: hognose_snake 55: green_snake 56: king_snake 57: garter_snake 58: water_snake 59: vine_snake 60: night_snake 61: boa_constrictor 62: rock_python 63: Indian_cobra 64: green_mamba 65: sea_snake 66: horned_viper 67: diamondback 68: sidewinder 69: trilobite 70: harvestman 71: scorpion 72: black_and_gold_garden_spider 73: barn_spider 74: garden_spider 75: black_widow 76: tarantula 77: wolf_spider 78: tick 79: centipede 80: black_grouse 81: ptarmigan 82: ruffed_grouse 83: prairie_chicken 84: peacock 85: quail 86: partridge 87: African_grey 88: macaw 89: sulphur-crested_cockatoo 90: lorikeet 91: coucal 92: bee_eater 93: hornbill 94: hummingbird 95: jacamar 96: toucan 97: drake 98: red-breasted_merganser 99: goose 100: black_swan 101: tusker 102: echidna 103: platypus 104: wallaby 105: koala 106: wombat 107: jellyfish 108: sea_anemone 109: brain_coral 110: flatworm 111: nematode 112: conch 113: snail 114: slug 115: sea_slug 116: chiton 117: chambered_nautilus 118: Dungeness_crab 119: rock_crab 120: fiddler_crab 121: king_crab 122: American_lobster 123: spiny_lobster 124: crayfish 125: hermit_crab 126: isopod 127: white_stork 128: black_stork 129: spoonbill 130: flamingo 131: little_blue_heron 132: American_egret 133: bittern 134: crane 135: limpkin 136: European_gallinule 137: American_coot 138: bustard 139: ruddy_turnstone 140: red-backed_sandpiper 141: redshank 142: dowitcher 143: oystercatcher 144: pelican 145: king_penguin 146: albatross 147: grey_whale 148: killer_whale 149: dugong 150: sea_lion 151: Chihuahua 152: Japanese_spaniel 153: Maltese_dog 154: Pekinese 155: Shih-Tzu 156: Blenheim_spaniel 157: papillon 158: toy_terrier 159: Rhodesian_ridgeback 160: Afghan_hound 161: basset 162: beagle 163: bloodhound 164: bluetick 165: black-and-tan_coonhound 166: Walker_hound 167: English_foxhound 168: redbone 169: borzoi 170: Irish_wolfhound 171: Italian_greyhound 172: whippet 173: Ibizan_hound 174: Norwegian_elkhound 175: otterhound 176: Saluki 177: Scottish_deerhound 178: Weimaraner 179: Staffordshire_bullterrier 180: American_Staffordshire_terrier 181: Bedlington_terrier 182: Border_terrier 183: Kerry_blue_terrier 184: Irish_terrier 185: Norfolk_terrier 186: Norwich_terrier 187: Yorkshire_terrier 188: wire-haired_fox_terrier 189: Lakeland_terrier 190: Sealyham_terrier 191: Airedale 192: cairn 193: Australian_terrier 194: Dandie_Dinmont 195: Boston_bull 196: miniature_schnauzer 197: giant_schnauzer 198: standard_schnauzer 199: Scotch_terrier 200: Tibetan_terrier 201: silky_terrier 202: soft-coated_wheaten_terrier 203: West_Highland_white_terrier 204: Lhasa 205: flat-coated_retriever 206: curly-coated_retriever 207: golden_retriever 208: Labrador_retriever 209: Chesapeake_Bay_retriever 210: German_short-haired_pointer 211: vizsla 212: English_setter 213: Irish_setter 214: Gordon_setter 215: Brittany_spaniel 216: clumber 217: English_springer 218: Welsh_springer_spaniel 219: cocker_spaniel 220: Sussex_spaniel 221: Irish_water_spaniel 222: kuvasz 223: schipperke 224: groenendael 225: malinois 226: briard 227: kelpie 228: komondor 229: Old_English_sheepdog 230: Shetland_sheepdog 231: collie 232: Border_collie 233: Bouvier_des_Flandres 234: Rottweiler 235: German_shepherd 236: Doberman 237: miniature_pinscher 238: Greater_Swiss_Mountain_dog 239: Bernese_mountain_dog 240: Appenzeller 241: EntleBucher 242: boxer 243: bull_mastiff 244: Tibetan_mastiff 245: French_bulldog 246: Great_Dane 247: Saint_Bernard 248: Eskimo_dog 249: malamute 250: Siberian_husky 251: dalmatian 252: affenpinscher 253: basenji 254: pug 255: Leonberg 256: Newfoundland 257: Great_Pyrenees 258: Samoyed 259: Pomeranian 260: chow 261: keeshond 262: Brabancon_griffon 263: Pembroke 264: Cardigan 265: toy_poodle 266: miniature_poodle 267: standard_poodle 268: Mexican_hairless 269: timber_wolf 270: white_wolf 271: red_wolf 272: coyote 273: dingo 274: dhole 275: African_hunting_dog 276: hyena 277: red_fox 278: kit_fox 279: Arctic_fox 280: grey_fox 281: tabby 282: tiger_cat 283: Persian_cat 284: Siamese_cat 285: Egyptian_cat 286: cougar 287: lynx 288: leopard 289: snow_leopard 290: jaguar 291: lion 292: tiger 293: cheetah 294: brown_bear 295: American_black_bear 296: ice_bear 297: sloth_bear 298: mongoose 299: meerkat 300: tiger_beetle 301: ladybug 302: ground_beetle 303: long-horned_beetle 304: leaf_beetle 305: dung_beetle 306: rhinoceros_beetle 307: weevil 308: fly 309: bee 310: ant 311: grasshopper 312: cricket 313: walking_stick 314: cockroach 315: mantis 316: cicada 317: leafhopper 318: lacewing 319: dragonfly 320: damselfly 321: admiral 322: ringlet 323: monarch 324: cabbage_butterfly 325: sulphur_butterfly 326: lycaenid 327: starfish 328: sea_urchin 329: sea_cucumber 330: wood_rabbit 331: hare 332: Angora 333: hamster 334: porcupine 335: fox_squirrel 336: marmot 337: beaver 338: guinea_pig 339: sorrel 340: zebra 341: hog 342: wild_boar 343: warthog 344: hippopotamus 345: ox 346: water_buffalo 347: bison 348: ram 349: bighorn 350: ibex 351: hartebeest 352: impala 353: gazelle 354: Arabian_camel 355: llama 356: weasel 357: mink 358: polecat 359: black-footed_ferret 360: otter 361: skunk 362: badger 363: armadillo 364: three-toed_sloth 365: orangutan 366: gorilla 367: chimpanzee 368: gibbon 369: siamang 370: guenon 371: patas 372: baboon 373: macaque 374: langur 375: colobus 376: proboscis_monkey 377: marmoset 378: capuchin 379: howler_monkey 380: titi 381: spider_monkey 382: squirrel_monkey 383: Madagascar_cat 384: indri 385: Indian_elephant 386: African_elephant 387: lesser_panda 388: giant_panda 389: barracouta 390: eel 391: coho 392: rock_beauty 393: anemone_fish 394: sturgeon 395: gar 396: lionfish 397: puffer 398: abacus 399: abaya 400: academic_gown 401: accordion 402: acoustic_guitar 403: aircraft_carrier 404: airliner 405: airship 406: altar 407: ambulance 408: amphibian 409: analog_clock 410: apiary 411: apron 412: ashcan 413: assault_rifle 414: backpack 415: bakery 416: balance_beam 417: balloon 418: ballpoint 419: Band_Aid 420: banjo 421: bannister 422: barbell 423: barber_chair 424: barbershop 425: barn 426: barometer 427: barrel 428: barrow 429: baseball 430: basketball 431: bassinet 432: bassoon 433: bathing_cap 434: bath_towel 435: bathtub 436: beach_wagon 437: beacon 438: beaker 439: bearskin 440: beer_bottle 441: beer_glass 442: bell_cote 443: bib 444: bicycle-built-for-two 445: bikini 446: binder 447: binoculars 448: birdhouse 449: boathouse 450: bobsled 451: bolo_tie 452: bonnet 453: bookcase 454: bookshop 455: bottlecap 456: bow 457: bow_tie 458: brass 459: brassiere 460: breakwater 461: breastplate 462: broom 463: bucket 464: buckle 465: bulletproof_vest 466: bullet_train 467: butcher_shop 468: cab 469: caldron 470: candle 471: cannon 472: canoe 473: can_opener 474: cardigan 475: car_mirror 476: carousel 477: carpenter's_kit 478: carton 479: car_wheel 480: cash_machine 481: cassette 482: cassette_player 483: castle 484: catamaran 485: CD_player 486: cello 487: cellular_telephone 488: chain 489: chainlink_fence 490: chain_mail 491: chain_saw 492: chest 493: chiffonier 494: chime 495: china_cabinet 496: Christmas_stocking 497: church 498: cinema 499: cleaver 500: cliff_dwelling 501: cloak 502: clog 503: cocktail_shaker 504: coffee_mug 505: coffeepot 506: coil 507: combination_lock 508: computer_keyboard 509: confectionery 510: container_ship 511: convertible 512: corkscrew 513: cornet 514: cowboy_boot 515: cowboy_hat 516: cradle 517: crane 518: crash_helmet 519: crate 520: crib 521: Crock_Pot 522: croquet_ball 523: crutch 524: cuirass 525: dam 526: desk 527: desktop_computer 528: dial_telephone 529: diaper 530: digital_clock 531: digital_watch 532: dining_table 533: dishrag 534: dishwasher 535: disk_brake 536: dock 537: dogsled 538: dome 539: doormat 540: drilling_platform 541: drum 542: drumstick 543: dumbbell 544: Dutch_oven 545: electric_fan 546: electric_guitar 547: electric_locomotive 548: entertainment_center 549: envelope 550: espresso_maker 551: face_powder 552: feather_boa 553: file 554: fireboat 555: fire_engine 556: fire_screen 557: flagpole 558: flute 559: folding_chair 560: football_helmet 561: forklift 562: fountain 563: fountain_pen 564: four-poster 565: freight_car 566: French_horn 567: frying_pan 568: fur_coat 569: garbage_truck 570: gasmask 571: gas_pump 572: goblet 573: go-kart 574: golf_ball 575: golfcart 576: gondola 577: gong 578: gown 579: grand_piano 580: greenhouse 581: grille 582: grocery_store 583: guillotine 584: hair_slide 585: hair_spray 586: half_track 587: hammer 588: hamper 589: hand_blower 590: hand-held_computer 591: handkerchief 592: hard_disc 593: harmonica 594: harp 595: harvester 596: hatchet 597: holster 598: home_theater 599: honeycomb 600: hook 601: hoopskirt 602: horizontal_bar 603: horse_cart 604: hourglass 605: iPod 606: iron 607: jack-o'-lantern 608: jean 609: jeep 610: jersey 611: jigsaw_puzzle 612: jinrikisha 613: joystick 614: kimono 615: knee_pad 616: knot 617: lab_coat 618: ladle 619: lampshade 620: laptop 621: lawn_mower 622: lens_cap 623: letter_opener 624: library 625: lifeboat 626: lighter 627: limousine 628: liner 629: lipstick 630: Loafer 631: lotion 632: loudspeaker 633: loupe 634: lumbermill 635: magnetic_compass 636: mailbag 637: mailbox 638: maillot 639: maillot 640: manhole_cover 641: maraca 642: marimba 643: mask 644: matchstick 645: maypole 646: maze 647: measuring_cup 648: medicine_chest 649: megalith 650: microphone 651: microwave 652: military_uniform 653: milk_can 654: minibus 655: miniskirt 656: minivan 657: missile 658: mitten 659: mixing_bowl 660: mobile_home 661: Model_T 662: modem 663: monastery 664: monitor 665: moped 666: mortar 667: mortarboard 668: mosque 669: mosquito_net 670: motor_scooter 671: mountain_bike 672: mountain_tent 673: mouse 674: mousetrap 675: moving_van 676: muzzle 677: nail 678: neck_brace 679: necklace 680: nipple 681: notebook 682: obelisk 683: oboe 684: ocarina 685: odometer 686: oil_filter 687: organ 688: oscilloscope 689: overskirt 690: oxcart 691: oxygen_mask 692: packet 693: paddle 694: paddlewheel 695: padlock 696: paintbrush 697: pajama 698: palace 699: panpipe 700: paper_towel 701: parachute 702: parallel_bars 703: park_bench 704: parking_meter 705: passenger_car 706: patio 707: pay-phone 708: pedestal 709: pencil_box 710: pencil_sharpener 711: perfume 712: Petri_dish 713: photocopier 714: pick 715: pickelhaube 716: picket_fence 717: pickup 718: pier 719: piggy_bank 720: pill_bottle 721: pillow 722: ping-pong_ball 723: pinwheel 724: pirate 725: pitcher 726: plane 727: planetarium 728: plastic_bag 729: plate_rack 730: plow 731: plunger 732: Polaroid_camera 733: pole 734: police_van 735: poncho 736: pool_table 737: pop_bottle 738: pot 739: potter's_wheel 740: power_drill 741: prayer_rug 742: printer 743: prison 744: projectile 745: projector 746: puck 747: punching_bag 748: purse 749: quill 750: quilt 751: racer 752: racket 753: radiator 754: radio 755: radio_telescope 756: rain_barrel 757: recreational_vehicle 758: reel 759: reflex_camera 760: refrigerator 761: remote_control 762: restaurant 763: revolver 764: rifle 765: rocking_chair 766: rotisserie 767: rubber_eraser 768: rugby_ball 769: rule 770: running_shoe 771: safe 772: safety_pin 773: saltshaker 774: sandal 775: sarong 776: sax 777: scabbard 778: scale 779: school_bus 780: schooner 781: scoreboard 782: screen 783: screw 784: screwdriver 785: seat_belt 786: sewing_machine 787: shield 788: shoe_shop 789: shoji 790: shopping_basket 791: shopping_cart 792: shovel 793: shower_cap 794: shower_curtain 795: ski 796: ski_mask 797: sleeping_bag 798: slide_rule 799: sliding_door 800: slot 801: snorkel 802: snowmobile 803: snowplow 804: soap_dispenser 805: soccer_ball 806: sock 807: solar_dish 808: sombrero 809: soup_bowl 810: space_bar 811: space_heater 812: space_shuttle 813: spatula 814: speedboat 815: spider_web 816: spindle 817: sports_car 818: spotlight 819: stage 820: steam_locomotive 821: steel_arch_bridge 822: steel_drum 823: stethoscope 824: stole 825: stone_wall 826: stopwatch 827: stove 828: strainer 829: streetcar 830: stretcher 831: studio_couch 832: stupa 833: submarine 834: suit 835: sundial 836: sunglass 837: sunglasses 838: sunscreen 839: suspension_bridge 840: swab 841: sweatshirt 842: swimming_trunks 843: swing 844: switch 845: syringe 846: table_lamp 847: tank 848: tape_player 849: teapot 850: teddy 851: television 852: tennis_ball 853: thatch 854: theater_curtain 855: thimble 856: thresher 857: throne 858: tile_roof 859: toaster 860: tobacco_shop 861: toilet_seat 862: torch 863: totem_pole 864: tow_truck 865: toyshop 866: tractor 867: trailer_truck 868: tray 869: trench_coat 870: tricycle 871: trimaran 872: tripod 873: triumphal_arch 874: trolleybus 875: trombone 876: tub 877: turnstile 878: typewriter_keyboard 879: umbrella 880: unicycle 881: upright 882: vacuum 883: vase 884: vault 885: velvet 886: vending_machine 887: vestment 888: viaduct 889: violin 890: volleyball 891: waffle_iron 892: wall_clock 893: wallet 894: wardrobe 895: warplane 896: washbasin 897: washer 898: water_bottle 899: water_jug 900: water_tower 901: whiskey_jug 902: whistle 903: wig 904: window_screen 905: window_shade 906: Windsor_tie 907: wine_bottle 908: wing 909: wok 910: wooden_spoon 911: wool 912: worm_fence 913: wreck 914: yawl 915: yurt 916: web_site 917: comic_book 918: crossword_puzzle 919: street_sign 920: traffic_light 921: book_jacket 922: menu 923: plate 924: guacamole 925: consomme 926: hot_pot 927: trifle 928: ice_cream 929: ice_lolly 930: French_loaf 931: bagel 932: pretzel 933: cheeseburger 934: hotdog 935: mashed_potato 936: head_cabbage 937: broccoli 938: cauliflower 939: zucchini 940: spaghetti_squash 941: acorn_squash 942: butternut_squash 943: cucumber 944: artichoke 945: bell_pepper 946: cardoon 947: mushroom 948: Granny_Smith 949: strawberry 950: orange 951: lemon 952: fig 953: pineapple 954: banana 955: jackfruit 956: custard_apple 957: pomegranate 958: hay 959: carbonara 960: chocolate_sauce 961: dough 962: meat_loaf 963: pizza 964: potpie 965: burrito 966: red_wine 967: espresso 968: cup 969: eggnog 970: alp 971: bubble 972: cliff 973: coral_reef 974: geyser 975: lakeside 976: promontory 977: sandbar 978: seashore 979: valley 980: volcano 981: ballplayer 982: groom 983: scuba_diver 984: rapeseed 985: daisy 986: yellow_lady's_slipper 987: corn 988: acorn 989: hip 990: buckeye 991: coral_fungus 992: agaric 993: gyromitra 994: stinkhorn 995: earthstar 996: hen-of-the-woods 997: bolete 998: ear 999: toilet_tissue Is VGG16 doing its job? Before visualizing the fully-connected layer's filters, I check if VGG16 is doing its highly-reputated job. I took a picture of a dog from Google image, and see if the VGG16 can predict the picture correctly. VGG16 correctly guesses that the picture has a dog! Amazingly, it also guesses the type of the dog correctly! In [111]: from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array image = load_img ( './dog.jpg' , target_size = ( 224 , 224 , 3 )) plt . imshow ( image ) plt . show () image = img_to_array ( image ) #output Numpy-array y_pred = model . predict ( image . reshape ( 1 , image . shape [ 0 ], image . shape [ 1 ], image . shape [ 2 ])) . flatten () ## top 5 selected classes top = 5 chosen_classes = classlabel [ np . argsort ( y_pred )[:: - 1 ][: top ] ] print ( \"The top {} labels\" . format ( top )) print ( \"-----------------------------------------------------\" ) for myclass in chosen_classes : myprob = y_pred [ classlabel == myclass ][ 0 ] print ( \"{:30} prob={:4.3}\" . format ( myclass , myprob )) The top 5 labels ----------------------------------------------------- Maltese_dog prob=0.474 Dandie_Dinmont prob=0.0834 soft-coated_wheaten_terrier prob=0.0734 toy_poodle prob=0.0686 Lhasa prob=0.0521 Visualization of the filters of the final output layers Finally, I visualize the filters of output layers. Here I choose the following 4 output classes for visualization. sea snake (just like Keras's official blog ) magpie (just like Keras's official blog ) bee eater pillow The following codes look a bit complicated because of the double loop. The first loop goes over the four classes. The next while loop iterates until the gradient-ascent algorithm \"converges\". In comparisons to my Model 4, VGG16 is more complex (with about 17 times more parameters (17=138357544/8051502)). Therefore the simple gradient ascent algorithm yield quite different pictures every time it runs with different initial start image. As the solution to this optimization problem is not unique, it is expected to have a different images as the solution. However, the resulting picture should yield the probability of belonging to the corresponding class to be very high, as high as 1. Otherwise, it is most likely that the algorithm did not converge. From the trial and error, I found that the convergence (in terms of the probability of belonging to the class = 1) depends highly on the initial start image. For these reasons, my code runs until it finds the image that yieled high probability of belonging to the class. Observations I was able to find pictures with 100% probability of being sea snake, magpie, cliff or siamang! The pictures that I found look a bit different from the one found by Keras's official blog . Expectedly. The sea snake image seems to have some curvy texture? The magpie image looks to have a texture of feather ... ? some beaks? If you say so. Do I see the colourful bee eater at the center? The pillow image looks soft and fluffy.. I would not classify any of these pictures into sea snake or magpie! I wonder what would happen if I label them as not-sea-snake or not-magie, include as parts of the training data and retrain the models. (Is this the idea of GAN? Something I would love to learn.) But I guess CNN look at pictures differently. I will conclude this blog by quoting the comments from Keras's official blog : we should refrain from our natural tendency to anthropomorphize them and believe that they \"understand\", say, the concept of dog, or the appearance of a magpie, just because they are able to classify these objects with high accuracy. They don't, at least not to any any extent that would make sense to us humans. In [132]: layer_output = layer_dict [ \"predictions\" ] . output out_index = [ 65 , 18 , 92 , 721 ] for i in out_index : visualizer = VisualizeImageMaximizeFmap ( pic_shape = ( 224 , 224 , 3 )) images = [] probs = [] myprob = 0 n_alg = 0 while ( myprob < 0.9 ): myimage = visualizer . find_images_for_layer ( input_img , layer_output ,[ i ], picorig = True , n_iter = 20 ) y_pred = model . predict ( myimage [ 0 ][ 0 ]) . flatten () myprob = y_pred [ i ] n_alg += 1 print ( \"The total number of times the gradient ascent needs to run: {}\" . format ( n_alg )) argimage = { \"prediction\" :[ myimage ]} print ( \"{} probability:\" . format ( classlabel [ i ])), print ( \"{:4.3}\" . format ( myprob )), visualizer . plot_images_wrapper ( argimage , n_row = 1 , scale = 4 ) The total number of times the gradient ascent needs to run: 11 sea_snake probability: 1.0 The total number of times the gradient ascent needs to run: 50 magpie probability: 1.0 The total number of times the gradient ascent needs to run: 4 bee_eater probability: 1.0 The total number of times the gradient ascent needs to run: 10 pillow probability: 1.0 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Visualization of Filters with Keras"},{"url":"achieving-top-23-in-kaggles-facial-keypoints-detection-with-keras-tensorflow.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this post, I will review deep learning methods for detect the location of keypoints on face images. The data is provided by Kaggle's Facial Keypoints Detection . I will use Keras framework (2.0.6) with tensorflow (1.2.1) backend. There are many nice blog posts that review this data: Daniel Nouri applied convolutional neural nets using Lasagne. Shinya Yuki more recently applied same methodologies using Keras. This post follows the same line of discussions. With several new additions by me: Experiments with the data augmentation with shifted images. Previous posts only considered mirror images for data augmentations. Submittion of the predictions to Kaggle. The final model ranks top XX out of Kaggle competition. The public score=. Preparation Under data folder, training.csv and test.csv are saved. These are doanloaded from Kaggle In [57]: import os import matplotlib.pyplot as plt import numpy as np from pandas.io.parsers import read_csv from sklearn.utils import shuffle FTRAIN = 'data/training.csv' FTEST = 'data/test.csv' FIdLookup = 'data/IdLookupTable.csv' Use a single GPU In [2]: import tensorflow as tf from keras.backend.tensorflow_backend import set_session print ( tf . __version__ ) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" #### 1 GPU1 #### 2 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) Using TensorFlow backend. 1.2.1 common function defenitions These functions are written by Daniel Nouri . I made very minor changes. For example, I changed the X dimention structure to have (Nsample, Nrows in frame, N columns in frame, 1) in load2d. In [3]: def plot_sample ( X , y , axs ): ''' kaggle picture is 96 by 96 y is rescaled to range between -1 and 1 ''' axs . imshow ( X . reshape ( 96 , 96 ), cmap = \"gray\" ) axs . scatter ( 48 * y [ 0 :: 2 ] + 48 , 48 * y [ 1 :: 2 ] + 48 ) def load ( test = False , cols = None ): \"\"\" load test/train data cols : a list containing landmark label names. If this is specified, only the subset of the landmark labels are extracted. for example, cols could be: [left_eye_center_x, left_eye_center_y] return: X: 2-d numpy array (Nsample, Ncol*Nrow) y: 2-d numpy array (Nsample, Nlandmarks*2) In total there are 15 landmarks. As x and y coordinates are recorded, u.shape = (Nsample,30) \"\"\" fname = FTEST if test else FTRAIN df = read_csv ( os . path . expanduser ( fname )) df [ 'Image' ] = df [ 'Image' ] . apply ( lambda im : np . fromstring ( im , sep = ' ' )) if cols : df = df [ list ( cols ) + [ 'Image' ]] myprint = df . count () myprint = myprint . reset_index () print ( myprint ) ## row with at least one NA columns are removed! df = df . dropna () X = np . vstack ( df [ 'Image' ] . values ) / 255. # changes valeus between 0 and 1 X = X . astype ( np . float32 ) if not test : # labels only exists for the training data ## standardization of the response y = df [ df . columns [: - 1 ]] . values y = ( y - 48 ) / 48 # y values are between [-1,1] X , y = shuffle ( X , y , random_state = 42 ) # shuffle data y = y . astype ( np . float32 ) else : y = None return X , y def load2d ( test = False , cols = None ): re = load ( test , cols ) X = re [ 0 ] . reshape ( - 1 , 96 , 96 , 1 ) y = re [ 1 ] return X , y def plot_loss ( hist , name , plt , RMSE_TF = False ): ''' RMSE_TF: if True, then RMSE is plotted with original scale ''' loss = hist [ 'loss' ] val_loss = hist [ 'val_loss' ] if RMSE_TF : loss = np . sqrt ( np . array ( loss )) * 48 val_loss = np . sqrt ( np . array ( val_loss )) * 48 plt . plot ( loss , \"--\" , linewidth = 3 , label = \"train:\" + name ) plt . plot ( val_loss , linewidth = 3 , label = \"val:\" + name ) load data X is a 2 dimentional numpy array (Nsample, Ncol * Nrow) Ncol = The number of columns in original picture = 96. The landmarks labels tend to be missing a lot. The right_eyebrow_outer_end_x are recorded only for 2236 pictures. We only use the pictures with all non missing landmarks (for now). How to use all the data will be discussed later. In [4]: X , y = load () print ( \"X.shape == {}; X.min == {:.3f}; X.max == {:.3f}\" . format ( X . shape , X . min (), X . max ())) print ( \"y.shape == {}; y.min == {:.3f}; y.max == {:.3f}\" . format ( y . shape , y . min (), y . max ())) index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 left_eye_inner_corner_x 2271 5 left_eye_inner_corner_y 2271 6 left_eye_outer_corner_x 2267 7 left_eye_outer_corner_y 2267 8 right_eye_inner_corner_x 2268 9 right_eye_inner_corner_y 2268 10 right_eye_outer_corner_x 2268 11 right_eye_outer_corner_y 2268 12 left_eyebrow_inner_end_x 2270 13 left_eyebrow_inner_end_y 2270 14 left_eyebrow_outer_end_x 2225 15 left_eyebrow_outer_end_y 2225 16 right_eyebrow_inner_end_x 2270 17 right_eyebrow_inner_end_y 2270 18 right_eyebrow_outer_end_x 2236 19 right_eyebrow_outer_end_y 2236 20 nose_tip_x 7049 21 nose_tip_y 7049 22 mouth_left_corner_x 2269 23 mouth_left_corner_y 2269 24 mouth_right_corner_x 2270 25 mouth_right_corner_y 2270 26 mouth_center_top_lip_x 2275 27 mouth_center_top_lip_y 2275 28 mouth_center_bottom_lip_x 7016 29 mouth_center_bottom_lip_y 7016 30 Image 7049 X.shape == (2140, 9216); X.min == 0.000; X.max == 1.000 y.shape == (2140, 30); y.min == -0.920; y.max == 0.996 Single layer Feed forward network for setting the baseline performance The original is from Shinya Yuki In [5]: %% time from keras.models import Sequential from keras.layers import Dense, Activation from keras.optimizers import SGD model = Sequential() model.add(Dense(100,input_dim=X.shape[1])) model.add(Activation('relu')) model.add(Dense(30)) sgd = SGD(lr=0.01, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) hist = model.fit(X, y, nb_epoch=100, validation_split=0.2,verbose=False) /home/fairy/anaconda2/lib/python2.7/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`. warnings.warn('The `nb_epoch` argument in `fit` ' CPU times: user 34.2 s, sys: 3.7 s, total: 37.9 s Wall time: 18.6 s In [6]: plot_loss ( hist . history , \"model 1\" , plt ) plt . legend () plt . grid () plt . yscale ( \"log\" ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"log loss\" ) plt . show () Test data evaluation and visualization In [7]: X_test , _ = load ( test = True ) y_test = model . predict ( X_test ) index 0 0 ImageId 1783 1 Image 1783 The simple feedfoward network shows some descent performance. but sometimes landmarks are off the face! In [10]: fig = plt . figure ( figsize = ( 7 , 7 )) fig . subplots_adjust ( hspace = 0.13 , wspace = 0.0001 , left = 0 , right = 1 , bottom = 0 , top = 1 ) Npicture = 9 count = 1 for irow in range ( Npicture ): ipic = np . random . choice ( X_test . shape [ 0 ]) ax = fig . add_subplot ( Npicture / 3 , 3 , count , xticks = [], yticks = []) plot_sample ( X_test [ ipic ], y_test [ ipic ], ax ) ax . set_title ( \"picture \" + str ( ipic )) count += 1 plt . show () Save model weights and architecture In [11]: from keras.models import model_from_json def save_model ( model , name ): ''' save model architecture and model weights ''' json_string = model . to_json () open ( name + '_architecture.json' , 'w' ) . write ( json_string ) model . save_weights ( name + '_weights.h5' ) def load_model ( name ): model = model_from_json ( open ( name + '_architecture.json' ) . read ()) model . load_weights ( name + '_weights.h5' ) return ( model ) save_model ( model , \"model1\" ) model = load_model ( \"model1\" ) Delete these data as we use differently structured data from now on In [12]: del X , y , X_test , y_test Covolusional neural network Let's make our model more complecated. Loading training data. Notice that X is now 4-d numpy array (Nsample, Nrow, Ncol, 1). In [13]: ## load data X , y = load2d () X . shape index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 left_eye_inner_corner_x 2271 5 left_eye_inner_corner_y 2271 6 left_eye_outer_corner_x 2267 7 left_eye_outer_corner_y 2267 8 right_eye_inner_corner_x 2268 9 right_eye_inner_corner_y 2268 10 right_eye_outer_corner_x 2268 11 right_eye_outer_corner_y 2268 12 left_eyebrow_inner_end_x 2270 13 left_eyebrow_inner_end_y 2270 14 left_eyebrow_outer_end_x 2225 15 left_eyebrow_outer_end_y 2225 16 right_eyebrow_inner_end_x 2270 17 right_eyebrow_inner_end_y 2270 18 right_eyebrow_outer_end_x 2236 19 right_eyebrow_outer_end_y 2236 20 nose_tip_x 7049 21 nose_tip_y 7049 22 mouth_left_corner_x 2269 23 mouth_left_corner_y 2269 24 mouth_right_corner_x 2270 25 mouth_right_corner_y 2270 26 mouth_center_top_lip_x 2275 27 mouth_center_top_lip_y 2275 28 mouth_center_bottom_lip_x 7016 29 mouth_center_bottom_lip_y 7016 30 Image 7049 Out[13]: (2140, 96, 96, 1) Define simple CNN funciton as this model will be constructed several times In [40]: from keras.layers import Conv2D , MaxPooling2D , Flatten , Dropout def SimpleCNN ( withDropout = False ): ''' WithDropout: If True, then dropout regularlization is added. This feature is experimented later. ''' model = Sequential () model . add ( Conv2D ( 32 ,( 3 , 3 ), input_shape = ( 96 , 96 , 1 ))) model . add ( Activation ( 'relu' )) ## 96 - 3 + 2 model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) ## 96 - (3-1)*2 if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Conv2D ( 64 ,( 2 , 2 ))) model . add ( Activation ( 'relu' )) ## model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Conv2D ( 128 ,( 2 , 2 ))) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Flatten ()) model . add ( Dense ( 500 )) model . add ( Activation ( 'relu' )) if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Dense ( 500 )) model . add ( Activation ( 'relu' )) if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Dense ( 30 )) sgd = SGD ( lr = 0.01 , momentum = 0.9 , nesterov = True ) model . compile ( loss = \"mean_squared_error\" , optimizer = sgd ) return ( model ) In [15]: %% time model2 = SimpleCNN() hist2 = model2.fit(X,y,nb_epoch=1000,validation_split=0.2,verbose=False) CPU times: user 13min 3s, sys: 1min 28s, total: 14min 31s Wall time: 10min 52s plot validation loss, train loss In [16]: plt . figure ( figsize = ( 8 , 8 )) plot_loss ( hist . history , \"model 1\" , plt ) plot_loss ( hist2 . history , \"model 2\" , plt ) plt . legend () plt . grid () plt . yscale ( \"log\" ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"loss\" ) plt . show () In [17]: sample1 , _ = load ( test = True ) sample2 , _ = load2d ( test = True ) y_pred1 = model . predict ( sample1 ) y_pred2 = model2 . predict ( sample2 ) index 0 0 ImageId 1783 1 Image 1783 index 0 0 ImageId 1783 1 Image 1783 Compare the model performance fully connected layers vs simple CNN In [18]: fig = plt . figure ( figsize = ( 4 , 10 )) fig . subplots_adjust ( hspace = 0.001 , wspace = 0.001 , left = 0 , right = 1 , bottom = 0 , top = 1 ) Npicture = 5 count = 1 for irow in range ( Npicture ): ipic = np . random . choice ( sample2 . shape [ 0 ]) ax = fig . add_subplot ( Npicture , 2 , count , xticks = [], yticks = []) plot_sample ( sample1 [ ipic ], y_pred1 [ ipic ], ax ) if count < 3 : ax . set_title ( \"model 1\" ) count += 1 ax = fig . add_subplot ( Npicture , 2 , count , xticks = [], yticks = []) plot_sample ( sample2 [ ipic ], y_pred2 [ ipic ], ax ) if count < 3 : ax . set_title ( \"model 2\" ) count += 1 plt . show () Data augmentation flipping pictures FlippedImageDataGenerator is written by Shinya . This class extends the keras.preprocessing.image.mageDataGenerator and overwrites \"next\" method. When I try using this class, it was not correctly generating flipped pictures. *This could be because of the different version of the keras? It seems ImageDataGenerator.flow() is not internally calling next() method). So I take more manual approach and explicitly write codes to modify each batch within each iteration/epoch (just as in the first example of keras documentation ). Why not just using ImageDataGenerator? Keras has exisiting ImageDataGenerator but this only flip the data and not the land marks. I guess ImageDataGenerator can be used for image classification purpose where the classification labels do not need to be flipped. (e.g., Dog picture is a dog picture even if the picutre is flipped.) In [19]: class DataModifier ( object ): def fit ( self , X_ , y_ ): return ( NotImplementedError ) class FlipPic ( DataModifier ): def __init__ ( self , flip_indices = None ): if flip_indices is None : flip_indices = [ ( 0 , 2 ), ( 1 , 3 ), ( 4 , 8 ), ( 5 , 9 ), ( 6 , 10 ), ( 7 , 11 ), ( 12 , 16 ), ( 13 , 17 ), ( 14 , 18 ), ( 15 , 19 ), ( 22 , 24 ), ( 23 , 25 ) ] self . flip_indices = flip_indices def fit ( self , X_batch , y_batch ): batch_size = X_batch . shape [ 0 ] indices = np . random . choice ( batch_size , batch_size / 2 , replace = False ) X_batch [ indices ] = X_batch [ indices , :, :: - 1 ,:] y_batch [ indices , :: 2 ] = y_batch [ indices , :: 2 ] * - 1 # flip left eye to right eye, left mouth to right mouth and so on .. for a , b in self . flip_indices : y_batch [ indices , a ], y_batch [ indices , b ] = ( y_batch [ indices , b ], y_batch [ indices , a ] ) return X_batch , y_batch manually splitting training and validation data In [20]: from sklearn.model_selection import train_test_split X_train , X_val , y_train , y_val = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) print ( X_train . shape ) (1712, 96, 96, 1) Make sure that the pictures show up in both directions In [21]: from keras.preprocessing.image import ImageDataGenerator generator = ImageDataGenerator () modifier = FlipPic () fig = plt . figure ( figsize = ( 7 , 7 )) count = 1 for batch in generator . flow ( X_train [: 2 ], y_train [: 2 ]): X_batch , y_batch = modifier . fit ( * batch ) ax = fig . add_subplot ( 3 , 3 , count , xticks = [], yticks = []) plot_sample ( X_batch [ 0 ], y_batch [ 0 ], ax ) count += 1 if count == 10 : break plt . show () training As previously discussed, we manually write fit function. Our fit function allows \"Early stopping\", which means that the back propagration algorithm will terminate if the validation loss does not decrease for conseqtive epochs In [22]: def fit ( model , modifier , train , validation , batch_size = 32 , epochs = 2000 , print_every = 10 , patience = np . Inf ): ''' model : keras model object Modifier: DataModifier() object train: tuple containing two numpy arrays (X_train,y_train) validation: tuple containing two numpy arrays (X_val,y_val) patience: The back propagation algorithm will stop if the val_loss does not decrease after epochs ''' ## manually write fit method X_train , y_train = train X_val , y_val = validation generator = ImageDataGenerator () history = { \"loss\" :[], \"val_loss\" :[]} for e in range ( epochs ): if e % print_every == 0 : print ( 'Epoch {:4}:' . format ( e )), ## -------- ## ## training ## -------- ## batches = 0 loss_epoch = [] for X_batch , y_batch in generator . flow ( X_train , y_train , batch_size = batch_size ): X_batch , y_batch = modifier . fit ( X_batch , y_batch ) hist = model . fit ( X_batch , y_batch , verbose = False , epochs = 1 ) loss_epoch . extend ( hist . history [ \"loss\" ]) batches += 1 if batches >= len ( X_train ) / batch_size : # we need to break the loop by hand because # the generator loops indefinitely break loss = np . mean ( loss_epoch ) history [ \"loss\" ] . append ( loss ) ## --------- ## ## validation ## --------- ## y_pred = model . predict ( X_val ) val_loss = np . mean (( y_pred - y_val ) ** 2 ) history [ \"val_loss\" ] . append ( val_loss ) if e % print_every == 0 : print ( \"loss - {:6.5f}, val_loss - {:6.5f}\" . format ( loss , val_loss )) min_val_loss = np . min ( history [ \"val_loss\" ]) ## Early stopping if patience is not np . Inf : if np . all ( min_val_loss < np . array ( history [ \"val_loss\" ])[ - patience :]): break return ( history ) In [23]: %% time #X, y = load2d() model3 = SimpleCNN() hist3 = fit(model3,modifier, train=(X_train,y_train), validation=(X_val,y_val), batch_size=32,epochs=2000,print_every=100 ) Epoch 0: loss - 0.03308, val_loss - 0.00749 Epoch 100: loss - 0.00289, val_loss - 0.00274 Epoch 200: loss - 0.00174, val_loss - 0.00176 Epoch 300: loss - 0.00141, val_loss - 0.00152 Epoch 400: loss - 0.00120, val_loss - 0.00139 Epoch 500: loss - 0.00106, val_loss - 0.00131 Epoch 600: loss - 0.00097, val_loss - 0.00125 Epoch 700: loss - 0.00087, val_loss - 0.00122 Epoch 800: loss - 0.00080, val_loss - 0.00118 Epoch 900: loss - 0.00074, val_loss - 0.00114 Epoch 1000: loss - 0.00068, val_loss - 0.00113 Epoch 1100: loss - 0.00063, val_loss - 0.00110 Epoch 1200: loss - 0.00059, val_loss - 0.00109 Epoch 1300: loss - 0.00055, val_loss - 0.00108 Epoch 1400: loss - 0.00052, val_loss - 0.00107 Epoch 1500: loss - 0.00048, val_loss - 0.00107 Epoch 1600: loss - 0.00046, val_loss - 0.00106 Epoch 1700: loss - 0.00043, val_loss - 0.00106 Epoch 1800: loss - 0.00040, val_loss - 0.00106 Epoch 1900: loss - 0.00038, val_loss - 0.00106 CPU times: user 26min 35s, sys: 2min 40s, total: 29min 16s Wall time: 23min plot the training and validation losses Data augmentation with flipped pictures help improving the model prediction accuracy. One potential issue: \"train:model 3\" (final value: 0.00038) is by far less than the \"val:model 3\" (final value: 0.00106), indicating that the model might have overfitted. In [24]: plt . figure ( figsize = ( 8 , 8 )) plot_loss ( hist . history , \"model 1\" , plt ) plot_loss ( hist2 . history , \"model 2\" , plt ) plot_loss ( hist3 , \"model 3\" , plt ) plt . legend () plt . grid () plt . yscale ( \"log\" ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"loss\" ) plt . show () data augmentation shifting pictures Flipping pictures can double the number of pictures twice. If we allow the pictures to shift by some pixcels within frames, this can increase the number of pictures substantially! Here is my code to randomly shift the pictures to left, right, top, bottom by prespecified proportion. In [25]: class ShiftFlipPic ( FlipPic ): def __init__ ( self , flip_indices = None , prop = 0.1 ): super ( ShiftFlipPic , self ) . __init__ ( flip_indices ) self . prop = prop def fit ( self , X , y ): X , y = super ( ShiftFlipPic , self ) . fit ( X , y ) X , y = self . shift_image ( X , y , prop = self . prop ) return ( X , y ) def random_shift ( self , shift_range , n = 96 ): ''' :param shift_range: The maximum number of columns/rows to shift :return: keep(0): minimum row/column index to keep keep(1): maximum row/column index to keep assign(0): minimum row/column index to assign assign(1): maximum row/column index to assign shift: amount to shift the landmark assign(1) - assign(0) == keep(1) - keep(0) ''' shift = np . random . randint ( - shift_range , shift_range ) def shift_left ( n , shift ): shift = np . abs ( shift ) return ( 0 , n - shift ) def shift_right ( n , shift ): shift = np . abs ( shift ) return ( shift , n ) if shift < 0 : keep = shift_left ( n , shift ) assign = shift_right ( n , shift ) else : assign = shift_left ( n , shift ) ## less than 96 keep = shift_right ( n , shift ) return (( keep , assign , shift )) def shift_single_image ( self , x_ , y_ , prop = 0.1 ): ''' :param x_: a single picture array (96, 96, 1) :param y_: 15 landmark locations [0::2] contains x axis values [1::2] contains y axis values :param prop: proportion of random horizontal and vertical shift relative to the number of columns e.g. prop = 0.1 then the picture is moved at least by 0.1*96 = 8 columns/rows :return: x_, y_ ''' w_shift_max = int ( x_ . shape [ 0 ] * prop ) h_shift_max = int ( x_ . shape [ 1 ] * prop ) w_keep , w_assign , w_shift = self . random_shift ( w_shift_max ) h_keep , h_assign , h_shift = self . random_shift ( h_shift_max ) x_ [ w_assign [ 0 ]: w_assign [ 1 ], h_assign [ 0 ]: h_assign [ 1 ],:] = x_ [ w_keep [ 0 ]: w_keep [ 1 ], h_keep [ 0 ]: h_keep [ 1 ],:] y_ [ 0 :: 2 ] = y_ [ 0 :: 2 ] - h_shift / float ( x_ . shape [ 0 ] / 2. ) y_ [ 1 :: 2 ] = y_ [ 1 :: 2 ] - w_shift / float ( x_ . shape [ 1 ] / 2. ) return ( x_ , y_ ) def shift_image ( self , X , y , prop = 0.1 ): ## This function may be modified to be more efficient e.g. get rid of loop? for irow in range ( X . shape [ 0 ]): x_ = X [ irow ] y_ = y [ irow ] X [ irow ], y [ irow ] = self . shift_single_image ( x_ , y_ , prop = prop ) return ( X , y ) Following codes plot the generated pictures. Observations The landmarks are shiftting together with the picture frame. Notice that if you shift the pictures too much, then the landmarks go outside of the frame. (For now we are going to ignore this potential problem.) In [26]: from keras.preprocessing.image import ImageDataGenerator generator = ImageDataGenerator () shiftFlipPic = ShiftFlipPic ( prop = 0.1 ) fig = plt . figure ( figsize = ( 7 , 7 )) count = 1 for batch in generator . flow ( X_train [: 2 ], y_train [: 2 ]): X_batch , y_batch = shiftFlipPic . fit ( * batch ) ax = fig . add_subplot ( 3 , 3 , count , xticks = [], yticks = []) plot_sample ( X_batch [ 0 ], y_batch [ 0 ], ax ) count += 1 if count == 10 : break plt . show () training In [42]: %% time model4 = SimpleCNN() hist4 = fit(model4,shiftFlipPic, train=(X_train,y_train), validation=(X_val,y_val), batch_size=32,epochs=3000,print_every=50,patience=100) Epoch 0: loss - 0.04738, val_loss - 0.00727 Epoch 50: loss - 0.00461, val_loss - 0.00318 Epoch 100: loss - 0.00343, val_loss - 0.00225 Epoch 150: loss - 0.00293, val_loss - 0.00195 Epoch 200: loss - 0.00250, val_loss - 0.00177 Epoch 250: loss - 0.00230, val_loss - 0.00166 Epoch 300: loss - 0.00225, val_loss - 0.00155 Epoch 350: loss - 0.00213, val_loss - 0.00151 Epoch 400: loss - 0.00201, val_loss - 0.00142 Epoch 450: loss - 0.00192, val_loss - 0.00138 Epoch 500: loss - 0.00182, val_loss - 0.00134 Epoch 550: loss - 0.00173, val_loss - 0.00131 Epoch 600: loss - 0.00169, val_loss - 0.00131 Epoch 650: loss - 0.00166, val_loss - 0.00125 Epoch 700: loss - 0.00158, val_loss - 0.00123 Epoch 750: loss - 0.00154, val_loss - 0.00121 Epoch 800: loss - 0.00148, val_loss - 0.00117 Epoch 850: loss - 0.00147, val_loss - 0.00116 Epoch 900: loss - 0.00141, val_loss - 0.00113 Epoch 950: loss - 0.00137, val_loss - 0.00111 Epoch 1000: loss - 0.00136, val_loss - 0.00112 Epoch 1050: loss - 0.00135, val_loss - 0.00110 Epoch 1100: loss - 0.00128, val_loss - 0.00108 Epoch 1150: loss - 0.00131, val_loss - 0.00106 Epoch 1200: loss - 0.00130, val_loss - 0.00106 Epoch 1250: loss - 0.00124, val_loss - 0.00104 Epoch 1300: loss - 0.00120, val_loss - 0.00102 Epoch 1350: loss - 0.00122, val_loss - 0.00102 Epoch 1400: loss - 0.00118, val_loss - 0.00100 Epoch 1450: loss - 0.00115, val_loss - 0.00100 Epoch 1500: loss - 0.00116, val_loss - 0.00100 Epoch 1550: loss - 0.00114, val_loss - 0.00099 Epoch 1600: loss - 0.00114, val_loss - 0.00097 Epoch 1650: loss - 0.00109, val_loss - 0.00096 Epoch 1700: loss - 0.00108, val_loss - 0.00096 Epoch 1750: loss - 0.00108, val_loss - 0.00095 Epoch 1800: loss - 0.00106, val_loss - 0.00094 Epoch 1850: loss - 0.00102, val_loss - 0.00095 Epoch 1900: loss - 0.00104, val_loss - 0.00093 Epoch 1950: loss - 0.00101, val_loss - 0.00093 Epoch 2000: loss - 0.00101, val_loss - 0.00092 Epoch 2050: loss - 0.00098, val_loss - 0.00093 Epoch 2100: loss - 0.00097, val_loss - 0.00090 Epoch 2150: loss - 0.00096, val_loss - 0.00091 Epoch 2200: loss - 0.00096, val_loss - 0.00089 Epoch 2250: loss - 0.00093, val_loss - 0.00089 Epoch 2300: loss - 0.00095, val_loss - 0.00088 Epoch 2350: loss - 0.00094, val_loss - 0.00088 Epoch 2400: loss - 0.00092, val_loss - 0.00087 Epoch 2450: loss - 0.00090, val_loss - 0.00088 Epoch 2500: loss - 0.00090, val_loss - 0.00087 Epoch 2550: loss - 0.00089, val_loss - 0.00086 Epoch 2600: loss - 0.00089, val_loss - 0.00086 Epoch 2650: loss - 0.00088, val_loss - 0.00084 Epoch 2700: loss - 0.00089, val_loss - 0.00086 Epoch 2750: loss - 0.00088, val_loss - 0.00085 Epoch 2800: loss - 0.00085, val_loss - 0.00084 Epoch 2850: loss - 0.00086, val_loss - 0.00084 Epoch 2900: loss - 0.00087, val_loss - 0.00084 Epoch 2950: loss - 0.00083, val_loss - 0.00085 CPU times: user 42min 39s, sys: 4min 4s, total: 46min 44s Wall time: 37min 56s plot the training and validation losses Data augmentation with flipped + shifted pictures help improving the model prediction accuracy. The val_loss is below 0.001 Model 3 had overfitting issue when the number of epoch was 2000 (\"train:model 3\" << \"val:model 3\"). Model 4 seems to have an opposite situation, underfitting, at epoch = 2000 as (\"train:model 4\" > \"val:model 4\"). As \"val: model4\" seems still decreasing at epoch = 2000, we increase the number of epoch to 3000. By epoch = 3000 \"train:model 4\" ~= \"val:model 4\". In practice, it is better to stop the training when training loss is slightly less than the validation, but I stop training here. In [43]: plt . figure ( figsize = ( 8 , 8 )) plot_loss ( hist . history , \"model 1\" , plt ) plot_loss ( hist2 . history , \"model 2\" , plt ) plot_loss ( hist3 , \"model 3\" , plt ) plot_loss ( hist4 , \"model 4\" , plt ) plt . legend () plt . grid () plt . yscale ( \"log\" ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"loss\" ) plt . show () Save the model object as model 4 seems to be the best so far In [45]: save_model ( model4 , \"model4\" ) More complex model Shinya's blog also experimented with adding Dropout regulaziation layer. You can do this by setting model5 = SimpleCNN(True). As the data augmentation with the random shifting already served the purpose of raguralization, I did not see much improvement by adding Dropout layer. Create separate models for different landmarks separately. Up to this point, we are only using about 20% (X.shape[0] = 2140) of the original data. This is quite a waste as for some landmarks (e.g., left_eye_center, right_eye_center), more than 7000 frames are available. If I separately create model for separate set of landmarks, I can use more data for training. This motivated Danile Nouri to create seprate models. I follow this blog and create 6 separate models. The 15 landmarks are devided into 6 separate groups as shown in the list \"SPECIALIST_SETTINGS\" below. All 6 models contains the same CNN architecture but the final output layer is adjusted for different number of outputs: for example we have a model for left eye and right eye center landmark prediction. As there are are x and y coordinates for both eye centers, we have 4 nodes in the output layer of this model. Remind you that it took me 38 minutes to train Model 4 with epochs = 3000. If I train all 6 models from a scratch, it could take about 4 hours (38 minutes x 6). Instead, I will take advantage of the knowledge (i.e., weights) from model 4. We will use the weights from model 4 and only train the weights from the final output layer. This is the idea of transfer learning. About 1000 epochs would be enough to train the weights from the final output layers. However, we will just train 500 epochs due to the lack of time. In [194]: SPECIALIST_SETTINGS = [ dict ( columns = ( 'left_eye_center_x' , 'left_eye_center_y' , 'right_eye_center_x' , 'right_eye_center_y' , ), flip_indices = (( 0 , 2 ), ( 1 , 3 )), ), dict ( columns = ( 'nose_tip_x' , 'nose_tip_y' , ), flip_indices = (), ), dict ( columns = ( 'mouth_left_corner_x' , 'mouth_left_corner_y' , 'mouth_right_corner_x' , 'mouth_right_corner_y' , 'mouth_center_top_lip_x' , 'mouth_center_top_lip_y' , ), flip_indices = (( 0 , 2 ), ( 1 , 3 )), ), dict ( columns = ( 'mouth_center_bottom_lip_x' , 'mouth_center_bottom_lip_y' , ), flip_indices = (), ), dict ( columns = ( 'left_eye_inner_corner_x' , 'left_eye_inner_corner_y' , 'right_eye_inner_corner_x' , 'right_eye_inner_corner_y' , 'left_eye_outer_corner_x' , 'left_eye_outer_corner_y' , 'right_eye_outer_corner_x' , 'right_eye_outer_corner_y' , ), flip_indices = (( 0 , 2 ), ( 1 , 3 ), ( 4 , 6 ), ( 5 , 7 )), ), dict ( columns = ( 'left_eyebrow_inner_end_x' , 'left_eyebrow_inner_end_y' , 'right_eyebrow_inner_end_x' , 'right_eyebrow_inner_end_y' , 'left_eyebrow_outer_end_x' , 'left_eyebrow_outer_end_y' , 'right_eyebrow_outer_end_x' , 'right_eyebrow_outer_end_y' , ), flip_indices = (( 0 , 2 ), ( 1 , 3 ), ( 4 , 6 ), ( 5 , 7 )), ), ] from collections import OrderedDict def fit_specialists ( freeze = True , print_every = 50 , epochs = 3000 , prop = 0.1 , name_transfer_model = \"model4\" ): specialists = OrderedDict () for setting in SPECIALIST_SETTINGS : cols = setting [ 'columns' ] flip_indices = setting [ 'flip_indices' ] X , y = load2d ( cols = cols ) X_train , X_val , y_train , y_val = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) model = load_model ( name_transfer_model ) if freeze : for layer in model . layers : layer . trainable = False model . layers . pop () # get rid of output layer model . outputs = [ model . layers [ - 1 ] . output ] model . layers [ - 1 ] . outbound_nodes = [] model . add ( Dense ( len ( cols ))) # add new output layer model . compile ( loss = 'mean_squared_error' , optimizer = \"adam\" ) shiftFlipPic = ShiftFlipPic ( flip_indices = flip_indices , prop = prop ) ## print(model.summary()) hist = fit ( model , shiftFlipPic , train = ( X_train , y_train ), validation = ( X_val , y_val ), batch_size = 32 , epochs = epochs , print_every = print_every ) specialists [ cols ] = { \"model\" : model , \"hist\" : hist } return ( specialists ) Set freeze = True for training the weights only from the final outputlayers. If freeze = False then the weights from the transfer model are used only for the initialization of the model and all the weights are re-trained. epochs = 500 seems to be enough (checked by looking at the plot of val_loss vs epochs) use model4 as the transfer model In [47]: %% time specialists1 = fit_specialists(freeze=True, print_every=50, epochs=500, name_transfer_model=\"model4\") index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 Image 7049 Epoch 0: loss - 0.01192, val_loss - 0.00540 Epoch 50: loss - 0.00252, val_loss - 0.00319 Epoch 100: loss - 0.00250, val_loss - 0.00319 Epoch 150: loss - 0.00260, val_loss - 0.00313 Epoch 200: loss - 0.00260, val_loss - 0.00309 Epoch 250: loss - 0.00261, val_loss - 0.00317 Epoch 300: loss - 0.00254, val_loss - 0.00311 Epoch 350: loss - 0.00253, val_loss - 0.00312 Epoch 400: loss - 0.00249, val_loss - 0.00310 Epoch 450: loss - 0.00258, val_loss - 0.00335 index 0 0 nose_tip_x 7049 1 nose_tip_y 7049 2 Image 7049 Epoch 0: loss - 0.01236, val_loss - 0.00851 Epoch 50: loss - 0.00565, val_loss - 0.00483 Epoch 100: loss - 0.00573, val_loss - 0.00476 Epoch 150: loss - 0.00557, val_loss - 0.00490 Epoch 200: loss - 0.00575, val_loss - 0.00469 Epoch 250: loss - 0.00563, val_loss - 0.00466 Epoch 300: loss - 0.00565, val_loss - 0.00476 Epoch 350: loss - 0.00569, val_loss - 0.00534 Epoch 400: loss - 0.00555, val_loss - 0.00462 Epoch 450: loss - 0.00572, val_loss - 0.00487 index 0 0 mouth_left_corner_x 2269 1 mouth_left_corner_y 2269 2 mouth_right_corner_x 2270 3 mouth_right_corner_y 2270 4 mouth_center_top_lip_x 2275 5 mouth_center_top_lip_y 2275 6 Image 7049 Epoch 0: loss - 0.05808, val_loss - 0.00793 Epoch 50: loss - 0.00175, val_loss - 0.00156 Epoch 100: loss - 0.00157, val_loss - 0.00144 Epoch 150: loss - 0.00146, val_loss - 0.00139 Epoch 200: loss - 0.00162, val_loss - 0.00133 Epoch 250: loss - 0.00149, val_loss - 0.00130 Epoch 300: loss - 0.00149, val_loss - 0.00143 Epoch 350: loss - 0.00151, val_loss - 0.00134 Epoch 400: loss - 0.00152, val_loss - 0.00135 Epoch 450: loss - 0.00142, val_loss - 0.00137 index 0 0 mouth_center_bottom_lip_x 7016 1 mouth_center_bottom_lip_y 7016 2 Image 7049 Epoch 0: loss - 0.01435, val_loss - 0.00720 Epoch 50: loss - 0.00476, val_loss - 0.00393 Epoch 100: loss - 0.00467, val_loss - 0.00372 Epoch 150: loss - 0.00478, val_loss - 0.00377 Epoch 200: loss - 0.00476, val_loss - 0.00389 Epoch 250: loss - 0.00490, val_loss - 0.00366 Epoch 300: loss - 0.00474, val_loss - 0.00390 Epoch 350: loss - 0.00471, val_loss - 0.00402 Epoch 400: loss - 0.00455, val_loss - 0.00366 Epoch 450: loss - 0.00473, val_loss - 0.00363 index 0 0 left_eye_inner_corner_x 2271 1 left_eye_inner_corner_y 2271 2 right_eye_inner_corner_x 2268 3 right_eye_inner_corner_y 2268 4 left_eye_outer_corner_x 2267 5 left_eye_outer_corner_y 2267 6 right_eye_outer_corner_x 2268 7 right_eye_outer_corner_y 2268 8 Image 7049 Epoch 0: loss - 0.02120, val_loss - 0.00348 Epoch 50: loss - 0.00123, val_loss - 0.00081 Epoch 100: loss - 0.00098, val_loss - 0.00074 Epoch 150: loss - 0.00094, val_loss - 0.00073 Epoch 200: loss - 0.00094, val_loss - 0.00068 Epoch 250: loss - 0.00095, val_loss - 0.00070 Epoch 300: loss - 0.00089, val_loss - 0.00069 Epoch 350: loss - 0.00103, val_loss - 0.00070 Epoch 400: loss - 0.00090, val_loss - 0.00068 Epoch 450: loss - 0.00092, val_loss - 0.00070 index 0 0 left_eyebrow_inner_end_x 2270 1 left_eyebrow_inner_end_y 2270 2 right_eyebrow_inner_end_x 2270 3 right_eyebrow_inner_end_y 2270 4 left_eyebrow_outer_end_x 2225 5 left_eyebrow_outer_end_y 2225 6 right_eyebrow_outer_end_x 2236 7 right_eyebrow_outer_end_y 2236 8 Image 7049 Epoch 0: loss - 0.04013, val_loss - 0.00667 Epoch 50: loss - 0.00195, val_loss - 0.00117 Epoch 100: loss - 0.00186, val_loss - 0.00113 Epoch 150: loss - 0.00166, val_loss - 0.00111 Epoch 200: loss - 0.00185, val_loss - 0.00118 Epoch 250: loss - 0.00164, val_loss - 0.00108 Epoch 300: loss - 0.00160, val_loss - 0.00106 Epoch 350: loss - 0.00166, val_loss - 0.00106 Epoch 400: loss - 0.00167, val_loss - 0.00104 Epoch 450: loss - 0.00170, val_loss - 0.00108 CPU times: user 1h 2min 46s, sys: 5min 11s, total: 1h 7min 57s Wall time: 48min 20s plot validation losses of each specialist model You might think that the model performance is relatively poor especially for the nose tip models, and month center models, because their losses are above 0.03. However, you should not compare these validation losses with the validation losses from model 4, because the data size has increased almost 4 times for these data. In [218]: def plot_specialist ( specialists1 , plt ): i = 1 for key , value in specialists1 . items (): plot_loss ( value [ \"hist\" ], key [ 0 ] + str ( len ( key )), plt ) i += 1 plt . legend () plt . grid () plt . set_yscale ( \"log\" ) plt . set_xlabel ( \"epoch\" ) plt . set_ylabel ( \"loss\" ) fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . set_ylim ( 7 ** ( - 4 ), 10 ** ( - 2 )) plot_specialist ( specialists1 , ax ) plt . show () Where do I stand in the kaggle competition? We predict the landmarks of testing set, using model4 and specialist model. In [201]: from pandas import DataFrame , concat X_test , _ = load2d ( test = True ) ## prediction with model 4 y_pred4 = model4 . predict ( X_test ) landmark_nm = read_csv ( os . path . expanduser ( FTRAIN )) . columns [: - 1 ] . values df_y_pred4 = DataFrame ( y_pred4 , columns = landmark_nm ) ## prediction with specialist model def predict_specialist ( specialists1 , X_test ): y_pred_s = [] for columns , value in specialists1 . items (): smodel = value [ \"model\" ] y_pred = smodel . predict ( X_test ) y_pred = DataFrame ( y_pred , columns = columns ) y_pred_s . append ( y_pred ) df_y_pred_s = concat ( y_pred_s , axis = 1 ) return ( df_y_pred_s ) df_y_pred_s = predict_specialist ( specialists1 , X_test ) y_pred_s = df_y_pred_s . values index 0 0 ImageId 1783 1 Image 1783 Create .csv files to submit to keggle (Late submission) In [143]: IdLookup = read_csv ( os . path . expanduser ( FIdLookup )) def prepare_submission ( y_pred4 , filename ): ''' save a .csv file that can be submitted to kaggle ''' ImageId = IdLookup [ \"ImageId\" ] FeatureName = IdLookup [ \"FeatureName\" ] RowId = IdLookup [ \"RowId\" ] submit = [] for rowId , irow , landmark in zip ( RowId , ImageId , FeatureName ): submit . append ([ rowId , y_pred4 [ landmark ] . iloc [ irow - 1 ]]) submit = DataFrame ( submit , columns = [ \"RowId\" , \"Location\" ]) ## adjust the scale submit [ \"Location\" ] = submit [ \"Location\" ] * 48 + 48 print ( submit . shape ) loc = \"result/\" + filename + \".csv\" submit . to_csv ( loc , index = False ) print ( \"File is saved at:\" + loc ) prepare_submission ( df_y_pred4 , \"model4\" ) prepare_submission ( df_y_pred_s , \"special\" ) (27124, 2) File is saved at:result/model4.csv (27124, 2) File is saved at:result/special.csv kaggle Results model4 scores 2.86 (Private score) and 2.93 (Public score) special scores 2.26819 (Private score) and 2.53439 (Public score) This means that the special model rank top 50 out of 175 teams. i.e., top 30%! at 2018, January 4th. In [175]: ## reorder the columns of df_y_pred_s df_y_pred_s = df_y_pred_s [ df_y_pred4 . columns ] df_compare = {} df_compare [ \"difference\" ] = (( df_y_pred_s - df_y_pred4 ) ** 2 ) . mean ( axis = 1 ) df_compare [ \"RowId\" ] = range ( df_y_pred_s . shape [ 0 ]) df_compare = DataFrame ( df_compare ) df_compare = df_compare . sort_values ( \"difference\" , ascending = False ) Intuitively, when landmarks are harder to detect, the two model performances are different. Therefore it makes sense to learn about the frames where the two models detections are similar/desimilar. Plot the model performance of the best 13 and worst 13 pictures. \"best\" - The two model results agree the most. \"worst\" - The two model results disgree the most. In [192]: fig = plt . figure ( figsize = ( 12 , 35 )) Nsample = 13 pic_index = df_compare [ \"RowId\" ] . iloc [: Nsample ] . values pic_index_good = df_compare [ \"RowId\" ] . iloc [ - Nsample :] . values count = 1 for ipic_g , ipic in zip ( pic_index_good , pic_index ): ## bad model 4 ax = fig . add_subplot ( Nsample , 4 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic_g ], y_pred4 [ ipic_g ], ax ) ax . set_title ( \"Good:model4:pic\" + str ( ipic_g )) ## bad special ax = fig . add_subplot ( Nsample , 4 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic_g ], y_pred_s [ ipic_g ], ax ) ax . set_title ( \"Good:special:pic\" + str ( ipic_g )) ## bad model 4 ax = fig . add_subplot ( Nsample , 4 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic ], y_pred4 [ ipic ], ax ) ax . set_title ( \"Bad:model4:pic\" + str ( ipic )) ## bad special ax = fig . add_subplot ( Nsample , 4 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic ], y_pred_s [ ipic ], ax ) ax . set_title ( \"Bad:special:pic\" + str ( ipic )) plt . show () Specialist model revisited From the plots above, it is clear that the model performances are good when the faces are located at the center and facing front. The model performances are poor when the faces are looking side or not at the center. So the next step to improve the model prediction performance would be to 1: increase the shifting proportion (to improve the model performance of pic750) 2: use the weights from the transfer model only as the initial weights and re-train all the weights. 3: augment the data to include more data with different shear angle (to improve the model performance of pic1234 or pic962) with different zooming (to improve the model performance of pic1064) We will only conider the 1st and 2nd points. Unexpectedly, we actually do not need too many epochs for the model to converge even when the weights are not frozen! In [198]: %% time specialists2 = fit_specialists(freeze=False, print_every=50, epochs=300, prop=0.2, name_transfer_model=\"model4\") index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 Image 7049 Epoch 0: loss - 0.05922, val_loss - 0.00636 Epoch 50: loss - 0.00197, val_loss - 0.00240 Epoch 100: loss - 0.00178, val_loss - 0.00312 Epoch 150: loss - 0.00123, val_loss - 0.00299 Epoch 200: loss - 0.00125, val_loss - 0.00262 Epoch 250: loss - 0.00105, val_loss - 0.00263 index 0 0 nose_tip_x 7049 1 nose_tip_y 7049 2 Image 7049 Epoch 0: loss - 0.08784, val_loss - 0.01316 Epoch 50: loss - 0.00389, val_loss - 0.00341 Epoch 100: loss - 0.00291, val_loss - 0.00262 Epoch 150: loss - 0.00264, val_loss - 0.00270 Epoch 200: loss - 0.00229, val_loss - 0.00236 Epoch 250: loss - 0.00219, val_loss - 0.00223 index 0 0 mouth_left_corner_x 2269 1 mouth_left_corner_y 2269 2 mouth_right_corner_x 2270 3 mouth_right_corner_y 2270 4 mouth_center_top_lip_x 2275 5 mouth_center_top_lip_y 2275 6 Image 7049 Epoch 0: loss - 0.08622, val_loss - 0.01097 Epoch 50: loss - 0.00209, val_loss - 0.00181 Epoch 100: loss - 0.00149, val_loss - 0.00173 Epoch 150: loss - 0.00124, val_loss - 0.00140 Epoch 200: loss - 0.00104, val_loss - 0.00118 Epoch 250: loss - 0.00087, val_loss - 0.00118 index 0 0 mouth_center_bottom_lip_x 7016 1 mouth_center_bottom_lip_y 7016 2 Image 7049 Epoch 0: loss - 0.08083, val_loss - 0.00932 Epoch 50: loss - 0.00308, val_loss - 0.00310 Epoch 100: loss - 0.00259, val_loss - 0.00326 Epoch 150: loss - 0.00209, val_loss - 0.00311 Epoch 200: loss - 0.00196, val_loss - 0.00299 Epoch 250: loss - 0.00211, val_loss - 0.00299 index 0 0 left_eye_inner_corner_x 2271 1 left_eye_inner_corner_y 2271 2 right_eye_inner_corner_x 2268 3 right_eye_inner_corner_y 2268 4 left_eye_outer_corner_x 2267 5 left_eye_outer_corner_y 2267 6 right_eye_outer_corner_x 2268 7 right_eye_outer_corner_y 2268 8 Image 7049 Epoch 0: loss - 0.05546, val_loss - 0.00585 Epoch 50: loss - 0.00107, val_loss - 0.00089 Epoch 100: loss - 0.00075, val_loss - 0.00071 Epoch 150: loss - 0.00083, val_loss - 0.00084 Epoch 200: loss - 0.00056, val_loss - 0.00058 Epoch 250: loss - 0.00050, val_loss - 0.00060 index 0 0 left_eyebrow_inner_end_x 2270 1 left_eyebrow_inner_end_y 2270 2 right_eyebrow_inner_end_x 2270 3 right_eyebrow_inner_end_y 2270 4 left_eyebrow_outer_end_x 2225 5 left_eyebrow_outer_end_y 2225 6 right_eyebrow_outer_end_x 2236 7 right_eyebrow_outer_end_y 2236 8 Image 7049 Epoch 0: loss - 0.06381, val_loss - 0.01177 Epoch 50: loss - 0.00224, val_loss - 0.00147 Epoch 100: loss - 0.00140, val_loss - 0.00133 Epoch 150: loss - 0.00117, val_loss - 0.00122 Epoch 200: loss - 0.00097, val_loss - 0.00108 Epoch 250: loss - 0.00092, val_loss - 0.00107 CPU times: user 59min 53s, sys: 5min 59s, total: 1h 5min 52s Wall time: 53min 44s In [222]: ylim = ( 7 ** ( - 4 ), 10 ** ( - 2 )) fig = plt . figure ( figsize = ( 20 , 10 )) ax = fig . add_subplot ( 1 , 2 , 1 ) ax . set_ylim ( ylim ) ax . set_title ( \"specialist 1\" ) plot_specialist ( specialists1 , ax ) ax = fig . add_subplot ( 1 , 2 , 2 ) ax . set_ylim ( ylim ) ax . set_title ( \"specialist 2\" ) plot_specialist ( specialists2 , ax ) plt . show () Kaggle results I am proud to say that this specialist model 2 achieves: private score 1.71181 public score 2.04064 top 40 out of 175 teams ( top 23% ) Pretty good! In [223]: df_y_pred_s2 = predict_specialist ( specialists2 , X_test ) prepare_submission ( df_y_pred_s2 , \"special2\" ) (27124, 2) File is saved at:result/special2.csv finally I look at the model performances of specialist 2 in the worst performing 23 pictures as before. Notice that the model performance of the pic 750, pic 962 and many others with off-the-center faces substantially improved. In [233]: fig = plt . figure ( figsize = ( 7 , 35 )) y_pred_s2 = df_y_pred_s2 . values Nsample = 13 pic_index = df_compare [ \"RowId\" ] . iloc [: Nsample ] . values pic_index_good = df_compare [ \"RowId\" ] . iloc [ - Nsample :] . values count = 1 for ipic_g , ipic in zip ( pic_index_good , pic_index ): ax = fig . add_subplot ( Nsample , 2 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic ], y_pred_s [ ipic ], ax ) ax . set_title ( \"Bad:special1:pic\" + str ( ipic )) ax = fig . add_subplot ( Nsample , 2 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic ], y_pred_s2 [ ipic ], ax ) ax . set_title ( \"Bad:special2:pic\" + str ( ipic )) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow"},{"url":"Learn-about-ImageDataGenerator.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Goal: learn ImagedataGenerator This script shows randomly generated images using various values of ImagedataGenerator from keras.preprocessing.image Reference The Keras Blog Keras Documentations Read in the function that read in the original image, generate manuplated images and save them in a specified folder, In [119]: import os from keras.preprocessing.image import ImageDataGenerator , img_to_array , load_img def generate_plot_pics ( datagen , orig_img ): dir_augmented_data = \"data/preview\" try : ## if the preview folder does not exist, create os . mkdir ( dir_augmented_data ) except : ## if the preview folder exists, then remove ## the contents (pictures) in the folder for item in os . listdir ( dir_augmented_data ): os . remove ( dir_augmented_data + \"/\" + item ) ## convert the original image to array x = img_to_array ( orig_img ) ## reshape (Sampke, Nrow, Ncol, 3) 3 = R, G or B x = x . reshape (( 1 ,) + x . shape ) ## -------------------------- ## ## randomly generate pictures ## -------------------------- ## i = 0 Nplot = 8 for batch in datagen . flow ( x , batch_size = 1 , save_to_dir = dir_augmented_data , save_prefix = \"pic\" , save_format = 'jpeg' ): i += 1 if i > Nplot - 1 : ## generate 8 pictures break ## -------------------------- ## ## plot the generated data ## -------------------------- ## fig = plt . figure ( figsize = ( 8 , 6 )) fig . subplots_adjust ( hspace = 0.02 , wspace = 0.01 , left = 0 , right = 1 , bottom = 0 , top = 1 ) ## original picture ax = fig . add_subplot ( 3 , 3 , 1 , xticks = [], yticks = []) ax . imshow ( orig_img ) ax . set_title ( \"original\" ) i = 2 for imgnm in os . listdir ( dir_augmented_data ): ax = fig . add_subplot ( 3 , 3 , i , xticks = [], yticks = []) img = load_img ( dir_augmented_data + \"/\" + imgnm ) ax . imshow ( img ) i += 1 plt . show () We will use Taylor Swift's picture as an original picture In [118]: orig_img = load_img ( \"data/TAYLOR-SWIFT.jpg\" ) In [120]: ## rotation_range: Int. Degree range for random rotations. datagen = ImageDataGenerator ( rotation_range = 180 ) generate_plot_pics ( datagen , orig_img ) In [127]: ## rotation_range: Int. Degree range for random rotations. datagen = ImageDataGenerator ( rotation_range = 20 ) generate_plot_pics ( datagen , orig_img ) In [130]: ## width_shift_range: Float (fraction of total width). Range for random horizontal shifts. datagen = ImageDataGenerator ( width_shift_range = 1 ) generate_plot_pics ( datagen , orig_img ) In [132]: datagen = ImageDataGenerator ( width_shift_range = 0.25 ) generate_plot_pics ( datagen , orig_img ) In [141]: ## height_shift_range: Float (fraction of total height). Range for random vertical shifts. ## fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Points outside the boundaries of the input are filled according to the given mode: ## \"constant\": kkkkkkkk|abcd|kkkkkkkk (cval=k) ## \"nearest\": aaaaaaaa|abcd|dddddddd ## \"reflect\": abcddcba|abcd|dcbaabcd ## \"wrap\": abcdabcd|abcd|abcdabcd datagen = ImageDataGenerator ( height_shift_range = 0.2 , fill_mode = \"constant\" ) generate_plot_pics ( datagen , orig_img ) In [142]: datagen = ImageDataGenerator ( height_shift_range = 0.2 , fill_mode = \"nearest\" ) generate_plot_pics ( datagen , orig_img ) In [143]: datagen = ImageDataGenerator ( height_shift_range = 0.2 , fill_mode = \"reflect\" ) generate_plot_pics ( datagen , orig_img ) In [144]: datagen = ImageDataGenerator ( height_shift_range = 0.2 , fill_mode = \"wrap\" ) generate_plot_pics ( datagen , orig_img ) In [124]: ## shear_range: Float. Shear Intensity (Shear angle in counter-clockwise direction as radians) datagen = ImageDataGenerator ( shear_range = 0.2 ) generate_plot_pics ( datagen , orig_img ) In [125]: ## zoom_range: Float or [lower, upper]. Range for random zoom. ## If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]. datagen = ImageDataGenerator ( zoom_range = 0.2 ) generate_plot_pics ( datagen , orig_img ) In [133]: datagen = ImageDataGenerator ( horizontal_flip = True ) generate_plot_pics ( datagen , orig_img ) In [145]: datagen = ImageDataGenerator ( vertical_flip = True ) generate_plot_pics ( datagen , orig_img ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Learn about ImageDataGenerator"}]}